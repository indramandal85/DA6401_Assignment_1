{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "wOf-9eHQIIN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ✅ Question 1 :\n",
        "## Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use from keras.datasets import fashion_mnist for getting the fashion mnist dataset."
      ],
      "metadata": {
        "id": "KQwDjvooANHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "hgqEXnh5H_NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "ybFbuM6D7N7q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "outputId": "55ba0904-9710-4f1c-bf30-f94dee6b0325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.23.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33med24s014\u001b[0m (\u001b[33med24s014-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Imports -------------------\n",
        "import numpy as np\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# ------------------- Load Fashion-MNIST Dataset -------------------\n",
        "# Download and split the dataset into train and test sets\n",
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
        "\n",
        "# ------------------- Define Constants -------------------\n",
        "# Number of classes in the dataset\n",
        "N_CLASSES = np.unique(trainy).shape[0]\n",
        "\n",
        "# Class labels for the Fashion-MNIST dataset\n",
        "IMG_LABELS = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "              'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# ------------------- Function to Get Sample Images and Log to W&B -------------------\n",
        "def getSampleImages(nClass, imgLabels, X, y, nSamples):\n",
        "    '''\n",
        "    Logs sample images from each class to W&B\n",
        "\n",
        "    Arguments:\n",
        "      nClass     : Number of output classes\n",
        "      imgLabels  : Class labels (as list of strings)\n",
        "      X          : Image data (numpy array)\n",
        "      y          : Labels (numpy array)\n",
        "      nSamples   : Number of samples per class to log\n",
        "    '''\n",
        "\n",
        "    # Initialize list to collect samples per class\n",
        "    sampleImgsX = [[] for _ in range(nClass)]\n",
        "\n",
        "    # Collect samples\n",
        "    for i in range(y.shape[0]):\n",
        "        label = y[i]\n",
        "        if len(sampleImgsX[label]) < nSamples:\n",
        "            sampleImgsX[label].append(X[i])\n",
        "\n",
        "    # Convert to wandb.Image with caption\n",
        "    sampleImgsList = []\n",
        "    for i in range(nClass):\n",
        "        for j in range(nSamples):\n",
        "            sampleImgsList.append(wandb.Image(sampleImgsX[i][j], caption=imgLabels[i]))\n",
        "\n",
        "    # Shuffle and log to W&B\n",
        "    np.random.shuffle(sampleImgsList)\n",
        "    wandb.log({\"example\": sampleImgsList})\n",
        "\n",
        "\n",
        "# ------------------- Question 1: W&B Logging -------------------\n",
        "# Initialize wandb run\n",
        "run = wandb.init(project=\"DL_A1_Q1\", name=\"Q1_FashionMNIST_Sample_Images\")\n",
        "\n",
        "getSampleImages(N_CLASSES, IMG_LABELS, trainX, trainy, 1)\n",
        "\n",
        "# Finish the run\n",
        "run.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "AjpkIvO_7RNo",
        "outputId": "ff5e219f-e528-46f1-cb04-c2cb2958f551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250321_064310-5i02nss8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_A1_Q1/runs/5i02nss8' target=\"_blank\">Q1_FashionMNIST_Sample_Images</a></strong> to <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_A1_Q1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_A1_Q1' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_A1_Q1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_A1_Q1/runs/5i02nss8' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_A1_Q1/runs/5i02nss8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Q1_FashionMNIST_Sample_Images</strong> at: <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_A1_Q1/runs/5i02nss8' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_A1_Q1/runs/5i02nss8</a><br> View project at: <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_A1_Q1' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_A1_Q1</a><br>Synced 5 W&B file(s), 10 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250321_064310-5i02nss8/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "RDPiii6aIDO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# ✅ Question 2 :\n",
        "## Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes. Your code should be flexible such that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
      ],
      "metadata": {
        "id": "AJgpSSS-_woL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing code**"
      ],
      "metadata": {
        "id": "UUTKPOYqAz3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "h2o0HqEZIERN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAOo8LX0relx"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This code defines classes for one-hot encoding labels, normalizing input features, and splitting data into training and validation sets,\n",
        "which are essential steps in preprocessing for machine learning tasks. Each class handles a specific transformation to prepare data for\n",
        "model training and evaluation.\n",
        "'''\n",
        "import numpy  as np\n",
        "\n",
        "class OneHotEncoder:\n",
        "\n",
        "    def __init__(self,x, y):\n",
        "        self.y = y\n",
        "        self.x = x\n",
        "        #self.num_class = num_class\n",
        "        self.onehot_encode()\n",
        "\n",
        "    def onehot_encode(self):\n",
        "        onehot = np.zeros((self.x.shape[0], 10))\n",
        "\n",
        "        for i, j in zip(range(len(self.x)), self.y):\n",
        "            onehot[i, j] = 1\n",
        "        return onehot.T\n",
        "\n",
        "class Normalize:\n",
        "\n",
        "    def __init__(self, unprocessed_X):\n",
        "        self.unprocessed_X = unprocessed_X\n",
        "        self.Norm_reshape()\n",
        "\n",
        "    def Norm_reshape(self):\n",
        "        X_norm = np.reshape(self.unprocessed_X,(self.unprocessed_X.shape[0],784)).T/255\n",
        "        X_norm= np.array(X_norm)\n",
        "\n",
        "        return X_norm\n",
        "\n",
        "\n",
        "class TrainValSplit:\n",
        "  def __init__(self, X_train, y_train, Val_split_ratio = 0.9):\n",
        "    self.X = X_train\n",
        "    self.y = y_train\n",
        "    self.vsr = Val_split_ratio\n",
        "\n",
        "  def Apply_split(self):\n",
        "    np.random.seed(0)\n",
        "\n",
        "    i = np.random.permutation(len(self.X))\n",
        "    split = int(self.X.shape[0] * (1 - self.vsr))\n",
        "\n",
        "    train = i[:split]\n",
        "    val = i[split:]\n",
        "\n",
        "    train_X = self.X[train]\n",
        "    val_X = self.X[val]\n",
        "\n",
        "    train_y = self.y[train]\n",
        "    val_y = self.y[val]\n",
        "\n",
        "    return train_X , train_y, val_X, val_y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Activation functions and their gradient calculation code**"
      ],
      "metadata": {
        "id": "4img7vaRBMXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CalActivation:\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "\n",
        "# Sigmoid (for hidden layers)\n",
        "class sigmoid(CalActivation):\n",
        "    def use_sigmoid(self):\n",
        "        return 1 / (1 + np.exp(-np.clip(self.m, -250, 250)))  # Clip inputs\n",
        "\n",
        "    def sigmoid_d(self):\n",
        "        sig = self.use_sigmoid()\n",
        "        return sig * (1 - sig)\n",
        "\n",
        "\n",
        "class relu(CalActivation):\n",
        "    def use_relu(self):\n",
        "        return np.maximum(0, self.m)\n",
        "\n",
        "    def relu_d(self):\n",
        "        return np.where(self.m > 0, 1, 0)\n",
        "\n",
        "class Leakyrelu(CalActivation):\n",
        "    def use_relu(self):\n",
        "        return np.maximum(0, self.m)\n",
        "\n",
        "    def relu_d(self):\n",
        "        return np.where(self.m > 0, 1, 0.01)\n",
        "\n",
        "\n",
        "class softmax(CalActivation):\n",
        "    def use_softmax(self):\n",
        "        shifted = self.m - np.max(self.m, axis=0, keepdims=True)\n",
        "        exps = np.exp(shifted)\n",
        "        return exps / np.sum(exps, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "    def softmax_d(self):\n",
        "        z=self.m - np.max(self.m,axis=0)\n",
        "        soft=np.exp(z)/np.sum(np.exp(z),axis=0)\n",
        "        return soft*(1-soft)\n",
        "\n",
        "class tanh(CalActivation):\n",
        "    def use_tanh(self):\n",
        "        return np.tanh(self.m)  # Built-in is stable\n",
        "\n",
        "    def tanh_d(self):\n",
        "        return 1 - (self.use_tanh() ** 2)\n",
        "\n",
        "\n",
        "\n",
        "# This function call different activation function\n",
        "class apply_activation(CalActivation):\n",
        "\n",
        "    def __init__(self, activation_function, m):\n",
        "        super().__init__(m)\n",
        "        self.activation_function = activation_function.lower()\n",
        "\n",
        "    def do_activation(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.m).use_sigmoid()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.m).use_relu()\n",
        "        elif self.activation_function == 'lrelu':\n",
        "            return Leakyrelu(self.m).use_relu()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.m).use_tanh()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.m).use_softmax()\n",
        "\n",
        "    def do_activation_derivative(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.m).sigmoid_d()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.m).relu_d()\n",
        "        elif self.activation_function == 'lrelu':\n",
        "            return Leakyrelu(self.m).relu_d()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.m).tanh_d()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.m).softmax_d()\n",
        "        else:\n",
        "           raise ValueError(\"Unknown activation function\")\n"
      ],
      "metadata": {
        "id": "_jniGnSv6P9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initilization of weights and biases code using Xavier / He / Random**"
      ],
      "metadata": {
        "id": "aKmIysBmBWES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Initilize:\n",
        "    def __init__(self, layer_dimension, activation_function, y_train, method = \"Xavier_U\"):\n",
        "        self.n = layer_dimension\n",
        "        self.activation_fn = activation_function\n",
        "        self.y = y_train\n",
        "        self.Init_method = method\n",
        "\n",
        "class InitializeWeights(Initilize):\n",
        "    def __init__(self, ip_size, op_size, activation_function, batch_size, method):\n",
        "        super().__init__([ip_size, op_size], activation_function, batch_size, method)  # Properly inherit Initilize attributes\n",
        "        self.i_size = ip_size\n",
        "        self.o_size = op_size\n",
        "        self.Init_weights()\n",
        "\n",
        "\n",
        "    def Init_weights(self):\n",
        "        np.random.seed(0)\n",
        "\n",
        "        if self.Init_method == \"Xavier_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(1 / self.i_size)\n",
        "          self.weight = np.random.randn(self.o_size,self.i_size)*a\n",
        "\n",
        "        elif self.Init_method == \"Xavier_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / (self.o_size + self.i_size))\n",
        "          self.weight = np.random.uniform((-a), a,( self.o_size,self.i_size))\n",
        "\n",
        "        elif self.Init_method == \"He_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(2 / self.i_size)\n",
        "          self.weight = np.random.randn(self.o_size,self.i_size)*a\n",
        "\n",
        "        elif self.Init_method == \"He_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / self.i_size)\n",
        "          self.weight = np.random.uniform(-a, a, (self.o_size,self.i_size))\n",
        "\n",
        "        elif self.Init_method == \"Random\":\n",
        "          np.random.seed(0)\n",
        "          self.weight = np.random.randn(self.o_size,self.i_size)*0.01\n",
        "        else:\n",
        "          raise ValueError(f\"Unknown initialization method: {self.Init_method}\")\n",
        "\n",
        "\n",
        "        # Initialize biases and activations\n",
        "        self.bias = np.zeros((self.o_size, 1))\n",
        "        self.a = np.zeros((self.o_size, len(self.y[1])))\n",
        "        self.h = np.zeros((self.o_size, len(self.y[1])))\n",
        "\n",
        "        # Activation function and its derivative\n",
        "        self.g = apply_activation(self.activation_fn, self.a).do_activation()\n",
        "        self.d_g = apply_activation(self.activation_fn, self.a).do_activation_derivative()\n",
        "\n",
        "        # Gradients\n",
        "        self.d_a = np.zeros_like(self.a)\n",
        "        self.d_h = np.zeros_like(self.h)\n",
        "        self.d_w = np.zeros_like(self.weight)\n",
        "        self.d_b = np.zeros_like(self.bias)\n",
        "        self.Weight_updates = np.zeros_like(self.weight)\n",
        "        self.bias_updates = np.zeros_like(self.bias)\n",
        "\n",
        "\n",
        "\n",
        "class Weight_bias(Initilize):\n",
        "    def __init__(self, layer_dimension, activation_function, y_train, method=\"Xavier_U\"):\n",
        "        super().__init__(layer_dimension, activation_function, y_train, method)\n",
        "        self.network = []\n",
        "\n",
        "\n",
        "    def Init_network(self):\n",
        "\n",
        "        for i in range(1, len(self.n)):\n",
        "          self.network.append(InitializeWeights( self.n[i-1], self.n[i], self.activation_fn[i-1], self.y, self.Init_method))\n",
        "\n",
        "        return self.network\n"
      ],
      "metadata": {
        "id": "PATIBefL65DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feedforward Code for Prediction**"
      ],
      "metadata": {
        "id": "nDDchVi6BiFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Pre_Feedforward:\n",
        "    def __init__(self, inputs, w, b, activation_fn):\n",
        "        self.ip = inputs\n",
        "        self.w = w\n",
        "        self.b = b\n",
        "        self.activation_fn = activation_fn\n",
        "        self.gamma = np.ones((w.shape[0], 1))  # Scale parameter for Batch Norm\n",
        "        self.beta = np.zeros((w.shape[0], 1))  # Shift parameter for Batch Norm\n",
        "        self.a, self.h = self.Preactivation_cal()\n",
        "\n",
        "    def Preactivation_cal(self):\n",
        "        A = np.dot(self.w, self.ip) + self.b\n",
        "\n",
        "        # Activation Normalization (Per Sample Normalization)\n",
        "        A_norm = (A - np.mean(A, axis=1, keepdims=True)) / (np.std(A, axis=1, keepdims=True) + 1e-8)\n",
        "\n",
        "        # Batch Normalization (Learnable Parameters: gamma & beta)\n",
        "        A_batch_norm = self.gamma * A_norm + self.beta  # Scale & Shift\n",
        "\n",
        "        H = apply_activation(self.activation_fn, A_batch_norm).do_activation()\n",
        "        return A_batch_norm, H  # Returning Batch Normalized Pre-Activation\n",
        "\n",
        "\n",
        "class Feedforward:\n",
        "    def __init__(self, X_train, activation_fn, method, network):\n",
        "        self.input = X_train\n",
        "        self.activation_fn = activation_fn\n",
        "        self.method = method\n",
        "        self.network = network\n",
        "\n",
        "    def Forward_prop(self):\n",
        "        L = len(self.network)\n",
        "\n",
        "        for i in range(L):\n",
        "            pre_layer = Pre_Feedforward(self.input, self.network[i].weight, self.network[i].bias, self.activation_fn[i])\n",
        "            self.network[i].a = pre_layer.a\n",
        "            self.network[i].h = pre_layer.h\n",
        "            self.input = self.network[i].h  # Propagate activated values\n",
        "\n",
        "        return self.network\n"
      ],
      "metadata": {
        "id": "FftS8rFt9VWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_new= Normalize(X_train).Norm_reshape()\n",
        "y_new =OneHotEncoder(X_train,y_train).onehot_encode()"
      ],
      "metadata": {
        "id": "p4VcmSf38u1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Here is the answer of question 2 where we have to predict the output of 10 class in probability disturbution form**"
      ],
      "metadata": {
        "id": "6FSYr70tAbvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Initialise the network with Xavier Uniform initilised weight and biases\n",
        "'''\n",
        "\n",
        "Initial_network = Weight_bias(layer_dimension = [784, 128, 64, 10],\n",
        "                              activation_function = ['sigmoid','relu','softmax'],\n",
        "                              y_train = y_new,\n",
        "                              method = \"Xavier_U\").Init_network()\n",
        "\n",
        "'''\n",
        "Predicting the output of 10 class with Xavier Uniform initilised weight and biases\n",
        "'''\n",
        "\n",
        "Predict = Feedforward(X_train = X_new,\n",
        "                     activation_fn= ['sigmoid','relu','softmax'],\n",
        "                     method = \"Xavier_U\",\n",
        "                     network= Initial_network).Forward_prop()\n",
        "\n",
        "\n",
        "print(\" Here is the prediction of output of 10 class in probability disturbution form \\n with Xavier Uniform initilised weight and biases: \\n\\n\", Predict[2].h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fEvrBBF8wnE",
        "outputId": "6f1bc4af-c8e1-4fc1-c8f1-e5758192133b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Here is the prediction of output of 10 class in probability disturbution form \n",
            " with Xavier Uniform initilised weight and biases: \n",
            "\n",
            " [[0.15131579 0.02254063 0.07011927 ... 0.02134032 0.1547177  0.27680912]\n",
            " [0.04236374 0.0366043  0.10287327 ... 0.09750509 0.02806686 0.03796533]\n",
            " [0.14732446 0.30968874 0.10342544 ... 0.27471801 0.07746666 0.01621352]\n",
            " ...\n",
            " [0.22240824 0.19012178 0.1045384  ... 0.014127   0.03969555 0.04926175]\n",
            " [0.01593975 0.23625835 0.06535765 ... 0.15377935 0.04608808 0.07835519]\n",
            " [0.2350658  0.03385474 0.10834192 ... 0.00426795 0.11168121 0.08030805]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "IeM0PGqhINGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ✅ Question 3 :\n",
        "##Implement the backpropagation algorithm with support for the following optimisation functions\n",
        "\n",
        "##sgd\n",
        "##momentum based gradient descent\n",
        "##nesterov accelerated gradient descent\n",
        "##rmsprop\n",
        "##adam\n",
        "##nadam (Not required as aked)\n",
        "##We will check the code for implementation and ease of use (e.g., how easy it is to add a new optimisation algorithm such as Eve). Note that the code should be flexible enough to work with different batch sizes."
      ],
      "metadata": {
        "id": "sjL6Lhp3B5e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "ax8lGGyzIOal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L2 / L1 Norm Regularization Code and their Gradient Calling functions :**"
      ],
      "metadata": {
        "id": "FEACWx0UDJWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Regularisation:\n",
        "    def __init__(self, network, weight=0):\n",
        "        self.network = network\n",
        "        self.weight = weight\n",
        "\n",
        "class L2_regularisation(Regularisation):\n",
        "    def Apply_L2(self):\n",
        "        \"\"\"Returns L2 regularization loss for the given network.\"\"\"\n",
        "        L = len(self.network)\n",
        "        res = 0\n",
        "        for j in range(L):\n",
        "            if np.isnan(self.network[j].weight).any():\n",
        "                print(f\"Warning: NaN detected in network weights at layer {j}\")\n",
        "                return 0  # Prevents NaN propagation\n",
        "            res += 0.5 * np.sum(self.network[j].weight ** 2)\n",
        "        return res\n",
        "\n",
        "    def Apply_L2_grad(self, weight):\n",
        "        \"\"\"Returns L2 regularization gradient for the given weight matrix/tensor.\"\"\"\n",
        "        return 2 * weight\n",
        "\n",
        "class L1_regularisation(Regularisation):\n",
        "    def Apply_L1(self):\n",
        "        \"\"\"Returns L1 regularization loss for the given network.\"\"\"\n",
        "        L = len(self.network)\n",
        "        res = 0\n",
        "        for j in range(L):\n",
        "            if np.isnan(self.network[j].weight).any():\n",
        "                print(f\"Warning: NaN detected in network weights at layer {j}\")\n",
        "                return 0  # Prevents NaN propagation\n",
        "            res += (1 / 2) * np.sum(np.abs(self.network[j].weight))\n",
        "        return res\n",
        "\n",
        "    def Apply_L1_grad(self, weight):\n",
        "        \"\"\"Returns L1 regularization gradient for the given weight matrix/tensor.\"\"\"\n",
        "        return np.sign(weight)\n",
        "\n",
        "class ApplyReg(Regularisation):\n",
        "    def __init__(self, reg_function, network, weight=0):\n",
        "        self.reg_function = reg_function\n",
        "        super().__init__(network, weight)\n",
        "\n",
        "    def do_reg(self):\n",
        "        if self.reg_function == 'L2':\n",
        "            return L2_regularisation(self.network).Apply_L2()\n",
        "        if self.reg_function == 'L1':\n",
        "            return L1_regularisation(self.network).Apply_L1()\n",
        "        if self.reg_function == 'L2_d':\n",
        "            return L2_regularisation(self.network).Apply_L2_grad(self.weight)  # Explicitly pass weight\n",
        "        if self.reg_function == 'L1_d':\n",
        "            return L1_regularisation(self.network).Apply_L1_grad(self.weight)  # Explicitly pass weight\n"
      ],
      "metadata": {
        "id": "kj4ZlQjV9Jem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overall Loss and Accuracy Calling functions code :**"
      ],
      "metadata": {
        "id": "ASwu48SQD_Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CalculateAllLoss:\n",
        "  def __init__(self, X_train, y_predicted,network, y_train, primary_loss, weight_decay=0, regularisation_fn=None):\n",
        "    self.y_predicted = y_predicted\n",
        "    self.y_true = y_train\n",
        "    self.network = network\n",
        "    self.X_train = X_train\n",
        "    self.loss_value = primary_loss\n",
        "    self.weight_decay = weight_decay\n",
        "    self.regularisation_fn= regularisation_fn\n",
        "    self.calc_accuracy_loss()\n",
        "\n",
        "\n",
        "  def overall_loss(self):\n",
        "    \"\"\"\n",
        "    Calculates the total loss of the network.\n",
        "\n",
        "    This includes:\n",
        "    - The primary loss (e.g., Cross-Entropy Loss)\n",
        "    - Optional regularization (like L2 regularization)\n",
        "\n",
        "    Parameters:\n",
        "    - network: The neural network model.\n",
        "    - Y_pred: The predicted output from the network.\n",
        "    - Y_true: The actual labels (ground truth).\n",
        "    - loss_fn: The loss function to be used (e.g., CrossEntropy_loss).\n",
        "    - weight_decay: A coefficient for regularization (default is 0, meaning no regularization).\n",
        "    - regularisation_fn: A function for computing the regularization term (optional).\n",
        "\n",
        "    Returns:\n",
        "    - Total loss value.\n",
        "    \"\"\"\n",
        "    # Get network predictions self.y_prediction from model\n",
        "    total_loss = self.loss_value  # Compute primary loss\n",
        "\n",
        "    if self.weight_decay > 0 and self.regularisation_fn:\n",
        "        regularized_val = ApplyReg(self.regularisation_fn, self.network).do_reg()\n",
        "        total_loss += self.weight_decay * regularized_val # Add regularization term if applicable  # Compute total loss\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def calc_accuracy_loss(self):\n",
        "    \"\"\"\n",
        "    Computes the accuracy and loss for a given neural network.\n",
        "\n",
        "    Parameters:\n",
        "    - network: The neural network model.\n",
        "    - X: Input data (features for prediction).\n",
        "    - Y: Actual labels (ground truth values).\n",
        "    - loss_fn: The loss function to be used.\n",
        "    - weight_decay: Regularization strength (default is 0, meaning no regularization).\n",
        "    - regularisation_fn: A function to compute the regularization term (optional).\n",
        "\n",
        "    Returns:\n",
        "    - accuracy: The percentage of correctly classified examples.\n",
        "    - loss: The computed total loss.\n",
        "    \"\"\"\n",
        "\n",
        "    total_loss = self.loss_value  # Compute primary loss\n",
        "\n",
        "    if self.weight_decay > 0 and self.regularisation_fn:\n",
        "        regularized_val = ApplyReg(self.regularisation_fn, self.network).do_reg()\n",
        "        total_loss += self.weight_decay * regularized_val\n",
        "\n",
        "    # Ensure dimensions match between input and labels\n",
        "    assert self.X_train.shape[1] == self.y_true.shape[1], \"Mismatch in batch size between inputs and labels\"\n",
        "\n",
        "    # Compute accuracy by comparing predicted vs actual labels\n",
        "    batch_size = self.X_train.shape[1]  # Number of examples\n",
        "    correct_predictions = np.sum(np.argmax(self.y_predicted, axis=0) == np.argmax(self.y_true, axis=0))\n",
        "\n",
        "    accuracy = correct_predictions / batch_size  # Compute accuracy as a fraction\n",
        "\n",
        "    return accuracy , total_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "lc5p7TWsCycT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All Loss functions and their Gradient Calling functions code :**"
      ],
      "metadata": {
        "id": "fvCL3kjNEO6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CalLoss:\n",
        "    def __init__(self, y, y_pred):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_pred\n",
        "        if y.shape != y_pred.shape:\n",
        "            raise ValueError(f\"Shape mismatch: y shape is {self.y.shape}, y_predicted shape is {self.y_predicted.shape}\")\n",
        "\n",
        "class CrossEntropy(CalLoss):\n",
        "    def give_celoss(self):\n",
        "        epsilon = 1e-8  # Small value to prevent log(0)\n",
        "        return -np.mean(self.y * np.log(self.y_predicted + epsilon))\n",
        "\n",
        "    def Give_cegrad(self):\n",
        "        epsilon = 1e-8  # Prevent division by zero\n",
        "        grad = -self.y / (self.y_predicted + epsilon)\n",
        "        return grad\n",
        "\n",
        "class SquaredError(CalLoss):\n",
        "    def give_seloss(self):\n",
        "        return np.mean((self.y - self.y_predicted) ** 2)\n",
        "\n",
        "    def Give_segrad(self):\n",
        "        grad = -2 * (self.y - self.y_predicted)\n",
        "        return np.clip(grad, -1, 1)  # Clip to prevent exploding gradients\n",
        "\n",
        "class callloss(CalLoss):\n",
        "    def __init__(self, loss_function, y, y_pred):\n",
        "        self.loss_function = loss_function.lower()\n",
        "        super().__init__(y, y_pred)\n",
        "\n",
        "    def give_loss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return CrossEntropy(self.y, self.y_predicted).give_celoss()\n",
        "        elif self.loss_function == 'se':\n",
        "            return SquaredError(self.y, self.y_predicted).give_seloss()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss function: {self.loss_function}\")\n",
        "\n",
        "    def give_gradloss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return CrossEntropy(self.y, self.y_predicted).Give_cegrad()\n",
        "        elif self.loss_function == 'se':\n",
        "            return SquaredError(self.y, self.y_predicted).Give_segrad()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss function: {self.loss_function}\")\n"
      ],
      "metadata": {
        "id": "qPTuWBjAC2Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Below we have the answers of question 3 : Proper Backprop code and along with that we ahve all 5 optimiser codes"
      ],
      "metadata": {
        "id": "Bb3D9CW3Fb10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backpropagation codes which returns the network :**"
      ],
      "metadata": {
        "id": "eGducKvQEYyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "class Backpropagation:\n",
        "\n",
        "    def __init__(self, loss_function, X_train, y_train, y_pred, network, weight_decay, batch, batch_size, activation_fn):\n",
        "        self.loss_function = loss_function\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.y_predicted = y_pred\n",
        "        self.network = network\n",
        "        self.weight_decay = weight_decay\n",
        "        self.batch = batch\n",
        "        self.batch_size = batch_size\n",
        "        self.activation_fn = activation_fn\n",
        "\n",
        "\n",
        "    def backward_propagation(self):\n",
        "      L = len(self.network)\n",
        "\n",
        "      assert(self.y_train.shape[1] == self.y_predicted.shape[1])\n",
        "\n",
        "\n",
        "      self.network[-1].d_a = callloss(self.loss_function,self.y_train, self.y_predicted).give_gradloss()\n",
        "      # print(\"network -[-1].d_a : \" , self.network[-1].d_a.shape)\n",
        "      A_k = apply_activation(self.activation_fn[-1], self.network[-1].h).do_activation_derivative()\n",
        "      # print(\"shape A_ k : \" , A_k.shape)\n",
        "      self.network[-1].d_h = self.network[-1].d_a * A_k\n",
        "      # print(\"network -[-1].d_h : \" , self.network[-1].d_a.shape)\n",
        "\n",
        "      self.network[-1].d_w = self.network[-1].d_h @ self.network[-2].a.T  + self.weight_decay * self.network[-1].weight\n",
        "      # print(\"network -[-1].d_w : \" , self.network[-1].d_w.shape)\n",
        "      d_b = -np.sum(self.network[-1].d_h, axis = 1)\n",
        "      self.network[-1].d_b = d_b.reshape(-1 , 1)\n",
        "      # print(\"network -[-1].d_b : \" , self.network[-1].d_b.shape)\n",
        "\n",
        "\n",
        "\n",
        "      for k in range(L-2,0,-1):\n",
        "          # print(f\"No of layers rotation {k}\")\n",
        "\n",
        "          self.network[k].d_h = self.network[k + 1].weight.T @ self.network[k + 1].d_a\n",
        "          # print(f\"shape self.network-{k}.d_h : \" , self.network[k].d_h.shape)\n",
        "          act_derv =  apply_activation(self.activation_fn[k], self.network[k].a)\n",
        "          self.network[k].d_a = self.network[k].d_h * act_derv.do_activation_derivative()\n",
        "          # print(f\"shape self.network-{k}.d_a : \" , self.network[k].d_a.shape)\n",
        "\n",
        "          self.network[k].d_w = self.network[k].d_a @ self.network[k-1].h.T  + self.weight_decay * self.network[k].weight\n",
        "          # print(f\"shape self.network-{k}.d_w : \" , self.network[k].d_w.shape)\n",
        "          derv_bias = -np.sum(self.network[k].d_a, axis=1)\n",
        "          self.network[k].d_b = derv_bias.reshape(-1 , 1)\n",
        "          # print(f\"shape self.network-{k}.d_b : \" , self.network[k].d_b.shape)\n",
        "\n",
        "      # print(f\"shape self.network-{0}.d_a : \" , self.network[0].d_a.shape)\n",
        "      d_a = self.network[0].d_a[:, self.batch*self.batch_size : (self.batch+1)*self.batch_size]\n",
        "      # print(f\"shape self.network-{0}.d_a : \" , d_a.shape)\n",
        "      self.network[0].d_w = np.dot(d_a , self.X_train.T) + self.weight_decay * self.network[0].weight\n",
        "      # print(\"network -[0].d_w : \" , self.network[0].d_w.shape)\n",
        "      self.network[0].d_b = np.sum(self.network[0].d_a, axis=1, keepdims = True)\n",
        "\n",
        "      return self.network\n"
      ],
      "metadata": {
        "id": "SAoaQhHmC4QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All 5 Optimisers ( SGD / Momentum / NAG / RMSProp / ADAM ) Calling functions :**"
      ],
      "metadata": {
        "id": "U0RoKfJ5Eyp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "import copy\n",
        "\n",
        "\n",
        "class Optimizer:\n",
        "  def __init__(self, loss_function, X_train, y_train, activation_fn, layers_dimensions ,method, batch_size, epochs,validX_train = None, validy_train = None,weight_decay = 0, eta = 0.01, beta = 0.9, beta2 = 0.999, regularization_fn = \"L2\",grad_reglr_fn = \"L2_d\"):\n",
        "      self.loss_function = loss_function\n",
        "      self.X_train = X_train\n",
        "      self.y_train = y_train\n",
        "      self.validX_train = validX_train\n",
        "      self.validy_train = validy_train\n",
        "      self.activation_fn = activation_fn\n",
        "      self.method = method\n",
        "      self.n = layers_dimensions\n",
        "      self.batch_size = batch_size\n",
        "      self.epochs = epochs\n",
        "      self.weight_decay = weight_decay\n",
        "      self.eta = eta\n",
        "      self.epsilon =1e-10\n",
        "      self.beta = beta\n",
        "      self.beta2 = beta2\n",
        "      self.regularization_fn = regularization_fn\n",
        "      self.grad_reglr_fn = grad_reglr_fn\n",
        "      self.batches_number = self.X_train.shape[1]//self.batch_size\n",
        "      z = Weight_bias(self.n, self.activation_fn, self.y_train, self.method)\n",
        "      self.init_network = z.Init_network()\n",
        "      self.min_eta = 1e-4\n",
        "      # wandb.init(project= \"DA6401_Assignment_1\" , name = \"RMSProp with validation data of 1/3rd of 60k rows\")\n",
        "\n",
        "\n",
        "class SGD(Optimizer):\n",
        "\n",
        "  def Gradient_descent(self):\n",
        "\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "    L = len(self.n) - 1\n",
        "    # Initialize variables neesed to keep track of loss\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    overall_loss = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    for epoch in tqdm(range(self.epochs)):\n",
        "\n",
        "      for batch in range(self.batches_number):\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:  # Ensure we have enough data points\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)  # Compute standard deviation of recent losses\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "\n",
        "        for j in range(L):\n",
        "          self.fw_network[j].weight -= self.eta * self.bp_network[j].d_w\n",
        "          self.fw_network[j].bias -= self.eta * self.bp_network[j].d_b\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 4)\n",
        "        train_loss = round(loss , 4)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 4)\n",
        "          val_loss = round(v_loss , 4)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        # wandb.log({'epoch': self.epochs,  # Use current epoch, not total epochs\n",
        "        #             'validation_accuracy': val_accuracy if self.validX_train is not None else 0,\n",
        "        #             'accuracy': accuracy,\n",
        "        #             'validation_loss': val_loss if self.validX_train is not None else 0,\n",
        "        #             'loss': train_loss\n",
        "        #         })\n",
        "      print(f\"   Epochs no: {(epoch+1)}  |  Train acc: {accuracy}  |  val acc: {val_accuracy}  |  Train Loss: {train_loss} |  Val Loss : {val_loss}\")\n",
        "\n",
        "    return self.init_network, train_loss, accuracy, val_accuracy, val_loss\n",
        "\n",
        "\n",
        "\n",
        "class MGD(Optimizer):\n",
        "\n",
        "  def momentum_GD(self):\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "    L = len(self.n) - 1\n",
        "    # Initialize variables neesed to keep track of loss\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "    for epoch in tqdm(range(self.epochs)):\n",
        "\n",
        "      for batch in range(self.batches_number):\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:  # Ensure we have enough data points\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)  # Compute standard deviation of recent losses\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "        # self.beta = min(1 - 2 ** (-1 - math.log((self.calls / 250.0) + 1, 2)), self.beta)\n",
        "\n",
        "        for j in range(L):\n",
        "          u_w[j] = u_w[j] * self.beta + self.bp_network[j].d_w * self.eta\n",
        "          u_b[j] = u_b[j] * self.beta + self.bp_network[j].d_b * self.eta\n",
        "\n",
        "          self.fw_network[j].weight -= u_w[j]\n",
        "          self.fw_network[j].bias -= u_b[j]\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 4)\n",
        "        train_loss = round(loss , 4)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 4)\n",
        "          val_loss = round(v_loss , 4)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        # wandb.log({'epoch': self.epochs,  # Use current epoch, not total epochs\n",
        "        #             'validation_accuracy': val_accuracy if self.validX_train is not None else 0,\n",
        "        #             'accuracy': accuracy,\n",
        "        #             'validation_loss': val_loss if self.validX_train is not None else 0,\n",
        "        #             'loss': train_loss\n",
        "        #         })\n",
        "      print(f\"   Epochs no: {(epoch+1)}  |  Train acc: {accuracy}  |  val acc: {val_accuracy}  |  Train Loss: {train_loss} |  Val Loss : {val_loss}\")\n",
        "\n",
        "    return self.init_network, train_loss, accuracy, val_accuracy, val_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NAG(Optimizer):\n",
        "\n",
        "  def Nesterov_AGD(self):\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "    L = len(self.n) - 1\n",
        "    # Initialize variables neesed to keep track of loss\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "    val_loss = 0\n",
        "    u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "    lookleap_network = self.init_network[:]\n",
        "\n",
        "    for epoch in tqdm(range(self.epochs)):\n",
        "\n",
        "      for batch in range(self.batches_number):\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:  # Ensure we have enough data points\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)  # Compute standard deviation of recent losses\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "\n",
        "        for j in range(L):\n",
        "          u_w[j] = u_w[j] * self.beta + self.bp_network[j].d_w * self.eta\n",
        "          u_b[j] = u_b[j] * self.beta + self.bp_network[j].d_b * self.eta\n",
        "\n",
        "          self.fw_network[j].weight -= u_w[j]\n",
        "          self.fw_network[j].bias -= u_b[j]\n",
        "\n",
        "          lookleap_network[j].weight -= (self.eta * self.bp_network[j].d_w + self.beta * u_w[j])\n",
        "          lookleap_network[j].bias -= (self.eta * self.bp_network[j].d_b + self.beta * u_b[j])\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 4)\n",
        "        train_loss = round(loss , 4)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 4)\n",
        "          val_loss = round(v_loss , 4)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        # wandb.log({'epoch': self.epochs,  # Use current epoch, not total epochs\n",
        "        #             'validation_accuracy': val_accuracy if self.validX_train is not None else 0,\n",
        "        #             'accuracy': accuracy,\n",
        "        #             'validation_loss': val_loss if self.validX_train is not None else 0,\n",
        "        #             'loss': train_loss\n",
        "        #         })\n",
        "      print(f\"   Epochs no: {(epoch+1)}  |  Train acc: {accuracy}  |  val acc: {val_accuracy}  |  Train Loss: {train_loss} |  Val Loss : {val_loss}\")\n",
        "\n",
        "    return self.init_network, train_loss, accuracy, val_accuracy, val_loss\n",
        "\n",
        "\n",
        "\n",
        "class RMSProp(Optimizer):\n",
        "\n",
        "  def rms_GD(self):\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "    L = len(self.n) - 1\n",
        "    # Initialize variables neesed to keep track of loss\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "    val_loss = 0\n",
        "    u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "    for epoch in tqdm(range(self.epochs)):\n",
        "\n",
        "      for batch in range(self.batches_number):\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:  # Ensure we have enough data points\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)  # Compute standard deviation of recent losses\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "\n",
        "        for j in range(L):\n",
        "          u_w[j] = u_w[j] * self.beta +  (1  - self.beta) * (self.bp_network[j].d_w)**2\n",
        "          u_b[j] = u_b[j] * self.beta +  (1  - self.beta) * (self.bp_network[j].d_b)**2\n",
        "\n",
        "          self.fw_network[j].weight -= (self.eta / np.sqrt(u_w[j] + self.epsilon)) * self.bp_network[j].d_w\n",
        "          self.fw_network[j].bias -= (self.eta / np.sqrt(u_b[j] + self.epsilon)) * self.bp_network[j].d_b\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 4)\n",
        "        train_loss = round(loss , 4)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 4)\n",
        "          val_loss = round(v_loss , 4)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        # wandb.log({'epoch': self.epochs,  # Use current epoch, not total epochs\n",
        "        #             'validation_accuracy': val_accuracy if self.validX_train is not None else 0,\n",
        "        #             'accuracy': accuracy,\n",
        "        #             'validation_loss': val_loss if self.validX_train is not None else 0,\n",
        "        #             'loss': train_loss\n",
        "        #         })\n",
        "      print(f\"   Epochs no: {(epoch+1)}  |  Train acc: {accuracy}  |  val acc: {val_accuracy}  |  Train Loss: {train_loss} |  Val Loss : {val_loss}\")\n",
        "\n",
        "    return self.init_network, train_loss, accuracy, val_accuracy, val_loss\n",
        "\n",
        "\n",
        "class ADAM(Optimizer):\n",
        "\n",
        "  def adam_GD(self):\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "    L = len(self.n) - 1\n",
        "    # Initialize variables neesed to keep track of loss\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    self.f_network = []\n",
        "    overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    i = 0\n",
        "    u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "    v_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    v_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "    for epoch in tqdm(range(self.epochs)):\n",
        "\n",
        "\n",
        "      for batch in range(self.batches_number):\n",
        "\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:  # Ensure we have enough data points\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)  # Compute standard deviation of recent losses\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "\n",
        "        for j in range(L):\n",
        "          u_w[j] = u_w[j] * self.beta +  (1  - self.beta) * (self.bp_network[j].d_w)\n",
        "          u_b[j] = u_b[j] * self.beta +  (1  - self.beta) * (self.bp_network[j].d_b)\n",
        "          v_w[j] = v_w[j] * self.beta2 +  (1  - self.beta2) * (self.bp_network[j].d_w)**2\n",
        "          v_b[j] = v_b[j] * self.beta2 +  (1  - self.beta2) * (self.bp_network[j].d_b)**2\n",
        "\n",
        "          u_w_pred = u_w[j] / (1 - np.power(self.beta , i+1))\n",
        "          u_b_pred = u_b[j] / (1 - np.power(self.beta , i+1))\n",
        "          v_w_pred = v_w[j] / (1 - np.power(self.beta2 , i+1))\n",
        "          v_b_pred = v_b[j] / (1 - np.power(self.beta2 , i+1))\n",
        "\n",
        "          self.fw_network[j].weight -= (self.eta / (np.sqrt(v_w_pred) + self.epsilon)) * u_w_pred\n",
        "          self.fw_network[j].bias -= (self.eta / (np.sqrt(v_b_pred) + self.epsilon)) * u_b_pred\n",
        "\n",
        "        self.init_network = self.fw_network\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 5)\n",
        "        train_loss = round(loss , 5)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "        i+=1\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 5)\n",
        "          val_loss = round(v_loss , 5)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        # wandb.log({'epoch': self.epochs,  # Use current epoch, not total epochs\n",
        "        #             'validation_accuracy': val_accuracy if self.validX_train is not None else 0,\n",
        "        #             'accuracy': accuracy,\n",
        "        #             'validation_loss': val_loss if self.validX_train is not None else 0,\n",
        "        #             'loss': train_loss\n",
        "        #         })\n",
        "      print(f\"   Epochs no: {(epoch+1)}  |  Train acc: {accuracy}  |  val acc: {val_accuracy}  |  Train Loss: {train_loss} |  Val Loss : {val_loss}\")\n",
        "\n",
        "    return self.init_network, train_loss, accuracy, val_accuracy, val_loss\n",
        "\n",
        "\n",
        "class GiveOptimizers(Optimizer):\n",
        "    def __init__(self, optimization_fn, loss_function, X_train, y_train, activation_fn, layers_dimensions ,method, batch_size, epochs,  validX_train , validy_train ,weight_decay = 0, eta = 0.01, beta = 0.9, beta2 = 0.999, regularization_fn = \"L2\",grad_reglr_fn = \"L2_d\"):\n",
        "      self.optimization_function = optimization_fn.lower()\n",
        "      super().__init__( loss_function, X_train, y_train, activation_fn, layers_dimensions ,method, batch_size, epochs, validX_train, validy_train ,weight_decay = 0, eta = 0.01, beta = 0.9, beta2 = 0.999, regularization_fn = \"L2\",grad_reglr_fn = \"L2_d\" )\n",
        "\n",
        "    def apply_optimization(self):\n",
        "      if self.optimization_function == 'sgd':\n",
        "          return SGD(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs,  self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn).Gradient_descent()\n",
        "      elif self.optimization_function == 'momentum':\n",
        "          return MGD(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn).momentum_GD()\n",
        "      elif self.optimization_function == 'nag':\n",
        "          return NAG(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn).Nesterov_AGD()\n",
        "      elif self.optimization_function == 'rmsp':\n",
        "          return RMSProp(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn).rms_GD()\n",
        "      elif self.optimization_function == 'adam':\n",
        "          return ADAM(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn).adam_GD()"
      ],
      "metadata": {
        "id": "876QHHBCC54L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, loss_function, X_train, y_train, activation_fn, optimization_fn,\n",
        "                 layers_dimensions, method, batch_size, epochs, val_ratio,\n",
        "                 weight_decay=0, eta=0.01, beta=0.9, beta2=0.999,\n",
        "                 regularization_fn=\"L2\", grad_reglr_fn=\"L2_d\"):\n",
        "\n",
        "        self.loss_function = loss_function\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.activation_fn = activation_fn\n",
        "        self.method = method\n",
        "        self.n = layers_dimensions\n",
        "        self.optimization_fn = optimization_fn\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.val_ratio = val_ratio\n",
        "        self.weight_decay = weight_decay\n",
        "        self.eta = eta\n",
        "        self.epsilon = 1e-10\n",
        "        self.beta = beta\n",
        "        self.beta2 = beta2\n",
        "        self.regularization_fn = regularization_fn\n",
        "        self.grad_reglr_fn = grad_reglr_fn\n",
        "        self.batches_number = max(1, self.X_train.shape[1] // self.batch_size)\n",
        "\n",
        "\n",
        "        # Splitting data\n",
        "        X_split, y_split, valX_split, valy_split = TrainValSplit(self.X_train, self.y_train, self.val_ratio).Apply_split()\n",
        "\n",
        "        # Normalization & Encoding\n",
        "        self.X_norm = Normalize(X_split).Norm_reshape()\n",
        "        self.y_norm = OneHotEncoder(X_split, y_split).onehot_encode()\n",
        "        self.valX_norm = Normalize(valX_split).Norm_reshape()\n",
        "        self.valy_norm = OneHotEncoder(valX_split, valy_split).onehot_encode()\n",
        "\n",
        "        # Ensure correct dimensions\n",
        "        if self.X_norm.shape[1] != self.y_norm.shape[1]:\n",
        "            raise ValueError(f\"Mismatch in training data shapes: {self.X_norm.shape} vs {self.y_norm.shape}\")\n",
        "\n",
        "        if self.valy_norm is not None and self.valX_norm.shape[1] != self.valy_norm.shape[1]:\n",
        "            raise ValueError(f\"Mismatch in validation data shapes: {self.valX_norm.shape} vs {self.valy_norm.shape}\")\n",
        "\n",
        "    def trainNN(self):\n",
        "        NNModel = GiveOptimizers(\n",
        "            self.optimization_fn, self.loss_function, self.X_norm, self.y_norm,\n",
        "            self.activation_fn, self.n, self.method, self.batch_size, self.epochs,\n",
        "            self.valX_norm, self.valy_norm, self.weight_decay, self.eta,\n",
        "            self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn\n",
        "        ).apply_optimization()\n",
        "\n",
        "        if NNModel is None:\n",
        "            raise ValueError(\"Model initialization failed. `GiveOptimizers.apply_optimization()` returned None.\")\n",
        "\n",
        "        if not isinstance(NNModel, (list, tuple)) or len(NNModel) < 5:\n",
        "            raise ValueError(f\"Unexpected NNModel structure: {type(NNModel)} with length {len(NNModel)}\")\n",
        "\n",
        "        final_network = NNModel[0]\n",
        "        overall_loss = NNModel[1]\n",
        "        accuracy = NNModel[2]\n",
        "        val_loss = NNModel[3]\n",
        "        val_acc = NNModel[4]\n",
        "\n",
        "        return final_network, overall_loss, accuracy, val_loss, val_acc\n"
      ],
      "metadata": {
        "id": "LE7CQHssD72f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "class NNTrainer:\n",
        "    def __init__(self, X_train, y_train, num_hidden_layers, hidden_layer_sizes, activation_fns,\n",
        "                 optimizer, batch_size, learning_rate, epochs, weight_init, loss_fn,\n",
        "                 reg_type=\"L2\", weight_decay=0, val_ratio=0.2, method=\"default\",\n",
        "                 beta=0.9, beta2=0.999,\n",
        "                 X_test=None, y_test=None):\n",
        "        \"\"\"\n",
        "        Wrapper class to simplify training a NeuralNetwork model.\n",
        "        Now it dynamically constructs the layer dimensions and activation functions.\n",
        "        \"\"\"\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "        self.activation_fns = activation_fns\n",
        "        self.optimizer = optimizer\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weight_init = weight_init\n",
        "        self.loss_fn = loss_fn\n",
        "        self.reg_type = reg_type\n",
        "        self.weight_decay = weight_decay\n",
        "        self.val_ratio = val_ratio\n",
        "        self.method = method\n",
        "        self.beta = beta\n",
        "        self.beta2 = beta2\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "        # Automatically construct layer dimensions\n",
        "        self.layers_dimensions = self._construct_layers()\n",
        "        self.activation_fn = self._construct_activations()\n",
        "\n",
        "        # Data Preprocessing\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _construct_layers(self):\n",
        "        \"\"\"Constructs layer dimensions based on the number of hidden layers and their sizes.\"\"\"\n",
        "        input_size = 784  # Fashion-MNIST input size\n",
        "        output_size = 10   # Number of classes in Fashion-MNIST\n",
        "        return [input_size] + self.hidden_layer_sizes[:self.num_hidden_layers] + [output_size]\n",
        "\n",
        "    def _construct_activations(self):\n",
        "        \"\"\"Constructs activation functions for each hidden layer, ending with softmax for classification.\"\"\"\n",
        "        return self.activation_fns[:self.num_hidden_layers] + [\"softmax\"]\n",
        "\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        \"\"\"Splits and normalizes train/validation/test data.\"\"\"\n",
        "        self.X_train, self.y_train, self.X_val, self.y_val = TrainValSplit(\n",
        "            self.X_train, self.y_train, self.val_ratio).Apply_split()\n",
        "\n",
        "        self.X_norm = Normalize(self.X_train).Norm_reshape()\n",
        "        self.y_norm = OneHotEncoder(self.X_train, self.y_train).onehot_encode()\n",
        "        self.X_val_norm = Normalize(self.X_val).Norm_reshape()\n",
        "        self.y_val_norm = OneHotEncoder(self.X_val, self.y_val).onehot_encode()\n",
        "\n",
        "        if self.X_test is not None and self.y_test is not None:\n",
        "            self.X_test_norm = Normalize(self.X_test).Norm_reshape()\n",
        "            self.y_test_norm = OneHotEncoder(self.X_test, self.y_test).onehot_encode()\n",
        "        else:\n",
        "            self.X_test_norm = None\n",
        "            self.y_test_norm = None\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Trains the neural network and returns train/val/test metrics.\"\"\"\n",
        "        model = NeuralNetwork(\n",
        "            loss_function=self.loss_fn,\n",
        "            X_train=self.X_train, y_train=self.y_train,\n",
        "            activation_fn=self.activation_fn,\n",
        "            optimization_fn=self.optimizer,\n",
        "            layers_dimensions=self.layers_dimensions,\n",
        "            method=self.method, batch_size=self.batch_size,\n",
        "            epochs=self.epochs, val_ratio=self.val_ratio,\n",
        "            weight_decay=self.weight_decay, eta=self.learning_rate,\n",
        "            beta=self.beta, beta2=self.beta2,\n",
        "            regularization_fn=self.reg_type,\n",
        "            grad_reglr_fn=f\"{self.reg_type}_d\",\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        final_network, overall_loss, train_accuracy, val_loss, val_accuracy = model.trainNN()\n",
        "\n",
        "        # If test data is provided, evaluate on test set\n",
        "        test_loss, test_accuracy = None, None\n",
        "        if self.X_test_norm is not None and self.y_test_norm is not None:\n",
        "            y_test_predicted = Feedforward(self.X_test_norm, self.activation_fn, self.method, final_network).Forward_prop()\n",
        "            y_test_pred = np.array(y_test_predicted[-1].h)\n",
        "            loss = callloss(self.loss_fn, self.y_test_norm, y_test_pred).give_loss()\n",
        "            test_loss_obj = CalculateAllLoss(X_train=self.X_test_norm, y_predicted=y_test_pred, network=final_network,\n",
        "                                             y_train=self.y_test_norm, primary_loss=loss, weight_decay=self.weight_decay,\n",
        "                                             regularisation_fn=self.reg_type)\n",
        "            test_accuracy, test_loss = test_loss_obj.calc_accuracy_loss()\n",
        "\n",
        "        return overall_loss, train_accuracy, val_loss, val_accuracy, test_loss, test_accuracy\n"
      ],
      "metadata": {
        "id": "Ho8aJ4PtD_uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Below the trainer function is used to run the whole model properly also as asked in the the Question-3 , the model is flexible enough to take varity of hyperparameters which canbe easily changed.**"
      ],
      "metadata": {
        "id": "iHO4rZCNEL7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import FALSE\n",
        "trainer = NNTrainer(\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    num_hidden_layers=5,  # Specify the number of hidden layers\n",
        "    hidden_layer_sizes=[128, 64, 32],  # Specify possible sizes per hidden layer\n",
        "    activation_fns=[\"tanh\", \"sigmoid\", \"tanh\"],  # Specify possible activations\n",
        "    optimizer=\"adam\",\n",
        "    batch_size=10000,\n",
        "    learning_rate=0.01,\n",
        "    epochs=5,\n",
        "    weight_init=\"Xavier_U\",\n",
        "    loss_fn=\"ce\",\n",
        "    reg_type=\"L2\",\n",
        "    weight_decay=0.0005,\n",
        "    val_ratio=0.1,\n",
        "    method=\"Xavier_U\",\n",
        "    beta=0.9,\n",
        "    beta2=0.999,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test\n",
        ")\n",
        "\n",
        "train_loss, train_acc, val_loss, val_acc, test_loss, test_acc = trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aILEkRJDEFOG",
        "outputId": "4a7cd2ab-9e6a-4c46-8e0a-cfee4414f6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [00:02<00:09,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Epochs no: 1  |  Train acc: 0.6086  |  val acc: 0.63444  |  Train Loss: 0.13577 |  Val Loss : 0.13577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:06<00:09,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Epochs no: 2  |  Train acc: 0.6699  |  val acc: 0.66259  |  Train Loss: 0.12993 |  Val Loss : 0.12993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [00:08<00:05,  2.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Epochs no: 3  |  Train acc: 0.6628  |  val acc: 0.66574  |  Train Loss: 0.13456 |  Val Loss : 0.13456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [00:10<00:02,  2.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Epochs no: 4  |  Train acc: 0.6636  |  val acc: 0.66444  |  Train Loss: 0.13843 |  Val Loss : 0.13843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:12<00:00,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Epochs no: 5  |  Train acc: 0.6636  |  val acc: 0.66074  |  Train Loss: 0.14107 |  Val Loss : 0.14107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "0sS5Hg07DpxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Question 4 :\n",
        "\n",
        "##Use the standard train/test split of fashion_mnist (use (X_train, y_tr(X_test, y_test) = fashion_mnist.load_data()).\n",
        "##Keep 10% of the training data aside as validation data for this hyperparameter search. Here are some suggestions for different values to try for hyperparameters.\n",
        "##As you can quickly see that this leads to an exponential number of combinations. You will have to think about strategies to do this hyperparameter search efficiently.\n",
        "##Check out the options provided by wandb.sweep and write down what strategy you chose and why.\n",
        "\n",
        "##number of epochs: 5, 10\n",
        "##number of hidden layers: 3, 4, 5\n",
        "##size of every hidden layer: 32, 64, 128\n",
        "##weight decay (L2 regularisation): 0, 0.0005, 0.5\n",
        "##learning rate: 1e-3, 1 e-4\n",
        "##optimizer: sgd, momentum, nesterov, rmsprop, adam, nadam\n",
        "##batch size: 16, 32, 64\n",
        "##weight initialisation: random, Xavier\n",
        "##activation functions: sigmoid, tanh, ReLU\n",
        "##wandb will automatically generate the following plots.\n",
        "##Paste these plots below using the \"Add Panel to Report\" feature.\n",
        "##Make sure you use meaningful names for each sweep (e.g. hl_3_bs_16_ac_tanh to indicate that there were 3 hidden layers, batch size was 16 and activation function was ReLU)\n",
        "##instead of using the default names (whole-sweep, kind-sweep) given by wandb."
      ],
      "metadata": {
        "id": "6WFF4wSQGiDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "AldeNbMPHtYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing the TrainValSplit, OneHotEncoder, Normalize classes to preprocess the data :**"
      ],
      "metadata": {
        "id": "3Rv-_i_PHQfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trying different hyperparametrs to tune my model"
      ],
      "metadata": {
        "id": "dZvV1jFhMHFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sweep configuration and sweep wrapper**"
      ],
      "metadata": {
        "id": "14roDDEYB-QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#  Sweep configuration matching all provided parameters:\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "        'name': 'val_accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {'values': [5, 10]},\n",
        "        'num_hidden_layers': {'values': [3, 4, 5]},\n",
        "        'hidden_layer_size': {'values': [32, 64, 128]},\n",
        "        'weight_decay': {'values': [0, 0.0005, 0.5]},\n",
        "        'learning_rate': {'values': [ 0.001, 0.0001]},  # Match large range from your example\n",
        "        'optimizer': {'values': [ \"sgd\", 'momentum', 'nag', 'rmsp', 'adam']},\n",
        "        'batch_size': {'values': [16, 32, 64]},  # Your example used 1000; also add variations\n",
        "        'weight_init': {'values': ['Random', 'Xavier_U']},\n",
        "        'activation_fn': {'values': ['sigmoid', 'tanh', 'relu']}\n",
        "    }\n",
        "}\n",
        "\n",
        "#  Sweep training function using correct call signature:\n",
        "def sweep_train():\n",
        "    wandb.init()\n",
        "    config = wandb.config\n",
        "\n",
        "    # Create hidden layers and activation lists dynamically\n",
        "    hidden_sizes = [config.hidden_layer_size] * config.num_hidden_layers\n",
        "    activations = [config.activation_fn] * config.num_hidden_layers\n",
        "\n",
        "    # Proper run name:\n",
        "    wandb.run.name = f\"hl_{config.num_hidden_layers}_bs_{config.batch_size}_ac_{config.activation_fn}\"\n",
        "\n",
        "    # Call trainer with your exact parameters and sweep values:\n",
        "    trainer = NNTrainer(\n",
        "        X_train=X_train,\n",
        "        y_train=y_train,\n",
        "        num_hidden_layers=config.num_hidden_layers,\n",
        "        hidden_layer_sizes=hidden_sizes,\n",
        "        activation_fns=activations,\n",
        "        optimizer=config.optimizer,\n",
        "        batch_size=config.batch_size,\n",
        "        learning_rate=config.learning_rate,\n",
        "        epochs=config.epochs,\n",
        "        weight_init=config.weight_init,\n",
        "        loss_fn=\"ce\",\n",
        "        reg_type=\"L2\",\n",
        "        weight_decay=config.weight_decay,\n",
        "        val_ratio=0.1,\n",
        "        method=config.weight_init,  # Use same value as weight_init for consistency\n",
        "        beta=0.9,\n",
        "        beta2=0.999,\n",
        "        X_test=X_test,\n",
        "        y_test=y_test\n",
        "    )\n",
        "\n",
        "    train_loss, train_acc, val_loss, val_acc, test_loss, test_acc = trainer.train()\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,\n",
        "        'train_accuracy': train_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'val_accuracy': val_acc,\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': test_acc\n",
        "    })\n",
        "\n",
        "# Start the sweep:\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"DL_Assignment_1\")\n",
        "wandb.agent(sweep_id, function=sweep_train, count = 200)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "WDqZpUT8B3yV",
        "outputId": "d042807d-9a63-4530-87cc-8a0165d6c269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: ls461827\n",
            "Sweep URL: https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/sweeps/ls461827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 99qhdcb0 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: Random\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250321_070504-99qhdcb0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/runs/99qhdcb0' target=\"_blank\">stellar-sweep-1</a></strong> to <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/sweeps/ls461827' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/sweeps/ls461827</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/sweeps/ls461827' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/sweeps/ls461827</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/runs/99qhdcb0' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/runs/99qhdcb0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "NQbJPgNaB3v-",
        "outputId": "88af9368-231b-41f7-8e8c-18a3af6e466f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">hl_3_bs_16_ac_relu</strong> at: <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/runs/99qhdcb0' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/runs/99qhdcb0</a><br> View project at: <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250321_070504-99qhdcb0/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O0JfqdrL_PPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "c7GtcsTv_QBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Question 5 & Question 6 : Answer of these two questions are in wandb.ai site report. The link is given below : -"
      ],
      "metadata": {
        "id": "UaQorKeJ_PyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "ofaag5hu_PoT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YwUcSUek_N3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "IYjtc3tceZ3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ✅ Question 7:\n",
        "## For the best model identified above, report the accuracy on the test set of fashion_mnist and plot the confusion matrix as shown below. More marks for creativity (less marks for producing the plot shown below as it is)"
      ],
      "metadata": {
        "id": "0-deUFJ4eZPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "SP_YLs2FeY-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix class**"
      ],
      "metadata": {
        "id": "GH44GWYDCQH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import wandb  # optional, for logging\n",
        "\n",
        "class plot_confusion_matrix:\n",
        "    def __init__(self, X_train, y_train, num_hidden_layers, hidden_layer_sizes, activation_fns,\n",
        "                 optimizer, batch_size, learning_rate, epochs, weight_init, loss_fn,\n",
        "                 reg_type=\"L2\", weight_decay=0, val_ratio=0.2, method=\"default\",\n",
        "                 beta=0.9, beta2=0.999,\n",
        "                 X_test=None, y_test=None):\n",
        "\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "        self.activation_fns = activation_fns\n",
        "        self.optimizer = optimizer\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weight_init = weight_init\n",
        "        self.loss_fn = loss_fn\n",
        "        self.reg_type = reg_type\n",
        "        self.weight_decay = weight_decay\n",
        "        self.val_ratio = val_ratio\n",
        "        self.method = method\n",
        "        self.beta = beta\n",
        "        self.beta2 = beta2\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "        self.layers_dimensions = self._construct_layers()\n",
        "        self.activation_fn = self._construct_activations()\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _construct_layers(self):\n",
        "        input_size = 784  # Fashion-MNIST input size\n",
        "        output_size = 10  # Number of classes\n",
        "        return [input_size] + self.hidden_layer_sizes[:self.num_hidden_layers] + [output_size]\n",
        "\n",
        "    def _construct_activations(self):\n",
        "        return self.activation_fns[:self.num_hidden_layers] + [\"softmax\"]\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        self.X_train, self.y_train, self.X_val, self.y_val = TrainValSplit(\n",
        "            self.X_train, self.y_train, self.val_ratio).Apply_split()\n",
        "\n",
        "        self.X_norm = Normalize(self.X_train).Norm_reshape()\n",
        "        self.y_norm = OneHotEncoder(self.X_train, self.y_train).onehot_encode()\n",
        "        self.X_val_norm = Normalize(self.X_val).Norm_reshape()\n",
        "        self.y_val_norm = OneHotEncoder(self.X_val, self.y_val).onehot_encode()\n",
        "\n",
        "        if self.X_test is not None and self.y_test is not None:\n",
        "            self.X_test_norm = Normalize(self.X_test).Norm_reshape()\n",
        "            self.y_test_norm = OneHotEncoder(self.X_test, self.y_test).onehot_encode()\n",
        "        else:\n",
        "            self.X_test_norm = None\n",
        "            self.y_test_norm = None\n",
        "\n",
        "    def confusion_matrix(self):\n",
        "        model = NeuralNetwork(\n",
        "            loss_function=self.loss_fn,\n",
        "            X_train=self.X_train, y_train=self.y_train,\n",
        "            activation_fn=self.activation_fn,\n",
        "            optimization_fn=self.optimizer,\n",
        "            layers_dimensions=self.layers_dimensions,\n",
        "            method=self.method, batch_size=self.batch_size,\n",
        "            epochs=self.epochs, val_ratio=self.val_ratio,\n",
        "            weight_decay=self.weight_decay, eta=self.learning_rate,\n",
        "            beta=self.beta, beta2=self.beta2,\n",
        "            regularization_fn=self.reg_type,\n",
        "            grad_reglr_fn=f\"{self.reg_type}_d\",\n",
        "        )\n",
        "\n",
        "        final_network, _, train_acc, _, val_acc = model.trainNN()\n",
        "\n",
        "        if self.X_test_norm is None or self.y_test_norm is None:\n",
        "            print(\"Test data not provided. Cannot compute confusion matrix.\")\n",
        "            return\n",
        "\n",
        "        # Predict\n",
        "        y_test_predicted = Feedforward(self.X_test_norm, self.activation_fn, self.method, final_network).Forward_prop()\n",
        "        y_test_pred = np.array(y_test_predicted[-1].h)\n",
        "\n",
        "        y_true = np.argmax(self.y_test_norm, axis=0)\n",
        "        y_pred = np.argmax(y_test_pred, axis=0)\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "        # Prepare plot\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        cmap = sn.light_palette(\"red\", as_cmap=True)\n",
        "\n",
        "        sn.heatmap(cm_normalized, annot=False, fmt=\".2f\", cmap=cmap,\n",
        "                   cbar=False, square=True, linewidths=0.5, ax=ax)\n",
        "\n",
        "        for i in range(cm.shape[0]):\n",
        "            ax.add_patch(plt.Rectangle((i, i), 1, 1, fill=False, edgecolor='lime', lw=3))\n",
        "\n",
        "        ax.set_xlabel(\"y_pred\", fontsize=16)\n",
        "        ax.set_ylabel(\"y_true\", fontsize=16)\n",
        "        ax.set_title(\"Confusion matrix\", fontsize=18, color=\"royalblue\", pad=20)\n",
        "\n",
        "        ax.xaxis.set_ticks_position('none')\n",
        "        ax.yaxis.set_ticks_position('none')\n",
        "        ax.tick_params(axis='x', labelrotation=45)\n",
        "\n",
        "        try:\n",
        "            ax.set_xticklabels([IMG_LABELS[i] for i in range(cm.shape[1])])\n",
        "            ax.set_yticklabels([IMG_LABELS[i] for i in range(cm.shape[0])])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        try:\n",
        "            wandb.log({'Confusion Matrix': wandb.Image(plt, caption=\"Creative Confusion Matrix\")})\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "ftflKo9OB3tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Below is the calling function to plot the Confusion Matrix also the plot will be logged into the wandb account**"
      ],
      "metadata": {
        "id": "Tn7bFXL2n2AT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "run = wandb.init(project=\"DA6401_Q7\", reinit=True)\n",
        "plot = plot_confusion_matrix(\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    num_hidden_layers=5,  # Specify the number of hidden layers\n",
        "    hidden_layer_sizes=[128, 64, 32],  # Specify possible sizes per hidden layer\n",
        "    activation_fns=[\"tanh\", \"sigmoid\", \"tanh\"],  # Specify possible activations\n",
        "    optimizer=\"adam\",\n",
        "    batch_size=10000,\n",
        "    learning_rate=0.01,\n",
        "    epochs=5,\n",
        "    weight_init=\"Xavier_U\",\n",
        "    loss_fn=\"ce\",\n",
        "    reg_type=\"L2\",\n",
        "    weight_decay=0.0005,\n",
        "    val_ratio=0.1,\n",
        "    method=\"Xavier_U\",\n",
        "    beta=0.9,\n",
        "    beta2=0.999,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test\n",
        ")\n",
        "plot.confusion_matrix()\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QBLndpn3j77p",
        "outputId": "b2f520d8-e067-40d2-e3c4-e2180e1991a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DA6401_Q7' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250321_073941-99qhdcb0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/runs/99qhdcb0' target=\"_blank\">hl_3_bs_16_ac_relu</a></strong> to <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/sweeps/ls461827' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/sweeps/ls461827</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/sweeps/ls461827' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/sweeps/ls461827</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/runs/99qhdcb0' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/runs/99qhdcb0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:03<00:15,  3.98s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Epochs no: 1  |  Train acc: 0.6086  |  val acc: 0.63444  |  Train Loss: 0.13577 |  Val Loss : 0.13577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 40%|████      | 2/5 [00:08<00:12,  4.18s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Epochs no: 2  |  Train acc: 0.6699  |  val acc: 0.66259  |  Train Loss: 0.12993 |  Val Loss : 0.12993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 60%|██████    | 3/5 [00:14<00:10,  5.30s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Epochs no: 3  |  Train acc: 0.6628  |  val acc: 0.66574  |  Train Loss: 0.13456 |  Val Loss : 0.13456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 80%|████████  | 4/5 [00:19<00:04,  4.84s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Epochs no: 4  |  Train acc: 0.6636  |  val acc: 0.66444  |  Train Loss: 0.13843 |  Val Loss : 0.13843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 5/5 [00:22<00:00,  4.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Epochs no: 5  |  Train acc: 0.6636  |  val acc: 0.66074  |  Train Loss: 0.14107 |  Val Loss : 0.14107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">hl_3_bs_16_ac_relu</strong> at: <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/runs/99qhdcb0' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1/runs/99qhdcb0</a><br> View project at: <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_1</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250321_073941-99qhdcb0/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "92sXduYXs698"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ✅ Question 8 :\n",
        "## In all the models above you would have used cross entropy loss. Now compare the cross entropy loss with the squared error loss. I would again like to see some automatically generated plots or your own plots to convince me whether one is better than the other."
      ],
      "metadata": {
        "id": "KYZ-_8-xp4dD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "_hbwj0qms8dM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"Cross-entropy\" and \"Square-loss\" is added in sweep for both loss comparision**"
      ],
      "metadata": {
        "id": "s027Q7Ko95Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new parameter for loss function to the sweep config\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "        'name': 'val_accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {'values': [5, 10]},\n",
        "        'num_hidden_layers': {'values': [3, 4, 5]},\n",
        "        'hidden_layer_size': {'values': [32, 64, 128]},\n",
        "        'weight_decay': {'values': [0, 0.0005, 0.5]},\n",
        "        'learning_rate': {'values': [0.001, 0.0001]},\n",
        "        'optimizer': {'values': [\"sgd\", 'momentum', 'nag', 'rmsp', 'adam']},\n",
        "        'batch_size': {'values': [1000, 2000, 5000]},\n",
        "        'weight_init': {'values': ['Random', 'Xavier_U']},\n",
        "        'activation_fn': {'values': ['sigmoid', 'tanh', 'relu']},\n",
        "        'loss_fn': {'values': ['ce', 'se']}  # <-- Added for comparison\n",
        "    }\n",
        "}\n",
        "\n",
        "# Training function using the chosen loss function\n",
        "def sweep_train():\n",
        "    wandb.init()\n",
        "    config = wandb.config\n",
        "\n",
        "    hidden_sizes = [config.hidden_layer_size] * config.num_hidden_layers\n",
        "    activations = [config.activation_fn] * config.num_hidden_layers\n",
        "\n",
        "    loss_name = \"ce\" if config.loss_fn == \"ce\" else \"se\"\n",
        "    wandb.run.name = f\"{loss_name}_hl_{config.num_hidden_layers}_bs_{config.batch_size}_ac_{config.activation_fn}\"\n",
        "\n",
        "    trainer = NNTrainer(\n",
        "        X_train=X_train,\n",
        "        y_train=y_train,\n",
        "        num_hidden_layers=config.num_hidden_layers,\n",
        "        hidden_layer_sizes=hidden_sizes,\n",
        "        activation_fns=activations,\n",
        "        optimizer=config.optimizer,\n",
        "        batch_size=config.batch_size,\n",
        "        learning_rate=config.learning_rate,\n",
        "        epochs=config.epochs,\n",
        "        weight_init=config.weight_init,\n",
        "        loss_fn=loss_name,\n",
        "        reg_type=\"L2\",\n",
        "        weight_decay=config.weight_decay,\n",
        "        val_ratio=0.1,\n",
        "        method=config.weight_init,\n",
        "        beta=0.9,\n",
        "        beta2=0.999,\n",
        "        X_test=X_test,\n",
        "        y_test=y_test\n",
        "    )\n",
        "\n",
        "    train_loss, train_acc, val_loss, val_acc, test_loss, test_acc = trainer.train()\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,\n",
        "        'train_accuracy': train_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'val_accuracy': val_acc,\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': test_acc,\n",
        "        'loss_fn': config.loss_fn\n",
        "    })\n",
        "\n",
        "# Launch sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"DL_Assignment_8\")\n",
        "wandb.agent(sweep_id, function=sweep_train, count=200)\n"
      ],
      "metadata": {
        "id": "9XxF5W9fpvjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "4_Glot9zsDPs",
        "outputId": "59e3a49c-363f-4a0e-e9e4-abddd95fe602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">se_hl_4_bs_1000_ac_sigmoid</strong> at: <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_8/runs/xbkpjw66' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_8/runs/xbkpjw66</a><br> View project at: <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_8' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/DL_Assignment_8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250321_075451-xbkpjw66/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Below function will plot the comparesion between \"Squared_error\" and \"Cross_entropy\" and it can be seen observed in wandb also**"
      ],
      "metadata": {
        "id": "_7-dj-2cs-_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "api = wandb.Api()\n",
        "runs = api.runs(\"DL_Assignment_8\")\n",
        "\n",
        "# Example: comparing final validation accuracy\n",
        "data = []\n",
        "for run in runs:\n",
        "    config = run.config\n",
        "    val_acc = run.summary.get(\"val_accuracy\")\n",
        "    loss_fn = config.get(\"loss_fn\")\n",
        "    data.append((loss_fn, val_acc))\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"Loss Function\", \"Validation Accuracy\"])\n",
        "sns.boxplot(x=\"Loss Function\", y=\"Validation Accuracy\", data=df)\n",
        "plt.title(\"Cross Entropy vs Squared Error on Validation Accuracy\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QY5Chq9eB3qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "-4nXhJQ8tuJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ✅ Question 9 :\n",
        "## Paste a link to your github code for this assignment\n",
        "\n",
        "## We will check for coding style, clarity in using functions and a README file with clear instructions on training and evaluating the model (the 10 marks will be based on this)\n",
        "\n",
        "## We will also run a plagiarism check to ensure that the code is not copied (0 marks in the assignment if we find that the code is plagiarized)\n",
        "\n",
        "## We will also check if the training and test data has been split properly and randomly. You will get 0 marks on the assignment if we find any cheating (e.g., adding test data to training data) to get higher accuracy"
      ],
      "metadata": {
        "id": "dqWjWR22tz5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "TK0tQFPktvcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All my observations are in README.md file in Github. The link of my Github account is given below**"
      ],
      "metadata": {
        "id": "z8-QdBFjST4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Github Link : https://github.com/indramandal85/DA6401_Assignment_1.git"
      ],
      "metadata": {
        "id": "kOo8KFWwX-Da"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "MGg5N8agvBYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Question 10 :\n",
        "## Based on your learnings above, give me 3 recommendations for what would work for the MNIST dataset (not Fashion-MNIST). Just to be clear, I am asking you to take your learnings based on extensive experimentation with one dataset and see if these learnings help on another dataset. If I give you a budget of running only 3 hyperparameter configurations as opposed to the large number of experiments you have run above then which 3 would you use and why. Report the accuracies that you obtain using these 3 configurations."
      ],
      "metadata": {
        "id": "YmOyeKkNvEsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "PwrOF7COvFIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In README.md**"
      ],
      "metadata": {
        "id": "dbsPC0Acj08b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3S6uSMzFu1eN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}