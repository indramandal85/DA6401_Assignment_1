{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "20588c62-fc21-4f54-a735-6a5423f7c691",
      "metadata": {
        "id": "20588c62-fc21-4f54-a735-6a5423f7c691"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class CalActivation:\n",
        "\n",
        "  def __init__(self, m):\n",
        "    self.m = m\n",
        "\n",
        "# This function calculates sigmoid activation function\n",
        "class sigmoid(CalActivation):\n",
        "\n",
        "    def use_sigmoid(self):\n",
        "        sig = np.where(self.m >= 0,1/(1 - np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) - 1))\n",
        "        return sig\n",
        "\n",
        "    def sigmoid_d(self):\n",
        "        sig_d = self.use_sigmoid()  # Compute sigmoid\n",
        "        # sig_d = np.where(self.m >= 0,1/(1 + np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) + 1))\n",
        "        return sig_d * (1 - sig_d)\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates tanh activation function\n",
        "class tanh(CalActivation):\n",
        "\n",
        "    def use_tanh(self):\n",
        "        z = (np.exp(self.m) - np.exp(-self.m))/(np.exp(self.m) + np.exp(-self.m))\n",
        "        return z\n",
        "\n",
        "    def tanh_d(self):\n",
        "        z = self.use_tanh()\n",
        "        return (1 - (z)**2)\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates relu activation function\n",
        "class relu(CalActivation):\n",
        "\n",
        "    def use_relu(self):\n",
        "        return np.where(self.m > 0, self.m, 0)\n",
        "        #     return self.m\n",
        "        # else:\n",
        "        #     return 0\n",
        "\n",
        "    def relu_d(self):\n",
        "        return np.where(self.m >0 , 1 ,0)\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates softmax activation function\n",
        "class softmax(CalActivation):\n",
        "\n",
        "    def use_softmax(self):\n",
        "        x = np.exp(self.m)/np.sum(np.exp(self.m), axis=0)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def softmax_d(self):\n",
        "        z=self.m - np.max(self.m,axis=0)\n",
        "        soft=np.exp(z)/np.sum(np.exp(z),axis=0)\n",
        "        return soft*(1-soft)\n",
        "\n",
        "\n",
        "\n",
        "# This function call different activation function\n",
        "class apply_activation(CalActivation):\n",
        "\n",
        "    def __init__(self, activation_function, m):\n",
        "        super().__init__(m)\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "    def do_activation(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.m).use_sigmoid()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.m).use_relu()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.m).use_tanh()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.m).use_softmax()\n",
        "\n",
        "\n",
        "\n",
        "class apply_activation_derivative(CalActivation):\n",
        "\n",
        "    def __init__(self, activation_function, m):\n",
        "        super().__init__(m)\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "    def do_activation_derivative(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.m).sigmoid_d()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.m).relu_d()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.m).tanh_d()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.m).softmax_d()\n",
        "\n",
        "        # else:\n",
        "        #   raise ValueError(\"Unknown activation function\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "eb1a9b5e-0b3d-4895-9847-6731e7c74e1c",
      "metadata": {
        "id": "eb1a9b5e-0b3d-4895-9847-6731e7c74e1c",
        "outputId": "9ab591be-e638-4e56-da0b-446665325321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "x = apply_activation('relu',2)\n",
        "x.do_activation()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}