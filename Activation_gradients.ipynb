{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20588c62-fc21-4f54-a735-6a5423f7c691",
      "metadata": {
        "id": "20588c62-fc21-4f54-a735-6a5423f7c691"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This module will computes the activation functions and their gradients for future use them alternatively in our model. here we have sigmoid activation function and its derivative,\n",
        "relu activation and its derivative, Leakyrelu activation function and its derivative and finally softmax activation function and its derivative.\n",
        "one calling function is also there through which we can call then by using a short name.\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class CalActivation:\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "\n",
        "# Sigmoid\n",
        "class sigmoid(CalActivation):\n",
        "    def use_sigmoid(self):\n",
        "        return 1 / (1 + np.exp(-np.clip(self.m, -250, 250)))\n",
        "\n",
        "    def sigmoid_d(self):\n",
        "        sig = self.use_sigmoid()\n",
        "        return sig * (1 - sig)\n",
        "\n",
        "\n",
        "class relu(CalActivation):\n",
        "    def use_relu(self):\n",
        "        return np.maximum(0, self.m)\n",
        "\n",
        "    def relu_d(self):\n",
        "        return np.where(self.m > 0, 1, 0)\n",
        "\n",
        "class Leakyrelu(CalActivation):\n",
        "    def use_relu(self):\n",
        "        return np.maximum(0, self.m)\n",
        "\n",
        "    def relu_d(self):\n",
        "        return np.where(self.m > 0, 1, 0.01)\n",
        "\n",
        "\n",
        "class softmax(CalActivation):\n",
        "    def use_softmax(self):\n",
        "        shifted = self.m - np.max(self.m, axis=0, keepdims=True)\n",
        "        exps = np.exp(shifted)\n",
        "        return exps / np.sum(exps, axis=0, keepdims=True)\n",
        "\n",
        "    def softmax_d(self):\n",
        "        z=self.m - np.max(self.m,axis=0)\n",
        "        soft=np.exp(z)/np.sum(np.exp(z),axis=0)\n",
        "        return soft*(1-soft)\n",
        "\n",
        "class tanh(CalActivation):\n",
        "    def use_tanh(self):\n",
        "        return np.tanh(self.m)  # Built-in is stable\n",
        "\n",
        "    def tanh_d(self):\n",
        "        return 1 - (self.use_tanh() ** 2)\n",
        "\n",
        "\n",
        "\n",
        "# This function call different activation function\n",
        "class apply_activation(CalActivation):\n",
        "\n",
        "    def __init__(self, activation_function, m):\n",
        "        super().__init__(m)\n",
        "        self.activation_function = activation_function.lower()\n",
        "\n",
        "    def do_activation(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.m).use_sigmoid()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.m).use_relu()\n",
        "        elif self.activation_function == 'lrelu':\n",
        "            return Leakyrelu(self.m).use_relu()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.m).use_tanh()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.m).use_softmax()\n",
        "\n",
        "    def do_activation_derivative(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.m).sigmoid_d()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.m).relu_d()\n",
        "        elif self.activation_function == 'lrelu':\n",
        "            return Leakyrelu(self.m).relu_d()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.m).tanh_d()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.m).softmax_d()\n",
        "        else:\n",
        "           raise ValueError(\"Unknown activation function\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sdgij9ujmIsK"
      },
      "id": "sdgij9ujmIsK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}