{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "EuDCYeTFOWhL"
      },
      "id": "EuDCYeTFOWhL",
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([[8,8,1],[7,9,1],[6,10,0],[5,5,0]], columns=['cgpa', 'profile_score', 'placed'])"
      ],
      "metadata": {
        "id": "fJ8rJlerOYGB"
      },
      "id": "fJ8rJlerOYGB",
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['cgpa', 'profile_score']].values[0].reshape(2,1) # Shape(no of features, no. of training example)\n",
        "y = df[['placed']].values[0][0]"
      ],
      "metadata": {
        "id": "9724OEXSOX35"
      },
      "id": "9724OEXSOX35",
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# This function calculates sigmoid activation function\n",
        "class sigmoid:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.use_sigmoid()\n",
        "\n",
        "    def use_sigmoid(self):\n",
        "        if self.m >= 0:\n",
        "            return 1/(1 - np.exp(-self.m))\n",
        "        else:\n",
        "            return np.exp(self.m)/(np.exp(-self.m) - 1)\n",
        "\n",
        "\n",
        "# This function calculates tanh activation function\n",
        "class tanh:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.use_tanh()\n",
        "\n",
        "    def use_tanh(self):\n",
        "        z = (np.exp(self.m) - np.exp(-self.m))/(np.exp(self.m) + np.exp(-self.m))\n",
        "        return z\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates relu activation function\n",
        "class relu:\n",
        "\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "        self.use_relu()\n",
        "\n",
        "    def use_relu(self):\n",
        "        if np.all(self.m > 0):\n",
        "            return self.m\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates softmax activation function\n",
        "class softmax:\n",
        "\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "        self.use_softmax()\n",
        "\n",
        "    def use_softmax(self):\n",
        "        x = np.copy(self.m)\n",
        "        max_exp = np.max(x)\n",
        "\n",
        "        x = np.exp(x - max_exp)\n",
        "        x = x / np.sum(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# This function call different activation function\n",
        "class apply_activation:\n",
        "\n",
        "    def __init__(self, activation_function, value):\n",
        "        self.activation_function = activation_function.lower()\n",
        "        self.value = value\n",
        "        self.do_activation()\n",
        "\n",
        "\n",
        "    def do_activation(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.value).use_sigmoid()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.value).use_relu()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.value).use_tanh()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.value).use_softmax()\n",
        "\n",
        "\n",
        "\n",
        "class sigmoid_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.sigmoid_d()\n",
        "\n",
        "    def sigmoid_d(self):\n",
        "        if self.m >= 0:\n",
        "            sig_d =  1/(1 - np.exp(-self.m))\n",
        "        else:\n",
        "            sig_d = np.exp(self.m)/(np.exp(-self.m) - 1)\n",
        "\n",
        "        return sig_d * (1 - sig_d)\n",
        "\n",
        "\n",
        "\n",
        "class relu_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.relu_d()\n",
        "\n",
        "    def relu_d(self):\n",
        "        if self.m > 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "\n",
        "\n",
        "class tanh_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.tanh_d()\n",
        "\n",
        "    def tanh_d(self):\n",
        "        z = (np.exp(self.m) - np.exp(-self.m))/(np.exp(self.m) + np.exp(-self.m))\n",
        "        return (1 - (z)**2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class apply_activation_derivative:\n",
        "\n",
        "    def __init__(self,activation_function, value):\n",
        "        self.activation_function = activation_function.lower()\n",
        "        self.value = value\n",
        "        self.do_activation_derivative()\n",
        "\n",
        "    def do_activation_derivative(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid_derv(self.value).sigmoid_d()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu_derv(self.value).relu_d()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh_derv(self.value).tanh_d()\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "01ZCcJdxviK6"
      },
      "id": "01ZCcJdxviK6",
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class crossentropy:\n",
        "\n",
        "    def __init__(self, y, y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.give_celoss()\n",
        "\n",
        "    def give_celoss(self):\n",
        "        threshold = 10**(-8)\n",
        "        res = np.sum(y * (-np.log(y_predicted)))\n",
        "        return res\n",
        "\n",
        "class squarderror:\n",
        "\n",
        "    def __init__(self, y, y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.give_seloss()\n",
        "\n",
        "    def give_seloss(self):\n",
        "        res = np.square(y - y_predicted)\n",
        "        return np.sum(res)\n",
        "\n",
        "\n",
        "class callloss:\n",
        "\n",
        "    def __init__(self,loss_function, value1, value2):\n",
        "        self.loss_function = loss_function.lower()\n",
        "        self.value1 = value1\n",
        "        self.value2 = value2\n",
        "        self.give_loss()\n",
        "\n",
        "    def give_loss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return crossentropy(self.value1, self.value2).give_celoss()\n",
        "        if self.loss_function == 'se':\n",
        "            return squarderror(self.value1, self.value2).give_seloss()\n",
        "\n",
        "class crossentropy_grad:\n",
        "\n",
        "    def __init__(self,y ,y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.Give_cegrad()\n",
        "\n",
        "    def Give_cegrad(self):\n",
        "        None\n",
        "\n",
        "\n",
        "class squarederror_grad:\n",
        "\n",
        "    def __init__(self,y , y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.Give_segrad()\n",
        "\n",
        "    def Give_segrad(self):\n",
        "        res = -np.sum(y - y_predicted)\n",
        "        return res\n",
        "\n",
        "class call_lossgrad:\n",
        "\n",
        "    def __init__(self,loss_function, value1, value2):\n",
        "        self.loss_function = loss_function.lower()\n",
        "        self.value1 = value1\n",
        "        self.value2 = value2\n",
        "        self.give_gradloss()\n",
        "\n",
        "    def give_gradloss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return crossentropy_grad(self.value1, self.value2).Give_cegrad()\n",
        "        if self.loss_function == 'se':\n",
        "            return squarederror_grad(self.value1, self.value2).Give_segrad()"
      ],
      "metadata": {
        "id": "wZobAAjyjTVB"
      },
      "id": "wZobAAjyjTVB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "id": "895d572d-2ffd-466a-a260-e57859a6449e",
      "metadata": {
        "id": "895d572d-2ffd-466a-a260-e57859a6449e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Initilize:\n",
        "\n",
        "    def __init__(self, i_size, o_size, method = \"Xavier_U\"):\n",
        "        self.i_size = i_size\n",
        "        self.o_size = o_size\n",
        "        self.w, self.b = self.Init_weight(method)\n",
        "\n",
        "    def Init_weight(self, init_method):\n",
        "\n",
        "\n",
        "        if init_method == \"Xavier_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(1 / self.i_size)\n",
        "          w = np.random.randn(self.o_size,self.i_size)*a\n",
        "\n",
        "        elif init_method == \"Xavier_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / (self.o_size + self.i_size))\n",
        "          w = np.random.uniform((-a), a,(self.o_size, self.i_size))\n",
        "\n",
        "        elif init_method == \"He_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(2 / self.i_size)\n",
        "          w = np.random.randn(self.o_size,self.i_size)*a\n",
        "\n",
        "        elif init_method == \"He_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / self.i_size)\n",
        "          w = np.random.uniform(-a, a, (self.o_size, self.i_size))\n",
        "\n",
        "        elif init_method == \"Random\":\n",
        "          np.random.seed(0)\n",
        "          w = np.random.randn(self.o_size,self.i_size)*0.01\n",
        "\n",
        "        b = np.zeros((self.o_size,1))\n",
        "\n",
        "        return w, b\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Weight_bias:\n",
        "  def __init__(self, layer_dimension, activation_fn, method):\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.method = method\n",
        "\n",
        "  def Init_network(self, layer_dimension):\n",
        "    self.n = layer_dimension\n",
        "    self.network = {}\n",
        "\n",
        "    for i in range(1, len(self.n)):\n",
        "      self.network[\"w\" + str(i)] = Initilize(self.n[i], self.n[i-1], self.method).Init_weight(self.method)[0]\n",
        "      self.network[\"b\" + str(i)] = Initilize(self.n[i], self.n[i-1], self.method).Init_weight(self.method)[1]\n",
        "      self.network [\"h\"+ str(i)] = str(self.activation_fn[i-1])\n",
        "\n",
        "    return self.network\n",
        "\n",
        "\n",
        "\n",
        "class Pre_Feedforward:\n",
        "\n",
        "  def __init__(self, Prev_layer_H, w, b, activation_fn):\n",
        "    self.Prev_layer_H= Prev_layer_H\n",
        "    self.w = w\n",
        "    self.b = b\n",
        "    self.activation_fn = activation_fn\n",
        "    Preactivation_cal()\n",
        "\n",
        "\n",
        "  def Preactivation_cal(self):\n",
        "\n",
        "    A = np.dot(self.w.T, self.Prev_layer_H) + self.b\n",
        "    H = apply_activation(self.activation_fn, A).do_activation()\n",
        "    cache = (self.Prev_layer_H, H)\n",
        "    return cache , A\n",
        "\n",
        "class Feedforward:\n",
        "\n",
        "  def __init__(self,X_train, layer_dimension, activation_fn, method):\n",
        "    self.X_train = X_train\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.method = method\n",
        "\n",
        "\n",
        "  def Forward_prop(self):\n",
        "    network = Weight_bias(self.n, self.activation_fn,self.method).Init_network(self.n)\n",
        "    H = self.X_train\n",
        "    L = len(network) // 3\n",
        "    cache = {}\n",
        "\n",
        "    for i in range(1,L+1):\n",
        "      Prev_layer_H = H\n",
        "\n",
        "      w_l = network[\"w\"+  str(i)]\n",
        "      b_l = network[\"b\"+  str(i)]\n",
        "      Act_fn_l = network[\"h\" + str(i)]\n",
        "\n",
        "      A = np.dot(w_l.T, Prev_layer_H) + b_l\n",
        "      H = apply_activation(Act_fn_l, A).do_activation()\n",
        "\n",
        "      cache[\"Input\" + str(i)] = Prev_layer_H\n",
        "      cache[\"Pre_act\" + str(i)] = A\n",
        "      #cache.append((Prev_layer_H, A))\n",
        "\n",
        "      #cache , New_H = Pre_Feedforward(Prev_layer_H, w_l, b_l, Act_fn_l).Preactivation_cal()\n",
        "      #cache.appand()\n",
        "\n",
        "    return cache, H"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MhJrZkPpp7Zv"
      },
      "id": "MhJrZkPpp7Zv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PFRmUzwVmaO",
        "outputId": "0083dcd8-e8bb-4557-aea9-cac6339a25f8"
      },
      "id": "3PFRmUzwVmaO",
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8],\n",
              "       [8]])"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Feedforward(X,[2,2,1],['relu','relu','softmax'],\"Xavier_U\").Forward_prop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV7gYASA0m8t",
        "outputId": "96ab6243-f260-4163-d9ea-5d686ea569f8"
      },
      "id": "NV7gYASA0m8t",
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'Input1': array([[8],\n",
              "         [8]]),\n",
              "  'Pre_act1': array([[2.9702881 ],\n",
              "         [5.09636034]]),\n",
              "  'Input2': array([[2.9702881 ],\n",
              "         [5.09636034]]),\n",
              "  'Pre_act2': array([[3.51198081],\n",
              "         [3.51198081]])},\n",
              " array([[3.51198081],\n",
              "        [3.51198081]]))"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "2064d83c-b27e-46e6-bef2-c24d9faf70ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2064d83c-b27e-46e6-bef2-c24d9faf70ff",
        "outputId": "aa5d4627-b69d-44cd-b46b-c2805f61fb9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.01356789, -1.25449632, -0.49676585],\n",
              "        [-0.47155413,  0.74081286,  0.57290153],\n",
              "        [ 0.14950529, -0.66005173,  0.60342294],\n",
              "        [ 0.65845703, -0.48812354,  0.04539087],\n",
              "        [-0.10556733,  0.74644992, -0.97459897]]),\n",
              " array([[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ],
      "source": [
        "Initilize(3,5,\"Xavier_N\").Init_weight(\"Xavier_N\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [relu,relu,sigmoid]\n",
        "print(x[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIOXHXcuRF9-",
        "outputId": "c15cbc1e-8445-452a-a6ba-a0fe9ae0edce"
      },
      "id": "AIOXHXcuRF9-",
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.relu'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Weight_bias([5,4,3,2],['relu','relu','sigmoid'],\"Xavier_N\").Init_network([5,4,3,2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NKWIZNDdN7u",
        "outputId": "3e375b82-0a4d-4281-9e78-d03d149dcf7d"
      },
      "id": "4NKWIZNDdN7u",
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'w1': array([[-0.84117663,  0.0214889 ,  0.80663283, -0.16841315],\n",
              "        [-0.93274035, -0.66675628, -0.78115316, -0.05809807],\n",
              "        [-0.09046519, -0.41788896, -0.05298089,  0.41793733],\n",
              "        [ 0.74649069, -0.00804288, -0.19982052, -0.38383287],\n",
              "        [ 0.48449515, -0.36236751, -0.25067937, -0.13372704]]),\n",
              " 'b1': array([[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]]),\n",
              " 'h1': 'relu',\n",
              " 'w2': array([[-0.32594142, -0.40731059, -0.15412021],\n",
              "        [ 0.15794187,  0.47889295, -0.18255195],\n",
              "        [ 0.22641304, -0.87348698,  0.22891319],\n",
              "        [-0.2061859 , -0.98225544, -0.52646661]]),\n",
              " 'b2': array([[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]]),\n",
              " 'h2': 'relu',\n",
              " 'w3': array([[-0.69542691, -0.95626005],\n",
              "        [ 0.24124141, -0.41219321],\n",
              "        [ 0.70387983, -0.10203239]]),\n",
              " 'b3': array([[0.],\n",
              "        [0.],\n",
              "        [0.]]),\n",
              " 'h3': 'sigmoid'}"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eAhzWxfwt_ub"
      },
      "id": "eAhzWxfwt_ub",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}