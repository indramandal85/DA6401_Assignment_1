{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "EuDCYeTFOWhL"
      },
      "id": "EuDCYeTFOWhL",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([[8,8,1],[7,9,1],[6,10,0],[5,5,0]], columns=['cgpa', 'profile_score', 'placed'])\n",
        "X = df[['cgpa', 'profile_score']].values[0].reshape(2,1) # Shape(no of features, no. of training example)\n",
        "y = df[['placed']].values[0][0]"
      ],
      "metadata": {
        "id": "H2q--JEjvU-Q"
      },
      "id": "H2q--JEjvU-Q",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "aNIlLeY_gnbE"
      },
      "id": "aNIlLeY_gnbE",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62rXWMGhyBbp",
        "outputId": "9d387ce9-4c25-4881-91af-6c37236e4a79"
      },
      "id": "62rXWMGhyBbp",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47040000"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy  as np\n",
        "\n",
        "class OneHotEncoder:\n",
        "\n",
        "    def __init__(self,x, y):\n",
        "        self.y = y\n",
        "        self.x = x\n",
        "        #self.num_class = num_class\n",
        "        self.onehot_encode()\n",
        "\n",
        "    def onehot_encode(self):\n",
        "        onehot = np.zeros((self.x.shape[0], 10))\n",
        "\n",
        "        for i, j in zip(range(len(self.x)), self.y):\n",
        "            onehot[i, j] = 1\n",
        "        return onehot.T\n",
        "\n",
        "class Normalize:\n",
        "\n",
        "    def __init__(self, unprocessed_X):\n",
        "        self.unprocessed_X = unprocessed_X\n",
        "        self.Norm_reshape()\n",
        "\n",
        "    def Norm_reshape(self):\n",
        "        X_norm = np.reshape(self.unprocessed_X,(self.unprocessed_X.shape[0],784)).T/255\n",
        "        X_norm= np.array(X_norm)\n",
        "\n",
        "        return X_norm"
      ],
      "metadata": {
        "id": "BW34ZLm_wv9o"
      },
      "id": "BW34ZLm_wv9o",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new= Normalize(X_train).Norm_reshape()"
      ],
      "metadata": {
        "id": "RUTyqFl2zbjL"
      },
      "id": "RUTyqFl2zbjL",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new= Normalize(X_train).Norm_reshape()\n",
        "y_new =OneHotEncoder(X_train,y_train).onehot_encode()"
      ],
      "metadata": {
        "id": "K4hGhlz9w6pa"
      },
      "id": "K4hGhlz9w6pa",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOeMXNH9g0hp",
        "outputId": "f6e99e5d-b5ec-4ffc-a686-93460e6489fd"
      },
      "id": "IOeMXNH9g0hp",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CalActivation:\n",
        "\n",
        "  def __init__(self, m):\n",
        "    self.m = m\n",
        "\n",
        "# This function calculates sigmoid activation function\n",
        "class sigmoid(CalActivation):\n",
        "\n",
        "    def use_sigmoid(self):\n",
        "        sig = np.where(self.m >= 0,1/(1 + np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) - 1))\n",
        "        return sig\n",
        "\n",
        "    def sigmoid_d(self):\n",
        "        sig_d = self.use_sigmoid()  # Compute sigmoid\n",
        "        # sig_d = np.where(self.m >= 0,1/(1 + np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) + 1))\n",
        "        return sig_d * (1 - sig_d)\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates tanh activation function\n",
        "class tanh(CalActivation):\n",
        "\n",
        "    def use_tanh(self):\n",
        "        z = (np.exp(self.m) - np.exp(-self.m))/(np.exp(self.m) + np.exp(-self.m))\n",
        "        return z\n",
        "\n",
        "    def tanh_d(self):\n",
        "        z = self.use_tanh()\n",
        "        return (1 - (z)**2)\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates relu activation function\n",
        "class relu(CalActivation):\n",
        "\n",
        "    def use_relu(self):\n",
        "        return np.where(self.m > 0, self.m, 0)\n",
        "        #     return self.m\n",
        "        # else:\n",
        "        #     return 0\n",
        "\n",
        "    def relu_d(self):\n",
        "        return np.where(self.m >0 , 1 ,0)\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates softmax activation function\n",
        "class softmax(CalActivation):\n",
        "\n",
        "    def use_softmax(self):\n",
        "        x = np.exp(self.m)/np.sum(np.exp(self.m), axis=0)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def softmax_d(self):\n",
        "        z=self.m - np.max(self.m,axis=0)\n",
        "        soft=np.exp(z)/np.sum(np.exp(z),axis=0)\n",
        "        return soft*(1-soft)\n",
        "\n",
        "\n",
        "\n",
        "# This function call different activation function\n",
        "class apply_activation(CalActivation):\n",
        "\n",
        "    def __init__(self, activation_function, m):\n",
        "        super().__init__(m)\n",
        "        self.activation_function = activation_function.lower()\n",
        "\n",
        "    def do_activation(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.m).use_sigmoid()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.m).use_relu()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.m).use_tanh()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.m).use_softmax()\n",
        "\n",
        "    def do_activation_derivative(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.m).sigmoid_d()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.m).relu_d()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.m).tanh_d()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.m).softmax_d()\n",
        "        else:\n",
        "           raise ValueError(\"Unknown activation function\")\n"
      ],
      "metadata": {
        "id": "01ZCcJdxviK6"
      },
      "id": "01ZCcJdxviK6",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-qdvScjQHQ2"
      },
      "id": "2-qdvScjQHQ2",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CalLoss:\n",
        "    def __init__(self, y, y_pred):\n",
        "      self.y = y\n",
        "      self.y_predicted = y_pred\n",
        "      if y.shape != y_pred.shape:\n",
        "            raise ValueError(f\"Shape mismatch: y shape is {self.y.shape}, y_predicted shape is {self.y_predicted.shape}\")\n",
        "\n",
        "\n",
        "class CrossEntropy(CalLoss):\n",
        "\n",
        "    def give_celoss(self):\n",
        "        return -np.mean(self.y * np.log(self.y_predicted))\n",
        "\n",
        "    def Give_cegrad(self):\n",
        "        grad = -self.y/(self.y_predicted)\n",
        "        return grad\n",
        "\n",
        "class SquaredError(CalLoss):\n",
        "\n",
        "    def give_seloss(self):\n",
        "        return np.mean((self.y - self.y_predicted) ** 2)\n",
        "\n",
        "    def Give_segrad(self):\n",
        "        res = -2*(self.y - self.y_predicted)\n",
        "        return res\n",
        "\n",
        "class callloss(CalLoss):\n",
        "    def __init__(self, loss_function, y, y_pred):\n",
        "        self.loss_function = loss_function.lower()\n",
        "        super().__init__(y, y_pred)\n",
        "\n",
        "    def give_loss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return CrossEntropy(self.y, self.y_predicted).give_celoss()\n",
        "        elif self.loss_function == 'se':\n",
        "            return SquaredError(self.y, self.y_predicted).give_seloss()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss function: {self.loss_function}\")\n",
        "\n",
        "    def give_gradloss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return CrossEntropy(self.y, self.y_predicted).Give_cegrad()\n",
        "        elif self.loss_function == 'se':\n",
        "            return SquaredError(self.y, self.y_predicted).Give_segrad()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss function: {self.loss_function}\")"
      ],
      "metadata": {
        "id": "iA7A6vN-l5K1"
      },
      "id": "iA7A6vN-l5K1",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callloss(\"se\", y_new, q).give_loss()\n",
        "y_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "SOKbXCm5ZTK8",
        "outputId": "03c975cf-6614-44e3-c915-653f4b713b8b"
      },
      "id": "SOKbXCm5ZTK8",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'q' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2abfa0093b6f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcallloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"se\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgive_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'q' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyUj2hy6Bdt6",
        "outputId": "6a45e30a-967b-45da-d80d-b37ed16b7850"
      },
      "id": "uyUj2hy6Bdt6",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 60000)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "895d572d-2ffd-466a-a260-e57859a6449e",
      "metadata": {
        "id": "895d572d-2ffd-466a-a260-e57859a6449e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Initilize:\n",
        "    def __init__(self, layer_dimension, activation_function, y_train, method = \"Xavier_U\"):\n",
        "        self.n = layer_dimension\n",
        "        self.activation_fn = activation_function\n",
        "        self.y = y_train\n",
        "        self.Init_method = method\n",
        "\n",
        "class InitializeWeights(Initilize):\n",
        "    def __init__(self, ip_size, op_size, activation_function, batch_size, method):\n",
        "        super().__init__([ip_size, op_size], activation_function, batch_size, method)  # Properly inherit Initilize attributes\n",
        "        self.i_size = ip_size\n",
        "        self.o_size = op_size\n",
        "        self.Init_weights()\n",
        "\n",
        "\n",
        "    def Init_weights(self):\n",
        "        np.random.seed(0)\n",
        "\n",
        "        if self.Init_method == \"Xavier_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(1 / self.i_size)\n",
        "          self.weight = np.random.randn(self.o_size,self.i_size)*a\n",
        "\n",
        "        elif self.Init_method == \"Xavier_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / (self.o_size + self.i_size))\n",
        "          self.weight = np.random.uniform((-a), a,( self.o_size,self.i_size))\n",
        "\n",
        "        elif self.Init_method == \"He_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(2 / self.i_size)\n",
        "          self.weight = np.random.randn(self.o_size,self.i_size)*a\n",
        "\n",
        "        elif self.Init_method == \"He_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / self.i_size)\n",
        "          self.weight = np.random.uniform(-a, a, (self.o_size,self.i_size))\n",
        "\n",
        "        elif self.Init_method == \"Random\":\n",
        "          np.random.seed(0)\n",
        "          self.weight = np.random.randn(self.o_size,self.i_size)*0.01\n",
        "        else:\n",
        "          raise ValueError(f\"Unknown initialization method: {self.Init_method}\")\n",
        "\n",
        "\n",
        "        # Initialize biases and activations\n",
        "        self.bias = np.zeros((self.o_size, 1))\n",
        "        self.a = np.zeros((self.o_size, len(self.y[1])))\n",
        "        self.h = np.zeros((self.o_size, len(self.y[1])))\n",
        "\n",
        "        # Activation function and its derivative\n",
        "        self.g = apply_activation(self.activation_fn, self.a).do_activation()\n",
        "        self.d_g = apply_activation(self.activation_fn, self.a).do_activation_derivative()\n",
        "\n",
        "        # Gradients\n",
        "        self.d_a = np.zeros_like(self.a)\n",
        "        self.d_h = np.zeros_like(self.h)\n",
        "        self.d_w = np.zeros_like(self.weight)\n",
        "        self.d_b = np.zeros_like(self.bias)\n",
        "        self.Weight_updates = np.zeros_like(self.weight)\n",
        "        self.bias_updates = np.zeros_like(self.bias)\n",
        "\n",
        "\n",
        "\n",
        "class Weight_bias(Initilize):\n",
        "    def __init__(self, layer_dimension, activation_function, y_train, method=\"Xavier_U\"):\n",
        "        super().__init__(layer_dimension, activation_function, y_train, method)\n",
        "        self.network = []\n",
        "\n",
        "\n",
        "    def Init_network(self):\n",
        "\n",
        "        for i in range(1, len(self.n)):\n",
        "          self.network.append(InitializeWeights( self.n[i-1], self.n[i], self.activation_fn[i-1], self.y, self.Init_method))\n",
        "\n",
        "        return self.network\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_new[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ehsUHC-CheX",
        "outputId": "47222b54-ace4-4e17-db77-1219a216b3c2"
      },
      "id": "_ehsUHC-CheX",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = Weight_bias([784, 128, 64, 10],['sigmoid','relu','softmax'], y_train=y_new, method= \"Xavier_U\").Init_network()\n",
        "for i in range(3):\n",
        "  print(f\"layer {i}\")\n",
        "  print(f\"{i}th layer w\" , a[i].weight.shape)\n",
        "  print(f\"{i}th layer b\" , a[i].bias.shape)\n",
        "  print(f\"{i}th layer h\" , a[i].h.shape)\n",
        "  print(f\"{i}th layer a\" , a[i].a.shape)\n",
        "  print(f\"{i}th layer g\" , a[i].g.shape)\n",
        "  print(f\"{i}th layer d_a\" , a[i].d_a.shape)\n",
        "  print(f\"{i}th layer d_h\" , a[i].d_h.shape)\n",
        "  print(f\"{i}th layer d_w\" , a[i].d_w.shape)\n",
        "  print(f\"{i}th layer d_b\" , a[i].d_b.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeaxRE4jdJ5C",
        "outputId": "0936eadd-0977-4ee9-92b7-df28418b9db1"
      },
      "id": "IeaxRE4jdJ5C",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-526b8f77b8ac>:12: RuntimeWarning: divide by zero encountered in divide\n",
            "  sig = np.where(self.m >= 0,1/(1 + np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) - 1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer 0\n",
            "0th layer w (128, 784)\n",
            "0th layer b (128, 1)\n",
            "0th layer h (128, 60000)\n",
            "0th layer a (128, 60000)\n",
            "0th layer g (128, 60000)\n",
            "0th layer d_a (128, 60000)\n",
            "0th layer d_h (128, 60000)\n",
            "0th layer d_w (128, 784)\n",
            "0th layer d_b (128, 1)\n",
            "layer 1\n",
            "1th layer w (64, 128)\n",
            "1th layer b (64, 1)\n",
            "1th layer h (64, 60000)\n",
            "1th layer a (64, 60000)\n",
            "1th layer g (64, 60000)\n",
            "1th layer d_a (64, 60000)\n",
            "1th layer d_h (64, 60000)\n",
            "1th layer d_w (64, 128)\n",
            "1th layer d_b (64, 1)\n",
            "layer 2\n",
            "2th layer w (10, 64)\n",
            "2th layer b (10, 1)\n",
            "2th layer h (10, 60000)\n",
            "2th layer a (10, 60000)\n",
            "2th layer g (10, 60000)\n",
            "2th layer d_a (10, 60000)\n",
            "2th layer d_h (10, 60000)\n",
            "2th layer d_w (10, 64)\n",
            "2th layer d_b (10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Pre_Feedforward:\n",
        "\n",
        "  def __init__(self, inputs, w, b, activation_fn):\n",
        "    self.ip= inputs\n",
        "    self.w = w\n",
        "    self.b = b\n",
        "    self.activation_fn = activation_fn\n",
        "    self.Preactivation_cal()\n",
        "\n",
        "\n",
        "  def Preactivation_cal(self):\n",
        "\n",
        "    A = np.dot(self.w, self.ip) + self.b\n",
        "    H = apply_activation(self.activation_fn, A).do_activation()\n",
        "    #cache = (H, A)\n",
        "    #self.cache[\"Input\"] = self.Prev_layer_H\n",
        "    #self.cache[\"Pre_act\"] = H\n",
        "    return A , H\n",
        "\n",
        "class Feedforward:\n",
        "\n",
        "  def __init__(self,X_train, activation_fn, method, network, batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.input = X_train\n",
        "    self.activation_fn = activation_fn\n",
        "    self.method = method\n",
        "    self.network = network\n",
        "\n",
        "  def Forward_prop(self):\n",
        "\n",
        "    L = len(self.network)\n",
        "    print(L)\n",
        "\n",
        "    for i in range(L):\n",
        "      # print(i)\n",
        "      # print(\"oth layer input\" , self.input.shape)\n",
        "\n",
        "      self.network[i].a, self.network[i].h = Pre_Feedforward(self.input, self.network[i].weight, self.network[i].bias, self.activation_fn[i]).Preactivation_cal()\n",
        "      self.input = self.network[i].a\n",
        "      # print(\"oth layer w\" , a[i].weight.shape)\n",
        "      # print(\"oth layer b\" , a[i].bias.shape)\n",
        "      # print(\"oth layer h\" , a[i].h.shape)\n",
        "      # print(\"oth layer a\" , a[i].a.shape)\n",
        "\n",
        "\n",
        "    return self.network\n",
        "\n",
        "\n",
        "class Findloss:\n",
        "  def __init__(self,X_train, y_train, layer_dimension, activation_fn, method, loss_function):\n",
        "    self.X_train = X_train\n",
        "    self.y_train = y_train\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.loss_function = loss_function\n",
        "    self.method = method\n",
        "    self.y_pred = Feedforward(self.X_train, self.n, self.activation_fn, self.method).Forward_prop()[-1]\n",
        "\n",
        "  def loss(self):\n",
        "    return callloss(self.loss_function, self.y_train, self.y_pred).give_loss()"
      ],
      "metadata": {
        "id": "kVB_Sy02O6Uz"
      },
      "id": "kVB_Sy02O6Uz",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = Feedforward(X_new,['sigmoid','relu','softmax'],\"Xavier_U\", a,batch_size= 60000).Forward_prop()\n",
        "for i in range(3):\n",
        "  print(f\"{i}th layer w\" , network[i].weight)\n",
        "  print(f\"{i}th layer b\" , network[i].bias)\n",
        "  print(f\"{i}th layer h\" , network[i].h)\n",
        "  print(f\"{i}th layer a\" , network[i].a.shape)\n",
        "  print(f\"{i}th layer g\" , network[i].g)\n",
        "  print(f\"{i}th layer d_g\" , network[i].d_g)\n",
        "  print(f\"{i}th layer d_a\" , network[i].d_a)\n",
        "  print(f\"{i}th layer d_h\" , network[i].d_h)\n",
        "  print(f\"{i}th layer d_w\" , network[i].d_w)\n",
        "  print(f\"{i}th layer d_b\" , network[i].d_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtK_qppdC0m6",
        "outputId": "87bafb74-e285-4b40-ab0c-f652f210641f"
      },
      "id": "VtK_qppdC0m6",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "0th layer w [[ 0.0079186   0.03490832  0.01667042 ... -0.01185768  0.05561841\n",
            "   0.05159181]\n",
            " [-0.06449701 -0.05574198 -0.03176317 ... -0.02593908 -0.06998283\n",
            "  -0.04397699]\n",
            " [-0.02303805 -0.01052136  0.01475026 ... -0.08096418 -0.06978534\n",
            "  -0.04436897]\n",
            " ...\n",
            " [ 0.05179032 -0.03124079  0.07870502 ...  0.04822593  0.03519785\n",
            "  -0.00536821]\n",
            " [ 0.07242752 -0.02499434 -0.03771518 ... -0.0059433  -0.01991812\n",
            "   0.04865525]\n",
            " [ 0.03747288 -0.03090882  0.02881455 ... -0.05079785 -0.03773598\n",
            "  -0.0132189 ]]\n",
            "0th layer b [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "0th layer h [[  1.61170724   0.78629914   8.70130476 ...   0.57777949   1.42430891\n",
            "    5.57170047]\n",
            " [  0.73640948   0.71645584   0.54959926 ...   0.57820542   0.5687523\n",
            "    0.57412061]\n",
            " [  0.56431554   0.75841772   4.20091791 ...   0.58510981   0.64634489\n",
            "    0.50608142]\n",
            " ...\n",
            " [  0.62002714   0.69166675   0.57815055 ...   0.65032857   0.5571856\n",
            "  105.48683537]\n",
            " [ 15.40906724   0.59386462   0.53109179 ...   0.67789307  61.89381202\n",
            "    0.54563027]\n",
            " [  0.69505762   0.54906275   0.54931648 ...   0.73201639   0.55063199\n",
            "    0.56165955]]\n",
            "0th layer a (128, 60000)\n",
            "0th layer g [[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            " ...\n",
            " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 ... 0.5 0.5 0.5]]\n",
            "0th layer d_g [[0.25 0.25 0.25 ... 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 ... 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 ... 0.25 0.25 0.25]\n",
            " ...\n",
            " [0.25 0.25 0.25 ... 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 ... 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 ... 0.25 0.25 0.25]]\n",
            "0th layer d_a [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "0th layer d_h [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "0th layer d_w [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "0th layer d_b [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "1th layer w [[ 0.01725818  0.07608093  0.03633234 ...  0.03761568 -0.16999087\n",
            "  -0.0701539 ]\n",
            " [ 0.0566299  -0.07421877  0.04172476 ...  0.08791587  0.14273648\n",
            "  -0.14728241]\n",
            " [ 0.01845282  0.0298668   0.16331917 ... -0.01512379  0.15976175\n",
            "   0.02678208]\n",
            " ...\n",
            " [-0.14754812  0.023115   -0.14604579 ... -0.16797796  0.1624065\n",
            "  -0.15850325]\n",
            " [-0.05338549  0.05792629 -0.06292692 ... -0.09659967 -0.02455546\n",
            "  -0.04217366]\n",
            " [-0.05761267  0.13478784 -0.10509819 ...  0.1102607   0.03137414\n",
            "   0.02335723]]\n",
            "1th layer b [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "1th layer h [[0.         0.9400258  0.26839603 ... 0.83372324 0.11257283 0.        ]\n",
            " [0.14133327 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.539564   0.18222283 ... 0.         0.1470746  0.02840578]\n",
            " ...\n",
            " [0.16051281 0.22491441 0.30338469 ... 1.27465099 0.13059249 0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.27055354 0.         0.03554326 ... 0.14120233 0.         0.        ]]\n",
            "1th layer a (64, 60000)\n",
            "1th layer g [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "1th layer d_g [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "1th layer d_a [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "1th layer d_h [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "1th layer d_w [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "1th layer d_b [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "2th layer w [[ 0.02779904  0.12254922  0.05852321  0.02556074 -0.04347819  0.08308594\n",
            "  -0.03554376  0.22311269  0.26405353 -0.06637945  0.16613589  0.01645551\n",
            "   0.03875102  0.24237507 -0.24429273 -0.23512772 -0.27323313  0.18942527\n",
            "   0.15840882  0.21071999  0.27257066  0.17036925 -0.0219373   0.15975991\n",
            "  -0.21739073  0.07968429 -0.20310845  0.25323663  0.01244251 -0.04859958\n",
            "  -0.13408435  0.15617466 -0.02497216  0.03897278 -0.27404671  0.0669928\n",
            "   0.06383793  0.0665933   0.25271222  0.10354571 -0.08000952 -0.03585997\n",
            "   0.11254994 -0.25044931  0.09497278  0.09717738 -0.16493562 -0.21132454\n",
            "  -0.10511259 -0.07761601  0.0399767  -0.03496612  0.27812636 -0.22663341\n",
            "  -0.16579317 -0.19288247  0.08719439 -0.14049915 -0.01918584 -0.1455483\n",
            "  -0.19421505 -0.22188933  0.08902889 -0.20605293]\n",
            " [-0.17279477 -0.07476033  0.18280397 -0.22944873  0.19245787 -0.23001986\n",
            "   0.27134119 -0.01785298  0.27151296  0.05970898  0.13625936 -0.26243015\n",
            "  -0.1236903  -0.21629608 -0.1160971  -0.21713258 -0.10365763 -0.04882678\n",
            "  -0.24821573  0.10961187  0.03792918 -0.13360946  0.01323965 -0.23124877\n",
            "   0.04325113  0.24448195 -0.10332404  0.09533934 -0.2096892   0.12319722\n",
            "  -0.11993213 -0.18042087  0.04926867 -0.27329626  0.18732964 -0.28207335\n",
            "   0.10126559 -0.13097926  0.13394177  0.26321397 -0.14308378  0.04337121\n",
            "   0.0524174   0.04114708 -0.15770357  0.25783821 -0.03011182  0.19727794\n",
            "   0.11360241 -0.1153586   0.17870623 -0.05893944  0.21703629  0.04628448\n",
            "   0.2173963   0.10964574  0.12828114  0.00075423  0.25973726  0.08200167\n",
            "  -0.04336415  0.06059038 -0.27381697 -0.11300211]\n",
            " [ 0.091218   -0.11954971  0.06720917 -0.04056585 -0.20759562 -0.11487717\n",
            "   0.03984465  0.05175156  0.04232784  0.08724707  0.08662202 -0.03905684\n",
            "   0.22583122 -0.07542283 -0.03652459  0.22319831  0.17437588  0.11611349\n",
            "  -0.22766871  0.23889317  0.12200931  0.28409077 -0.19963737  0.20964587\n",
            "  -0.19220852  0.06581057 -0.21423256  0.19818888  0.17501655  0.03935251\n",
            "  -0.05285863 -0.24535715  0.11243466 -0.0264572   0.12645951  0.20865283\n",
            "   0.27080702  0.20262815 -0.27807629 -0.07974176  0.13097843 -0.18700519\n",
            "   0.01198024 -0.2538022  -0.17085042 -0.27419933  0.16725931 -0.15722345\n",
            "  -0.08807141  0.24379007  0.11641294 -0.26661529 -0.19095493  0.06918132\n",
            "   0.04398128 -0.14926867  0.24728261  0.06490302  0.0202927   0.05120326\n",
            "   0.1310533  -0.10709635 -0.05796258 -0.16524248]\n",
            " [-0.17871145  0.25306776  0.13642293 -0.00543366 -0.15523595 -0.13989271\n",
            "  -0.25170009 -0.03734939 -0.10718127  0.1118166  -0.06961969 -0.18246404\n",
            "  -0.27069299 -0.24644908  0.10216325 -0.02636941  0.02083167  0.22590224\n",
            "   0.27924548 -0.16122569  0.09287219 -0.13478668 -0.27298676  0.1471453\n",
            "  -0.1024993  -0.06636671  0.05029614  0.18853037  0.07345449  0.21222261\n",
            "  -0.12896663  0.16973612 -0.17902869  0.25786249  0.1067736  -0.1620169\n",
            "   0.25477522  0.13147118 -0.14012895 -0.16326734  0.01036521 -0.27013261\n",
            "  -0.16659427 -0.04289123 -0.07165954 -0.02074361 -0.12663929  0.04942323\n",
            "   0.20721387 -0.21781362  0.00989731 -0.2095353   0.12350046 -0.05919346\n",
            "   0.0372571  -0.18037049 -0.20225735 -0.00680189 -0.08222779  0.2508237\n",
            "   0.15110135  0.14161264  0.22991629 -0.23723876]\n",
            " [ 0.02972334  0.04810868  0.26307036 -0.1183709  -0.14759666 -0.22763052\n",
            "  -0.27539081  0.24461471  0.09676659  0.1623931  -0.12430357  0.04921014\n",
            "  -0.24832521 -0.00818501  0.271931    0.21441778 -0.09216763  0.2628618\n",
            "  -0.15279453  0.25588473  0.25136231  0.17039432  0.07428942  0.21315505\n",
            "  -0.11787387  0.19872154  0.06713016 -0.27720908 -0.08699972 -0.20038195\n",
            "   0.27439933 -0.012318   -0.0014856   0.07942887 -0.07484038 -0.20678341\n",
            "   0.18344437 -0.17663     0.0064461  -0.15700002 -0.22902547  0.20626618\n",
            "   0.26932519  0.26244294  0.23153124  0.15606853 -0.09502297 -0.23856058\n",
            "  -0.05282567 -0.15249126 -0.20929638 -0.2543209   0.12847482 -0.27823952\n",
            "   0.15409433 -0.20106205 -0.23945999 -0.23371894  0.09798033 -0.14501205\n",
            "  -0.04525236  0.03267123  0.20533202  0.12930053]\n",
            " [-0.13079706 -0.20986863 -0.25321201 -0.11298855 -0.13547248 -0.02497772\n",
            "   0.10437777  0.11140767 -0.12328489 -0.06838097 -0.18158287  0.16432517\n",
            "  -0.25237271  0.1121889   0.15871558  0.15798216 -0.1370076  -0.07186276\n",
            "   0.04988754 -0.12937674 -0.07354866 -0.17252601 -0.02286187 -0.25934093\n",
            "   0.1707322  -0.2409211   0.01072652 -0.11002064  0.04416031  0.2616449\n",
            "   0.0829015  -0.26460868 -0.03963545  0.00570455  0.0206029   0.10330209\n",
            "  -0.12665787 -0.21136198 -0.06112064  0.25992068 -0.17817733  0.23006676\n",
            "   0.02494726 -0.02453872  0.2175706  -0.02357483  0.1276623  -0.05750455\n",
            "   0.23010118  0.10821826  0.11368372 -0.09811234  0.1462341   0.07748606\n",
            "  -0.1480571  -0.19332137  0.1687934   0.26149299 -0.02383972  0.05181501\n",
            "   0.20372118 -0.02436102  0.25734016  0.04313989]\n",
            " [ 0.18267521  0.23283437  0.17968917 -0.19396169  0.07340699 -0.05784116\n",
            "  -0.2490327  -0.04326324 -0.13742817  0.1987755  -0.26578059  0.26138827\n",
            "  -0.08236669 -0.08160468 -0.2754484  -0.17925855 -0.0562322   0.24447923\n",
            "  -0.22801721  0.25359691  0.2104218  -0.02610428 -0.09869295 -0.15220083\n",
            "   0.06518705 -0.26591159 -0.27585983 -0.04055047 -0.24597957 -0.14126832\n",
            "  -0.15879741 -0.14055633 -0.21011213 -0.27789283 -0.21897969  0.06747389\n",
            "   0.27008645  0.27924893 -0.05179322 -0.1919457   0.0790241  -0.00552105\n",
            "   0.27871632 -0.24755699  0.16130054 -0.12050595 -0.14726075  0.09254551\n",
            "  -0.1446157   0.0944559   0.00985711 -0.04323093  0.03114442 -0.12127305\n",
            "   0.11764322 -0.04848857 -0.07941858  0.1871684   0.24201645 -0.25854647\n",
            "  -0.15226754 -0.08626743  0.17937177  0.27648484]\n",
            " [ 0.26707695  0.23061598 -0.11586015  0.28019784 -0.14270398 -0.22443439\n",
            "   0.25681517 -0.15181578  0.10807204 -0.25151376  0.13138763  0.21738768\n",
            "  -0.129596   -0.06887647 -0.07158767  0.14168362 -0.14931741 -0.18687795\n",
            "  -0.02887814 -0.11135422  0.19316644 -0.14935467  0.00136078  0.25204906\n",
            "   0.07631099  0.20916941  0.25069713  0.14280928  0.11365696  0.26650396\n",
            "   0.28155868 -0.0274373  -0.24438743 -0.11800272 -0.19798219 -0.04699108\n",
            "  -0.20997881  0.05929455 -0.0667402   0.2251702   0.26640663  0.02670071\n",
            "  -0.12823681  0.05252474  0.22595342 -0.05311487  0.02965831 -0.13004256\n",
            "  -0.02537433 -0.05597363 -0.14327722  0.00334088 -0.10798713 -0.07230598\n",
            "   0.01422054  0.14271256 -0.09481663  0.24155621  0.20633853 -0.25701853\n",
            "  -0.1402993  -0.03067555 -0.22516236 -0.08629214]\n",
            " [ 0.13673429  0.10280206  0.0696973   0.11989483 -0.16804443 -0.0901521\n",
            "   0.10036918  0.21597222  0.02487442 -0.12375142 -0.26752858  0.11978573\n",
            "  -0.28025744 -0.07250861  0.01739078  0.24039028 -0.23378072 -0.05356536\n",
            "  -0.27090116 -0.08963223  0.06960995 -0.12581965 -0.16529589 -0.21885501\n",
            "   0.04393097  0.11120525  0.0979287   0.25562402 -0.28320793  0.08382773\n",
            "   0.05717286  0.05053675  0.26354529 -0.27513907  0.11189572  0.17863836\n",
            "   0.00558515 -0.09455614  0.16563196 -0.22936806 -0.0330104   0.01136277\n",
            "   0.11045717 -0.23298845 -0.15503955 -0.05108279  0.07021567  0.22037215\n",
            "   0.06767088 -0.20874179  0.27368789  0.21173004  0.00154946  0.24052498\n",
            "   0.02356615  0.2410706   0.18787484  0.26668667  0.23906413 -0.26422633\n",
            "  -0.18521565 -0.06313722  0.25749291 -0.11388249]\n",
            " [-0.19336191  0.2199985  -0.0305281   0.23228303 -0.19349698  0.09175558\n",
            "  -0.03401948 -0.24118858  0.11188474 -0.14385509 -0.26218656 -0.25060943\n",
            "  -0.24996349  0.2322018   0.13661264  0.22669444  0.09828473  0.01648114\n",
            "  -0.11136678  0.28358691 -0.07848261 -0.01671527 -0.06933874  0.27308809\n",
            "  -0.18528036 -0.09795994  0.10270763 -0.24875099  0.06107796 -0.0127302\n",
            "  -0.12301089 -0.14897228  0.00826493 -0.07521456 -0.0247617  -0.09255579\n",
            "   0.26794371 -0.20875434 -0.22961805 -0.0891876   0.05183935  0.09065017\n",
            "  -0.05851175  0.28433622 -0.08434617  0.12608995  0.07835263  0.17828255\n",
            "   0.27120804  0.22198546  0.15066667  0.11290148 -0.09368294 -0.20064123\n",
            "  -0.24907652 -0.14698564 -0.03856534  0.01252676  0.15551966  0.26125057\n",
            "  -0.217934   -0.2238091   0.05108068  0.13975293]]\n",
            "2th layer b [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "2th layer h [[0.0778683  0.02468536 0.04275008 ... 0.02468323 0.08180254 0.14728661]\n",
            " [0.1092721  0.0819699  0.10471752 ... 0.07152366 0.0819238  0.11605884]\n",
            " [0.09311495 0.15986403 0.12674986 ... 0.13688806 0.09259145 0.04590424]\n",
            " ...\n",
            " [0.15451005 0.08863799 0.10436297 ... 0.05099216 0.07806495 0.05192679]\n",
            " [0.13253684 0.28336721 0.16535948 ... 0.21272903 0.13890776 0.14826585]\n",
            " [0.1322844  0.0448755  0.06146801 ... 0.0250413  0.08221197 0.06426352]]\n",
            "2th layer a (10, 60000)\n",
            "2th layer g [[0.1 0.1 0.1 ... 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 ... 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 ... 0.1 0.1 0.1]\n",
            " ...\n",
            " [0.1 0.1 0.1 ... 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 ... 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 ... 0.1 0.1 0.1]]\n",
            "2th layer d_g [[0.09 0.09 0.09 ... 0.09 0.09 0.09]\n",
            " [0.09 0.09 0.09 ... 0.09 0.09 0.09]\n",
            " [0.09 0.09 0.09 ... 0.09 0.09 0.09]\n",
            " ...\n",
            " [0.09 0.09 0.09 ... 0.09 0.09 0.09]\n",
            " [0.09 0.09 0.09 ... 0.09 0.09 0.09]\n",
            " [0.09 0.09 0.09 ... 0.09 0.09 0.09]]\n",
            "2th layer d_a [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "2th layer d_h [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "2th layer d_w [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "2th layer d_b [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = Weight_bias([784, 128, 64 ,10],['sigmoid','relu','softmax'], y_new, method= \"Xavier_U\").Init_network()\n",
        "print(\"oth layer w\" , a[0].weight.shape)\n",
        "print(\"oth layer b\" , a[0].bias.shape)\n",
        "print(\"oth layer h\" , a[0].h.shape)\n",
        "print(\"oth layer a\" , a[0].a.shape)\n",
        "print(\"1th layer w\" , a[1].weight.shape)\n",
        "print(\"1th layer b\" , a[1].bias.shape)\n",
        "print(\"1th layer h\" , a[1].h.shape)\n",
        "print(\"1th layer a\" , a[1].a.shape)\n",
        "print(\"2th layer w\" , a[2].weight.shape)\n",
        "print(\"2th layer b\" , a[2].bias.shape)\n",
        "print(\"2th layer h\" , a[2].h.shape)\n",
        "print(\"2th layer a\" , a[2].a.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_llvujQYBewv",
        "outputId": "077c8f89-af63-41ba-d468-b5b5b5796341"
      },
      "id": "_llvujQYBewv",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-526b8f77b8ac>:12: RuntimeWarning: divide by zero encountered in divide\n",
            "  sig = np.where(self.m >= 0,1/(1 + np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) - 1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oth layer w (128, 784)\n",
            "oth layer b (128, 1)\n",
            "oth layer h (128, 60000)\n",
            "oth layer a (128, 60000)\n",
            "1th layer w (64, 128)\n",
            "1th layer b (64, 1)\n",
            "1th layer h (64, 60000)\n",
            "1th layer a (64, 60000)\n",
            "2th layer w (10, 64)\n",
            "2th layer b (10, 1)\n",
            "2th layer h (10, 60000)\n",
            "2th layer a (10, 60000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network = Feedforward(X_new,['sigmoid','relu','softmax'],\"Xavier_U\", a,batch_size= 10000).Forward_prop()\n",
        "q = network[2].h\n",
        "print(q.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZUDWcZAAiTI",
        "outputId": "123892a7-3984-4243-c18c-11f2e3cd10f6"
      },
      "id": "iZUDWcZAAiTI",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "(10, 60000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z =callloss(loss_function=\"se\",y= y_new,y_pred= q).give_loss()\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiTnApEkZ-ZV",
        "outputId": "75e714a8-94a5-4d2f-fcc7-c7fab83eb0a1"
      },
      "id": "fiTnApEkZ-ZV",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09521316379404962"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        for param, grad in zip(params, grads):\n",
        "            param -= self.lr * grad\n",
        "\n",
        "class Momentum:\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.velocities = None\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.velocities is None:\n",
        "            self.velocities = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.velocities[i] = self.momentum * self.velocities[i] - self.lr * grad\n",
        "            param += self.velocities[i]\n",
        "\n",
        "\n",
        "class Adagrad:\n",
        "    def __init__(self, lr=0.01, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "        self.cache = None\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.cache is None:\n",
        "            self.cache = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.cache[i] += grad ** 2\n",
        "            param -= self.lr * grad / (np.sqrt(self.cache[i]) + self.epsilon)\n",
        "\n",
        "\n",
        "class RMSProp:\n",
        "    def __init__(self, lr=0.001, beta=0.9, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "        self.cache = None\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.cache is None:\n",
        "            self.cache = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.cache[i] = self.beta * self.cache[i] + (1 - self.beta) * grad ** 2\n",
        "            param -= self.lr * grad / (np.sqrt(self.cache[i]) + self.epsilon\n",
        "\n",
        "\n",
        "\n",
        "class Adam:\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        self.t = 0\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m = [np.zeros_like(param) for param in params]\n",
        "            self.v = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        self.t += 1\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n",
        "\n",
        "            # Bias correction\n",
        "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
        "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon\n",
        "\n",
        "\n",
        "class Nadam:\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        self.t = 0\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m = [np.zeros_like(param) for param in params]\n",
        "            self.v = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        self.t += 1\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            # Update biased moment estimates\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n",
        "\n",
        "            # Compute bias-corrected estimates with Nesterov acceleration\n",
        "            m_hat = (self.beta1 * self.m[i] / (1 - self.beta1 ** (self.t + 1)) +\n",
        "                    ((1 - self.beta1) * grad / (1 - self.beta1 ** self.t))\n",
        "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "ynXoTieZS-7O",
        "outputId": "a78a2eef-3fd5-46c0-9829-63ddadfae6de"
      },
      "id": "ynXoTieZS-7O",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-24-229680b660aa>, line 56)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-229680b660aa>\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    class Adam:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Regularization Code\n",
        "class Regularisation:\n",
        "  def __init__(self, network, weight = 0):\n",
        "    self.network = network\n",
        "    self.weight = weight\n",
        "\n",
        "class L2_regularisation(Regularisation):\n",
        "\n",
        "  def Apply_L2(self):\n",
        "    # Function that returns L2 regularisation loss for the given network\n",
        "    L = len(self.network)\n",
        "    res = 0\n",
        "    for j in range(L-1):\n",
        "      n = np.mean(self.network[j].weight)\n",
        "      res += (1 / 2*n) * np.sum(self.network[j].weight ** 2)\n",
        "    return res\n",
        "\n",
        "  def Apply_L2_grad(self):\n",
        "  # Function that returns L2 regularisation gradient for the given Weight matrix / tensor\n",
        "    res_d = 2 * self.weight\n",
        "    return res_d\n",
        "\n",
        "\n",
        "class L1_regularisation(Regularisation):\n",
        "\n",
        "  def Apply_L1(self):\n",
        "    # Function that returns L1 regularisation loss for the given network\n",
        "    L = len(self.network)\n",
        "    res = 0\n",
        "    for j in range(L-1):\n",
        "      n = np.mean(self.network[j].weight)\n",
        "      res += (1 / 2*n) * np.sum(np.absolute(self.network[j].weight))\n",
        "    return res\n",
        "\n",
        "  def Apply_L1_grad(self):\n",
        "    # Function that returns L1 regularisation gradient for the given Weight matrix / tensor\n",
        "    res_d = np.sign(self.weight)\n",
        "    return res_d\n",
        "\n",
        "\n",
        "class ApplyReg(Regularisation):\n",
        "\n",
        "    def __init__(self,reg_function, network):\n",
        "        self.reg_function= reg_function\n",
        "        super().__init__(network)\n",
        "\n",
        "    def do_reg(self):\n",
        "        if self.reg_function == 'L2':\n",
        "            return L2_regularisation(self.network).Apply_L2()\n",
        "        if self.reg_function == 'L1':\n",
        "            return L1_regularisation(self.network).Apply_L1()\n",
        "        if self.reg_function == 'L2_d':\n",
        "            return L2_regularisation(self.network, self.weight).Apply_L2_grad()\n",
        "        if self.reg_function == 'L1_d':\n",
        "            return L1_regularisation(self.network, self.weight).Apply_L1_grad()"
      ],
      "metadata": {
        "id": "H0NBqkSyhNWW"
      },
      "id": "H0NBqkSyhNWW",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ApplyReg(\"L2\", a).do_reg()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r_OCAEvS04f",
        "outputId": "81857408-d1a8-459e-e815-afd4150925c1"
      },
      "id": "_r_OCAEvS04f",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.07999452224272594"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ApplyRegDerv(\"L2_d\",a, a[0].weight).do_reg_derv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "cmvClNIqTNAe",
        "outputId": "ebc6cb86-0a03-4135-c917-e792148ad225"
      },
      "id": "cmvClNIqTNAe",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ApplyRegDerv' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-23a127c9630d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mApplyRegDerv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"L2_d\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_reg_derv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ApplyRegDerv' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, learning_rate=0.01, decay=0.0):\n",
        "        self.eta = learning_rate  # Learning rate\n",
        "        self.decay = decay        # Learning rate decay\n",
        "        self.iterations = 0       # Track steps for decay\n",
        "\n",
        "    def update(self, weights, gradients):\n",
        "        \"\"\"Update weights using gradients.\"\"\"\n",
        "        weights -= self.eta * gradients  # Core SGD update rule\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Decay learning rate after each epoch (optional).\"\"\"\n",
        "        if self.decay > 0:\n",
        "            self.eta = self.eta / (1 + self.decay * self.iterations)\n",
        "            self.iterations += 1"
      ],
      "metadata": {
        "id": "mQDo0hSwfYx4"
      },
      "id": "mQDo0hSwfYx4",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CalculateAllLoss:\n",
        "  def __init__(self, X_train, y_predicted,network, y_true, primary_loss, weight_decay=0, regularisation_fn=None):\n",
        "    self.y_predicted = y_predicted\n",
        "    self.y_true = y_true\n",
        "    self.network = network\n",
        "    self.X_train = X_train\n",
        "    self.loss_value = primary_loss\n",
        "    self.weight_decay = weight_decay\n",
        "    self.regularisation_fn= regularisation_fn\n",
        "    self.calc_accuracy_loss()\n",
        "\n",
        "\n",
        "  #def overall_loss(self):\n",
        "    \"\"\"\n",
        "    Calculates the total loss of the network.\n",
        "\n",
        "    This includes:\n",
        "    - The primary loss (e.g., Cross-Entropy Loss)\n",
        "    - Optional regularization (like L2 regularization)\n",
        "\n",
        "    Parameters:\n",
        "    - network: The neural network model.\n",
        "    - Y_pred: The predicted output from the network.\n",
        "    - Y_true: The actual labels (ground truth).\n",
        "    - loss_fn: The loss function to be used (e.g., CrossEntropy_loss).\n",
        "    - weight_decay: A coefficient for regularization (default is 0, meaning no regularization).\n",
        "    - regularisation_fn: A function for computing the regularization term (optional).\n",
        "\n",
        "    Returns:\n",
        "    - Total loss value.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "  def calc_accuracy_loss(self):\n",
        "    \"\"\"\n",
        "    Computes the accuracy and loss for a given neural network.\n",
        "\n",
        "    Parameters:\n",
        "    - network: The neural network model.\n",
        "    - X: Input data (features for prediction).\n",
        "    - Y: Actual labels (ground truth values).\n",
        "    - loss_fn: The loss function to be used.\n",
        "    - weight_decay: Regularization strength (default is 0, meaning no regularization).\n",
        "    - regularisation_fn: A function to compute the regularization term (optional).\n",
        "\n",
        "    Returns:\n",
        "    - accuracy: The percentage of correctly classified examples.\n",
        "    - loss: The computed total loss.\n",
        "    \"\"\"\n",
        "    # Get network predictions self.y_prediction from model\n",
        "    total_loss = self.loss_value  # Compute primary loss\n",
        "\n",
        "    if self.weight_decay > 0 and self.regularisation_fn:\n",
        "        regularized_val = ApplyReg(self.regularisation_fn, self.network).do_reg()\n",
        "        print(f\"Reg value: {regularized_val}\")\n",
        "        total_loss += self.weight_decay * regularized_val # Add regularization term if applicable  # Compute total loss\n",
        "\n",
        "\n",
        "    # Ensure dimensions match between input and labels\n",
        "    assert self.X_train.shape[1] == self.y_true.shape[1], \"Mismatch in batch size between inputs and labels\"\n",
        "\n",
        "    # Compute accuracy by comparing predicted vs actual labels\n",
        "    batch_size = self.X_train.shape[1]  # Number of examples\n",
        "    correct_predictions = np.sum(np.argmax(self.y_predicted, axis=0) == np.argmax(self.y_true, axis=0))\n",
        "    accuracy = correct_predictions / batch_size  # Compute accuracy as a fraction\n",
        "\n",
        "    return accuracy, total_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "6B3z_WoEeGA5"
      },
      "id": "6B3z_WoEeGA5",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k, l = CalculateAllLoss(X_train=X_new,y_predicted=q,network=a,y_true=y_new,primary_loss=z, weight_decay=0.1, regularisation_fn=\"L2\").calc_accuracy_loss()\n",
        "k, l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHlffnq-Y6Ym",
        "outputId": "0d6db42b-8e42-41db-8ece-a9cc0140a9e4"
      },
      "id": "dHlffnq-Y6Ym",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reg value: -0.07999452224272594\n",
            "Reg value: -0.07999452224272594\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.11486666666666667, 0.08721371156977703)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3fZCFuv9YWra"
      },
      "id": "3fZCFuv9YWra",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "  def __init__(self, gradient, eta = 0.01):\n",
        "    self.eta = eta\n",
        "    self.grad = gradient\n",
        "    self.new_updates = 0\n",
        "    self.Apply_sgd()\n",
        "\n",
        "  def Apply_sgd(self):\n",
        "    self.new_updates= self.eta * self.grad\n",
        "    print(\"shape of self grad\" , self.grad.shape)\n",
        "    print(\"shape of self new updates\" , self.new_updates.shape)\n",
        "    return self.new_updates\n",
        "\n",
        "class MomentumGD:\n",
        "  def __init__(self, gradient, eta = 0.001 , beta = 0.9):\n",
        "    self.grad = gradient\n",
        "    self.eta = eta\n",
        "    self.beta = beta\n",
        "    self.new_updates = 0\n",
        "    self.Apply_mgd()\n",
        "\n",
        "  def Apply_mgd(self):\n",
        "    self.new_updates = [self.beta * prev + self.eta * grad for prev, grad in zip(self.new_updates, self.grad)]\n",
        "    return self.new_updates\n",
        "\n",
        "class Nag:\n",
        "  def __init__(self, gradient, eta = 0.001, beta = 0.9 ):\n",
        "    self.new_updates = 0\n",
        "    self.gradient = gradient\n",
        "    self.eta = eta\n",
        "    self.beta = beta\n",
        "    self.Apply_nag()\n",
        "\n",
        "  def Apply_nag(self):\n",
        "    L = len(self.gradient_w_L)\n",
        "\n",
        "    for j in range (L -1):\n",
        "      self.look = self.beta * self.new_updates + (self.gradient[j] - self.beta * self.new_updates)\n",
        "      self.new_updates = self.eta * self.look\n",
        "      return self.new_updates\n",
        "\n",
        "class CallOptimizers:\n",
        "\n",
        "  def __init__(self, optimization_function, gradient, eta, beta):\n",
        "    self.optimization_function = optimization_function\n",
        "    self.eta = eta\n",
        "    self.beta = beta\n",
        "    self.gradient = gradient\n",
        "    self.apply_optimization()\n",
        "\n",
        "  def apply_optimization(self):\n",
        "    if self.optimization_function == 'sgd':\n",
        "        return SGD(self.gradient, self.eta).Apply_sgd()\n",
        "    elif self.optimization_function == 'Momentum':\n",
        "        return MomentumGD(self.gradient, self.eta, self.beta).Apply_mgd()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZE3u3I_l6nGy"
      },
      "id": "ZE3u3I_l6nGy",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_norm= Normalize(X_train).Norm_reshape()\n",
        "y_norm =OneHotEncoder(X_train, y_train).onehot_encode()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "2uMpqFSKffHb",
        "outputId": "2dc9358e-9b9c-475c-f8ad-426725c585fd"
      },
      "id": "2uMpqFSKffHb",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Normalize' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ca7c9d88fde2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_norm\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNorm_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_norm\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monehot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Normalize' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "class Backpropagation:\n",
        "\n",
        "    def __init__(self, loss_function, X_train, y_train, activation_fn, layers_dimensions ,method, batch_size, optimizer_fn, epochs,weight_decay,regularization_fn,grad_reglr_fn ,X_val=None, t_val=None, use_wandb=False, optim_params=None):\n",
        "        self.loss_function = loss_function\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.activation_fn = activation_fn\n",
        "        self.method = method\n",
        "        self.n = layers_dimensions\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.optimizer_fn = optimizer_fn\n",
        "        self.weight_decay = weight_decay\n",
        "        self.regularization_fn = regularization_fn\n",
        "        self.grad_reglr_fn = grad_reglr_fn\n",
        "        self.batches_number = self.X_train.shape[1]//self.batch_size\n",
        "        z = Weight_bias(self.n, self.activation_fn, self.y_train, self.method)\n",
        "        self.init_network = z.Init_network()\n",
        "        self.network = Feedforward(self.X_train,self.activation_fn,self.method, self.init_network, self.batch_size).Forward_prop()\n",
        "\n",
        "        # print(\"X_norm shape \",self.X_norm.shape)\n",
        "        # print(\"y_norm shape \",self.y_norm.shape)\n",
        "        # print(\"H shape \",self.H.shape)\n",
        "        # print(\"length self.network \",len(self.network))\n",
        "        # print(\"self.batch_no \",self.batches_number)\n",
        "        # print(\"self.batch_size \",self.batch_size)\n",
        "\n",
        "        #self.loss = map_losses[loss]\n",
        "        # self.use_wandb = use_wandb\n",
        "        # if t_val is not None:\n",
        "        #     self.X_val = X_val\n",
        "        #     self.layers[0].a_val = X_val\n",
        "        #     self.t_val = t_val\n",
        "        #self.param_init(optimizer, optim_params)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def backward_propagation(self):\n",
        "        L = len(self.network)\n",
        "        self.y_predicted = self.network[L-1].h\n",
        "        # Initialize variables neesed to keep track of loss\n",
        "        self.eta_hist = []\n",
        "        self.loss_history = []\n",
        "        self.accuracy_history = []\n",
        "        self.loss_hist_val = []\n",
        "        self.accuracy_hist_val = []\n",
        "\n",
        "        # Perform Backprop\n",
        "        for epochs in tqdm(range(self.epochs)):\n",
        "            print(f\"No of epoch rotation {epochs}\")\n",
        "\n",
        "\n",
        "            self.loss = callloss(self.loss_function, self.y_train, self.y_predicted).give_loss()\n",
        "            print(\"loss : \" , self.loss)\n",
        "            accuracy, overall_loss = CalculateAllLoss(self.X_train, self.y_predicted, self.network, self.y_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "            self.accuracy = accuracy\n",
        "            self.overall_loss = overall_loss\n",
        "            self.loss_history.append(self.overall_loss)\n",
        "            self.accuracy_history.append(self.accuracy)\n",
        "            print(\"Overall_loss_history : \" , self.loss_history)\n",
        "            print(\"accuracy_history : \" , self.accuracy_history)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            for batch in range(self.batches_number):\n",
        "                # print(f\"No of batch rotation {batch}\")\n",
        "\n",
        "                X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "                self.X_batch = X_batch\n",
        "                # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "                # print(\"y_predicted shape : \" , self.y_predicted.shape)\n",
        "\n",
        "                y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "                # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "                self.y_true_batch = y_true_batch\n",
        "                self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "                assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "                # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "                # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "\n",
        "\n",
        "\n",
        "                self.network[-1].d_h = callloss(self.loss_function,self.y_true_batch, self.y_pred_batch).give_gradloss()\n",
        "                #print(\"network -[-1].d_h : \" , self.network[-1].d_h.shape)\n",
        "                A_k = apply_activation(self.activation_fn[-1], self.network[-1].a[:, batch*self.batch_size : (batch+1)*self.batch_size]).do_activation_derivative()\n",
        "                #print(\"shape A_ k : \" , A_k.shape)\n",
        "                self.network[-1].d_a = self.network[-1].d_h * A_k\n",
        "                #print(\"network -[-1].d_a : \" , self.network[-1].d_a.shape)\n",
        "\n",
        "                self.network[-1].d_w = self.network[-1].d_a @ self.network[-2].h[:, batch*self.batch_size : (batch+1)*self.batch_size].T\n",
        "                print(\"network -[-1].d_w : \" , self.network[-1].d_w.shape)\n",
        "                d_b = -np.sum(self.network[-1].d_a)\n",
        "                self.network[-1].d_b = d_b.reshape(-1 , 1)\n",
        "                print(\"network -[-1].d_b : \" , self.network[-1].d_b.shape)\n",
        "\n",
        "                op_prev_w =  CallOptimizers(optimization_function=self.optimizer_fn, gradient = self.network[-1].d_w, eta = 0.001, beta= 0.9)\n",
        "                op_prev_b = CallOptimizers(optimization_function=self.optimizer_fn, gradient = self.network[-1].d_b, eta = 0.001, beta= 0.9)\n",
        "                self.network[-1].Weight_updates =op_prev_w.apply_optimization()\n",
        "                print(\"len network -[-1].weight_updates : \" , len(self.network[-1].Weight_updates), self.network[-1].Weight_updates.size )\n",
        "                self.network[-1].bias_updates = op_prev_b.apply_optimization()\n",
        "                print(\"len network -[-1].bias_updates : \" , len(self.network[-1].bias_updates), self.network[-1].bias_updates.size)\n",
        "\n",
        "\n",
        "                for k in range(L-2,-1,-1):\n",
        "                    # print(f\"No of layers rotation {k}\")\n",
        "\n",
        "                    self.network[k].d_h = self.network[k + 1].weight.T @ self.network[k + 1].d_a\n",
        "                    #print(f\"shape self.network-{k}.d_h : \" , self.network[k].d_h.shape)\n",
        "                    act_derv =  apply_activation(self.activation_fn[k], self.network[k].a[:, batch*self.batch_size : (batch+1)*self.batch_size])\n",
        "                    self.network[k].d_a = self.network[k].d_h * act_derv.do_activation_derivative()\n",
        "                    #print(f\"shape self.network-{k}.d_a : \" , self.network[k].d_a.shape)\n",
        "\n",
        "                    self.network[k].d_w = self.network[k].d_a @ self.network[k-1].h[:, batch*self.batch_size : (batch+1)*self.batch_size].T\n",
        "                    print(f\"shape self.network-{k}.d_w : \" , self.network[k].d_w.shape)\n",
        "                    derv_bias = -np.sum(self.network[k].d_a, axis=1)\n",
        "                    self.network[k].d_b = derv_bias.reshape(-1 , 1)\n",
        "                    print(f\"shape self.network-{k}.d_b : \" , self.network[k].d_b.shape)\n",
        "\n",
        "                    op_w = CallOptimizers(optimization_function=self.optimizer_fn, gradient = self.network[k].d_w, eta = 0.001, beta= 0.9)\n",
        "                    op_b = CallOptimizers(optimization_function=self.optimizer_fn, gradient = self.network[k].d_b, eta = 0.001, beta= 0.9)\n",
        "                    self.network[k].Weight_updates =  op_w.apply_optimization()\n",
        "                    print(\"len network -[-1].weight_updates : \" , len(self.network[k].Weight_updates), self.network[k].Weight_updates.size )\n",
        "                    #print(f\"len network -{k}.weight_updates : \" , len(self.network[k].Weight_updates))\n",
        "                    self.network[k].bias_updates = op_b.apply_optimization()\n",
        "                    print(\"len network -[-1].bias_updates : \" , len(self.network[k].bias_updates), self.network[k].bias_updates.size)\n",
        "                    #print(f\"len network -{k}.bias_updates : \" , len(self.network[k].bias_updates))\n",
        "\n",
        "\n",
        "\n",
        "                    # w_updates = CallOptimizers(optimization_function=self.optimizer_fn, gradient = grad_w_L, eta = 0.001, beta= 0.9).apply_optimization()\n",
        "                    # b_updates = CallOptimizers(optimization_function=self.optimizer_fn, gradient = grad_b_L, eta = 0.001, beta= 0.9).apply_optimization()\n",
        "                    # print(\"shape w_updates : \" ,len(w_updates))\n",
        "                    # print(\"shape b_updates : \" ,len(b_updates))\n",
        "                    # for i in range(L):\n",
        "                    #   print(f\"{i}th layer w\" , network[i].weight.shape)\n",
        "                    #   print(f\"{i}th layer b\" , network[i].bias.shape)\n",
        "                    #   print(f\"{i}th layer h\" , network[i].h.shape)\n",
        "                    #   print(f\"{i}th layer a\" , network[i].a.shape)\n",
        "                    #   print(f\"{i}th layer g\" , network[i].g.shape)\n",
        "                    #   print(f\"{i}th layer d_g\" , network[i].d_g.shape)\n",
        "                    #   print(f\"{i}th layer d_a\" , network[i].d_a.shape)\n",
        "                    #   print(f\"{i}th layer d_h\" , network[i].d_h.shape)\n",
        "                    #   print(f\"{i}th layer d_w\" , network[i].d_w.shape)\n",
        "                    #   print(f\"{i}th layer d_b\" , network[i].d_b.shape)\n",
        "                for i in range(L):\n",
        "                    print(f\"{i} th layer w\" , self.network[i].weight.shape)\n",
        "                    print(f\"{i} th layer b\" , self.network[i].bias.shape)\n",
        "                    print(f\"{i} th layer w_update\" , self.network[i].Weight_updates.shape)\n",
        "                    print(f\"{i} th layer b_update\" , self.network[i].bias_updates.shape)\n",
        "                for k in range(L):\n",
        "                  self.network[k].weight -= self.network[k].Weight_updates\n",
        "                  self.network[k].bias -= self.network[k].bias_updates\n",
        "\n",
        "                self.network = Feedforward(self.X_train,self.activation_fn,self.method, self.network, self.batch_size).Forward_prop()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jHfISCRCqJDP"
      },
      "id": "jHfISCRCqJDP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Backpropagation(loss_function=\"se\",X_train= X_new, y_train= y_new,layers_dimensions= [784, 128, 64, 10],activation_fn=['relu', 'relu', 'softmax'], batch_size=1000,optimizer_fn=\"sgd\", epochs= 5,weight_decay=0.01,grad_reglr_fn= \"L1_d\", regularization_fn=\"L1\", method= \"Xavier_N\" ).backward_propagation()"
      ],
      "metadata": {
        "id": "_WFk5jtH19Ti",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "outputId": "4387f7b3-fb56-45f1-94e6-bcb5a6cb1334"
      },
      "id": "_WFk5jtH19Ti",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of epoch rotation 0\n",
            "loss :  0.09187630367383108\n",
            "Reg value: -0.299299735760645\n",
            "Reg value: -0.299299735760645\n",
            "Overall_loss_history :  [0.08888330631622463]\n",
            "accuracy_history :  [0.07641666666666666]\n",
            "network -[-1].d_w :  (10, 64)\n",
            "network -[-1].d_b :  (1, 1)\n",
            "len network -[-1].weight_updates :  10 640\n",
            "len network -[-1].bias_updates :  1 1\n",
            "shape self.network-1.d_w :  (64, 128)\n",
            "shape self.network-1.d_b :  (64, 1)\n",
            "len network -[-1].weight_updates :  64 8192\n",
            "len network -[-1].bias_updates :  64 64\n",
            "shape self.network-0.d_w :  (128, 10)\n",
            "shape self.network-0.d_b :  (128, 1)\n",
            "len network -[-1].weight_updates :  128 1280\n",
            "len network -[-1].bias_updates :  128 128\n",
            "0 th layer w (128, 784)\n",
            "0 th layer b (128, 1)\n",
            "0 th layer w_update (128, 10)\n",
            "0 th layer b_update (128, 1)\n",
            "1 th layer w (64, 128)\n",
            "1 th layer b (64, 1)\n",
            "1 th layer w_update (64, 128)\n",
            "1 th layer b_update (64, 1)\n",
            "2 th layer w (10, 64)\n",
            "2 th layer b (10, 1)\n",
            "2 th layer w_update (10, 64)\n",
            "2 th layer b_update (1, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (128,784) (128,10) (128,784) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-22d9cd68f029>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"se\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0my_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers_dimensions\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'softmax'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sgd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_reglr_fn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"L1_d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"Xavier_N\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-61-9a3322937b81>\u001b[0m in \u001b[0;36mbackward_propagation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{i} th layer b_update\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_updates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWeight_updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (128,784) (128,10) (128,784) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " a,b,c,d = Feedforward(X_new,[784, 128, 64, 10],['sigmoid','relu','softmax'],\"Xavier_U\").Forward_prop()"
      ],
      "metadata": {
        "id": "kGOmz4yXVtvE"
      },
      "id": "kGOmz4yXVtvE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c,d = Feedforward(X,[2,3,2,1],['sigmoid','relu','softmax'],\"Xavier_U\").Forward_prop()\n",
        "a,b,c,d"
      ],
      "metadata": {
        "id": "92hGBoNHLYP3"
      },
      "id": "92hGBoNHLYP3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_new.size"
      ],
      "metadata": {
        "id": "yx1WfEF9-dbd"
      },
      "id": "yx1WfEF9-dbd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = callloss(\"se\", y_new, d).give_loss()\n",
        "print(a)"
      ],
      "metadata": {
        "id": "QfbsEUn_-SnV"
      },
      "id": "QfbsEUn_-SnV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Backpropagation(loss_function=\"se\",X_train= X_train, y_train= y_train,layers_dimensions= [784, 128, 64 ,10],activation_fn=['sigmoid', 'sigmoid', 'sigmoid'], batch_size=10000,optimizer_fn=\"Adam\", epochs= 5,weight_decay=0,grad_reglr_fn=None, method= \"Xavier_U\" ).backward_propogation()"
      ],
      "metadata": {
        "id": "wss0tLWsslKV"
      },
      "id": "wss0tLWsslKV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MhJrZkPpp7Zv"
      },
      "id": "MhJrZkPpp7Zv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TMAZN5qaVr3t"
      },
      "id": "TMAZN5qaVr3t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Backprop(loss_function=\"se\",X_train= X_train, y_train= y_train,layers_dimensions= [784, 128, 64 ,10],activation_fn=['sigmoid', 'sigmoid', 'softmax'], batch_size=10000,optimizer_fn=\"Adam\", epochs= 5, method= \"Xavier_U\" ).backward_propogation()"
      ],
      "metadata": {
        "id": "CyW79eripCll"
      },
      "id": "CyW79eripCll",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new= Normalize(X_train).Norm_reshape()\n",
        "y_new =OneHotEncoder(X_train,y_train).onehot_encode()"
      ],
      "metadata": {
        "id": "lhnh7_96ftQs"
      },
      "id": "lhnh7_96ftQs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new.shape"
      ],
      "metadata": {
        "id": "gFlE6dbNAJE7"
      },
      "id": "gFlE6dbNAJE7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c,d = Feedforward(X,[2,3,3,1],['relu','relu','softmax'],\"Xavier_U\").Forward_prop()\n",
        "a,b,c,d"
      ],
      "metadata": {
        "id": "NV7gYASA0m8t"
      },
      "id": "NV7gYASA0m8t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(b[\"a\" + str(4-1)])"
      ],
      "metadata": {
        "id": "Xmxy4rN2cTM-"
      },
      "id": "Xmxy4rN2cTM-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(apply_activation_derivative([\"sigmoid\"], b[\"a\" + str(4-1)]).do_activation_derivative()))"
      ],
      "metadata": {
        "id": "X06X6gfoV1Ol"
      },
      "id": "X06X6gfoV1Ol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2064d83c-b27e-46e6-bef2-c24d9faf70ff",
      "metadata": {
        "id": "2064d83c-b27e-46e6-bef2-c24d9faf70ff"
      },
      "outputs": [],
      "source": [
        "Initilize(3,5,\"Xavier_N\").Init_weight(\"Xavier_N\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [relu,relu,sigmoid]\n",
        "print(x[1])"
      ],
      "metadata": {
        "id": "AIOXHXcuRF9-"
      },
      "id": "AIOXHXcuRF9-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weight_bias([4,5,3,1],['relu','relu','sigmoid'],\"Xavier_N\").Init_network([4,5,3,1])"
      ],
      "metadata": {
        "id": "4NKWIZNDdN7u"
      },
      "id": "4NKWIZNDdN7u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callloss(\"se\",y_new,d).give_loss()"
      ],
      "metadata": {
        "id": "BOwYYfPZCFzT"
      },
      "id": "BOwYYfPZCFzT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CmG8ToWcnOR7"
      },
      "id": "CmG8ToWcnOR7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}