{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "EuDCYeTFOWhL"
      },
      "id": "EuDCYeTFOWhL",
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([[8,8,1],[7,9,1],[6,10,0],[5,5,0]], columns=['cgpa', 'profile_score', 'placed'])\n",
        "X = df[['cgpa', 'profile_score']].values[0].reshape(2,1) # Shape(no of features, no. of training example)\n",
        "y = df[['placed']].values[0][0]"
      ],
      "metadata": {
        "id": "H2q--JEjvU-Q"
      },
      "id": "H2q--JEjvU-Q",
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "aNIlLeY_gnbE"
      },
      "id": "aNIlLeY_gnbE",
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62rXWMGhyBbp",
        "outputId": "445d97e6-7c71-41eb-ff02-19aebbb783f3"
      },
      "id": "62rXWMGhyBbp",
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47040000"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy  as np\n",
        "\n",
        "class OneHotEncoder:\n",
        "\n",
        "    def __init__(self,x, y):\n",
        "        self.y = y\n",
        "        self.x = x\n",
        "        #self.num_class = num_class\n",
        "        self.onehot_encode()\n",
        "\n",
        "    def onehot_encode(self):\n",
        "        onehot = np.zeros((self.x.shape[0], 10))\n",
        "\n",
        "        for i, j in zip(range(len(self.x)), self.y):\n",
        "            onehot[i, j] = 1\n",
        "        return onehot.T\n",
        "\n",
        "class Normalize:\n",
        "\n",
        "    def __init__(self, unprocessed_X):\n",
        "        self.unprocessed_X = unprocessed_X\n",
        "        self.Norm_reshape()\n",
        "\n",
        "    def Norm_reshape(self):\n",
        "        X_norm = np.reshape(self.unprocessed_X,(self.unprocessed_X.shape[0],784)).T/255\n",
        "        X_norm= np.array(X_norm)\n",
        "\n",
        "        return X_norm"
      ],
      "metadata": {
        "id": "BW34ZLm_wv9o"
      },
      "id": "BW34ZLm_wv9o",
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new= Normalize(X_train).Norm_reshape()"
      ],
      "metadata": {
        "id": "RUTyqFl2zbjL"
      },
      "id": "RUTyqFl2zbjL",
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new= Normalize(X_train).Norm_reshape()\n",
        "y_new =OneHotEncoder(X_train,y_train).onehot_encode()"
      ],
      "metadata": {
        "id": "K4hGhlz9w6pa"
      },
      "id": "K4hGhlz9w6pa",
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOeMXNH9g0hp",
        "outputId": "eea972fe-7042-4d87-e35d-89d4cf354a06"
      },
      "id": "IOeMXNH9g0hp",
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CalActivation:\n",
        "\n",
        "  def __init__(self, m):\n",
        "    self.m = m\n",
        "\n",
        "# This function calculates sigmoid activation function\n",
        "class sigmoid(CalActivation):\n",
        "\n",
        "    def use_sigmoid(self):\n",
        "        sig = np.where(self.m >= 0,1/(1 + np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) - 1))\n",
        "        return sig\n",
        "\n",
        "    def sigmoid_d(self):\n",
        "        sig_d = self.use_sigmoid()  # Compute sigmoid\n",
        "        # sig_d = np.where(self.m >= 0,1/(1 + np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) + 1))\n",
        "        return sig_d * (1 - sig_d)\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates tanh activation function\n",
        "class tanh(CalActivation):\n",
        "\n",
        "    def use_tanh(self):\n",
        "        z = (np.exp(self.m) - np.exp(-self.m))/(np.exp(self.m) + np.exp(-self.m))\n",
        "        return z\n",
        "\n",
        "    def tanh_d(self):\n",
        "        z = self.use_tanh()\n",
        "        return (1 - (z)**2)\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates relu activation function\n",
        "class relu(CalActivation):\n",
        "\n",
        "    def use_relu(self):\n",
        "        return np.where(self.m > 0, self.m, 0)\n",
        "        #     return self.m\n",
        "        # else:\n",
        "        #     return 0\n",
        "\n",
        "    def relu_d(self):\n",
        "        return np.where(self.m >0 , 1 ,0)\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates softmax activation function\n",
        "class softmax(CalActivation):\n",
        "\n",
        "    def use_softmax(self):\n",
        "        x = np.exp(self.m)/np.sum(np.exp(self.m), axis=0)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def softmax_d(self):\n",
        "        z=self.m - np.max(self.m,axis=0)\n",
        "        soft=np.exp(z)/np.sum(np.exp(z),axis=0)\n",
        "        return soft*(1-soft)\n",
        "\n",
        "\n",
        "\n",
        "# This function call different activation function\n",
        "class apply_activation(CalActivation):\n",
        "\n",
        "    def __init__(self, activation_function, m):\n",
        "        super().__init__(m)\n",
        "        self.activation_function = activation_function.lower()\n",
        "\n",
        "    def do_activation(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.m).use_sigmoid()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.m).use_relu()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.m).use_tanh()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.m).use_softmax()\n",
        "\n",
        "    def do_activation_derivative(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.m).sigmoid_d()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.m).relu_d()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.m).tanh_d()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.m).softmax_d()\n",
        "        else:\n",
        "           raise ValueError(\"Unknown activation function\")\n"
      ],
      "metadata": {
        "id": "01ZCcJdxviK6"
      },
      "id": "01ZCcJdxviK6",
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-qdvScjQHQ2"
      },
      "id": "2-qdvScjQHQ2",
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CalLoss:\n",
        "    def __init__(self, y, y_pred):\n",
        "      self.y = y\n",
        "      self.y_predicted = y_pred\n",
        "      if y.shape != y_pred.shape:\n",
        "            raise ValueError(f\"Shape mismatch: y shape is {self.y.shape}, y_predicted shape is {self.y_predicted.shape}\")\n",
        "\n",
        "\n",
        "class CrossEntropy(CalLoss):\n",
        "\n",
        "    def give_celoss(self):\n",
        "        return -np.mean(self.y * np.log(self.y_predicted))\n",
        "\n",
        "    def Give_cegrad(self):\n",
        "        grad = -self.y/(self.y_predicted)\n",
        "        return grad\n",
        "\n",
        "class SquaredError(CalLoss):\n",
        "\n",
        "    def give_seloss(self):\n",
        "        return np.mean((self.y - self.y_predicted) ** 2)\n",
        "\n",
        "    def Give_segrad(self):\n",
        "        res = -2*(self.y - self.y_predicted)\n",
        "        return res\n",
        "\n",
        "class callloss(CalLoss):\n",
        "    def __init__(self, loss_function, y, y_pred):\n",
        "        self.loss_function = loss_function.lower()\n",
        "        super().__init__(y, y_pred)\n",
        "\n",
        "    def give_loss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return CrossEntropy(self.y, self.y_predicted).give_celoss()\n",
        "        elif self.loss_function == 'se':\n",
        "            return SquaredError(self.y, self.y_predicted).give_seloss()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss function: {self.loss_function}\")\n",
        "\n",
        "    def give_gradloss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return CrossEntropy(self.y, self.y_predicted).Give_cegrad()\n",
        "        elif self.loss_function == 'se':\n",
        "            return SquaredError(self.y, self.y_predicted).Give_segrad()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss function: {self.loss_function}\")"
      ],
      "metadata": {
        "id": "iA7A6vN-l5K1"
      },
      "id": "iA7A6vN-l5K1",
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callloss(\"se\", y_new, q).give_loss()\n",
        "y_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOKbXCm5ZTK8",
        "outputId": "9ed0e55b-5b0c-4538-a62e-3af222c15b28"
      },
      "id": "SOKbXCm5ZTK8",
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 60000)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyUj2hy6Bdt6",
        "outputId": "065b51c1-9b05-43fb-a45f-865c504f8d65"
      },
      "id": "uyUj2hy6Bdt6",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 60000)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "895d572d-2ffd-466a-a260-e57859a6449e",
      "metadata": {
        "id": "895d572d-2ffd-466a-a260-e57859a6449e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Initilize:\n",
        "    def __init__(self, layer_dimension, activation_function, y_train, method = \"Xavier_U\"):\n",
        "        self.n = layer_dimension\n",
        "        self.activation_fn = activation_function\n",
        "        self.y = y_train\n",
        "        self.Init_method = method\n",
        "\n",
        "class InitializeWeights(Initilize):\n",
        "    def __init__(self, ip_size, op_size, activation_function, batch_size, method):\n",
        "        super().__init__([ip_size, op_size], activation_function, batch_size, method)  # Properly inherit Initilize attributes\n",
        "        self.i_size = ip_size\n",
        "        self.o_size = op_size\n",
        "        self.Init_weights()\n",
        "\n",
        "\n",
        "    def Init_weights(self):\n",
        "        np.random.seed(0)\n",
        "\n",
        "        if self.Init_method == \"Xavier_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(1 / self.i_size)\n",
        "          self.weight = np.random.randn(self.o_size,self.i_size)*a\n",
        "\n",
        "        elif self.Init_method == \"Xavier_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / (self.o_size + self.i_size))\n",
        "          self.weight = np.random.uniform((-a), a,( self.o_size,self.i_size))\n",
        "\n",
        "        elif self.Init_method == \"He_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(2 / self.i_size)\n",
        "          self.weight = np.random.randn(self.o_size,self.i_size)*a\n",
        "\n",
        "        elif self.Init_method == \"He_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / self.i_size)\n",
        "          self.weight = np.random.uniform(-a, a, (self.o_size,self.i_size))\n",
        "\n",
        "        elif self.Init_method == \"Random\":\n",
        "          np.random.seed(0)\n",
        "          self.weight = np.random.randn(self.o_size,self.i_size)*0.01\n",
        "        else:\n",
        "          raise ValueError(f\"Unknown initialization method: {self.Init_method}\")\n",
        "\n",
        "\n",
        "        # Initialize biases and activations\n",
        "        self.bias = np.zeros((self.o_size, 1))\n",
        "        self.a = np.zeros((self.o_size, len(self.y[1])))\n",
        "        self.h = np.zeros((self.o_size, len(self.y[1])))\n",
        "\n",
        "        # Activation function and its derivative\n",
        "        self.g = apply_activation(self.activation_fn, self.a).do_activation()\n",
        "        self.d_g = apply_activation(self.activation_fn, self.a).do_activation_derivative()\n",
        "\n",
        "        # Gradients\n",
        "        self.d_a = np.zeros_like(self.a)\n",
        "        self.d_h = np.zeros_like(self.h)\n",
        "        self.d_w = np.zeros_like(self.weight)\n",
        "        self.d_b = np.zeros_like(self.bias)\n",
        "        self.Weight_updates = np.zeros_like(self.weight)\n",
        "        self.bias_updates = np.zeros_like(self.bias)\n",
        "\n",
        "\n",
        "\n",
        "class Weight_bias(Initilize):\n",
        "    def __init__(self, layer_dimension, activation_function, y_train, method=\"Xavier_U\"):\n",
        "        super().__init__(layer_dimension, activation_function, y_train, method)\n",
        "        self.network = []\n",
        "\n",
        "\n",
        "    def Init_network(self):\n",
        "\n",
        "        for i in range(1, len(self.n)):\n",
        "          self.network.append(InitializeWeights( self.n[i-1], self.n[i], self.activation_fn[i-1], self.y, self.Init_method))\n",
        "\n",
        "        return self.network\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_new[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ehsUHC-CheX",
        "outputId": "d50d5ebc-6596-40f7-ad6f-ce2f49a58827"
      },
      "id": "_ehsUHC-CheX",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = Weight_bias([784, 128, 64, 10],['sigmoid','relu','softmax'], y_new, method= \"Xavier_U\").Init_network()\n",
        "for i in range(3):\n",
        "  print(f\"layer {i}\")\n",
        "  print(f\"{i}th layer w\" , a[i].weight.shape)\n",
        "  print(f\"{i}th layer b\" , a[i].bias.shape)\n",
        "  print(f\"{i}th layer h\" , a[i].h.shape)\n",
        "  print(f\"{i}th layer a\" , a[i].a.shape)\n",
        "  print(f\"{i}th layer g\" , a[i].g.shape)\n",
        "  print(f\"{i}th layer d_a\" , a[i].d_a.shape)\n",
        "  print(f\"{i}th layer d_h\" , a[i].d_h.shape)\n",
        "  print(f\"{i}th layer d_w\" , a[i].d_w.shape)\n",
        "  print(f\"{i}th layer d_b\" , a[i].d_b.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeaxRE4jdJ5C",
        "outputId": "358d67a7-88fc-46c8-f403-cd9837cd4ef6"
      },
      "id": "IeaxRE4jdJ5C",
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-106-526b8f77b8ac>:12: RuntimeWarning: divide by zero encountered in divide\n",
            "  sig = np.where(self.m >= 0,1/(1 + np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) - 1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer 0\n",
            "0th layer w (128, 784)\n",
            "0th layer b (128, 1)\n",
            "0th layer h (128, 60000)\n",
            "0th layer a (128, 60000)\n",
            "0th layer g (128, 60000)\n",
            "0th layer d_a (128, 60000)\n",
            "0th layer d_h (128, 60000)\n",
            "0th layer d_w (128, 784)\n",
            "0th layer d_b (128, 1)\n",
            "layer 1\n",
            "1th layer w (64, 128)\n",
            "1th layer b (64, 1)\n",
            "1th layer h (64, 60000)\n",
            "1th layer a (64, 60000)\n",
            "1th layer g (64, 60000)\n",
            "1th layer d_a (64, 60000)\n",
            "1th layer d_h (64, 60000)\n",
            "1th layer d_w (64, 128)\n",
            "1th layer d_b (64, 1)\n",
            "layer 2\n",
            "2th layer w (10, 64)\n",
            "2th layer b (10, 1)\n",
            "2th layer h (10, 60000)\n",
            "2th layer a (10, 60000)\n",
            "2th layer g (10, 60000)\n",
            "2th layer d_a (10, 60000)\n",
            "2th layer d_h (10, 60000)\n",
            "2th layer d_w (10, 64)\n",
            "2th layer d_b (10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Pre_Feedforward:\n",
        "\n",
        "  def __init__(self, inputs, w, b, activation_fn):\n",
        "    self.ip= inputs\n",
        "    self.w = w\n",
        "    self.b = b\n",
        "    self.activation_fn = activation_fn\n",
        "    self.Preactivation_cal()\n",
        "\n",
        "\n",
        "  def Preactivation_cal(self):\n",
        "\n",
        "    A = np.dot(self.w, self.ip) + self.b\n",
        "    H = apply_activation(self.activation_fn, A).do_activation()\n",
        "    #cache = (H, A)\n",
        "    #self.cache[\"Input\"] = self.Prev_layer_H\n",
        "    #self.cache[\"Pre_act\"] = H\n",
        "    return A , H\n",
        "\n",
        "class Feedforward:\n",
        "\n",
        "  def __init__(self,X_train, activation_fn, method, network):\n",
        "    self.input = X_train\n",
        "    self.activation_fn = activation_fn\n",
        "    self.method = method\n",
        "    self.network = network\n",
        "\n",
        "  def Forward_prop(self):\n",
        "\n",
        "    L = len(self.network)\n",
        "    print(L)\n",
        "\n",
        "    for i in range(L):\n",
        "      # print(i)\n",
        "      # print(\"oth layer input\" , self.input.shape)\n",
        "\n",
        "      self.network[i].a, self.network[i].h = Pre_Feedforward(self.input, self.network[i].weight, self.network[i].bias, self.activation_fn[i]).Preactivation_cal()\n",
        "      self.input = self.network[i].a\n",
        "      # print(\"oth layer w\" , a[i].weight.shape)\n",
        "      # print(\"oth layer b\" , a[i].bias.shape)\n",
        "      # print(\"oth layer h\" , a[i].h.shape)\n",
        "      # print(\"oth layer a\" , a[i].a.shape)\n",
        "\n",
        "\n",
        "    return self.network\n",
        "\n",
        "\n",
        "class Findloss:\n",
        "  def __init__(self,X_train, y_train, layer_dimension, activation_fn, method, loss_function):\n",
        "    self.X_train = X_train\n",
        "    self.y_train = y_train\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.loss_function = loss_function\n",
        "    self.method = method\n",
        "    self.y_pred = Feedforward(self.X_train, self.n, self.activation_fn, self.method).Forward_prop()[-1]\n",
        "\n",
        "  def loss(self):\n",
        "    return callloss(self.loss_function, self.y_train, self.y_pred).give_loss()"
      ],
      "metadata": {
        "id": "kVB_Sy02O6Uz"
      },
      "id": "kVB_Sy02O6Uz",
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = Feedforward(X_new,['sigmoid','relu','softmax'],\"Xavier_U\", a).Forward_prop()\n",
        "for i in range(3):\n",
        "  print(f\"{i}th layer w\" , network[i].weight.shape)\n",
        "  print(f\"{i}th layer b\" , network[i].bias.shape)\n",
        "  print(f\"{i}th layer h\" , network[i].h.shape)\n",
        "  print(f\"{i}th layer a\" , network[i].a.shape)\n",
        "  print(f\"{i}th layer g\" , network[i].g.shape)\n",
        "  print(f\"{i}th layer d_g\" , network[i].d_g.shape)\n",
        "  print(f\"{i}th layer d_a\" , network[i].d_a.shape)\n",
        "  print(f\"{i}th layer d_h\" , network[i].d_h.shape)\n",
        "  print(f\"{i}th layer d_w\" , network[i].d_w.shape)\n",
        "  print(f\"{i}th layer d_b\" , network[i].d_b.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtK_qppdC0m6",
        "outputId": "439909a3-54f5-4c8f-f100-513ad29283af"
      },
      "id": "VtK_qppdC0m6",
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "0th layer w (128, 784)\n",
            "0th layer b (128, 1)\n",
            "0th layer h (128, 60000)\n",
            "0th layer a (128, 60000)\n",
            "0th layer g (128, 60000)\n",
            "0th layer d_g (128, 60000)\n",
            "0th layer d_a (128, 60000)\n",
            "0th layer d_h (128, 60000)\n",
            "0th layer d_w (128, 784)\n",
            "0th layer d_b (128, 1)\n",
            "1th layer w (64, 128)\n",
            "1th layer b (64, 1)\n",
            "1th layer h (64, 60000)\n",
            "1th layer a (64, 60000)\n",
            "1th layer g (64, 60000)\n",
            "1th layer d_g (64, 60000)\n",
            "1th layer d_a (64, 60000)\n",
            "1th layer d_h (64, 60000)\n",
            "1th layer d_w (64, 128)\n",
            "1th layer d_b (64, 1)\n",
            "2th layer w (10, 64)\n",
            "2th layer b (10, 1)\n",
            "2th layer h (10, 60000)\n",
            "2th layer a (10, 60000)\n",
            "2th layer g (10, 60000)\n",
            "2th layer d_g (10, 60000)\n",
            "2th layer d_a (10, 60000)\n",
            "2th layer d_h (10, 60000)\n",
            "2th layer d_w (10, 64)\n",
            "2th layer d_b (10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = Weight_bias([784, 128, 64 ,10],['sigmoid','relu','softmax'], y_new, method= \"Xavier_U\").Init_network()\n",
        "print(\"oth layer w\" , a[0].weight.shape)\n",
        "print(\"oth layer b\" , a[0].bias.shape)\n",
        "print(\"oth layer h\" , a[0].h.shape)\n",
        "print(\"oth layer a\" , a[0].a.shape)\n",
        "print(\"1th layer w\" , a[1].weight.shape)\n",
        "print(\"1th layer b\" , a[1].bias.shape)\n",
        "print(\"1th layer h\" , a[1].h.shape)\n",
        "print(\"1th layer a\" , a[1].a.shape)\n",
        "print(\"2th layer w\" , a[2].weight.shape)\n",
        "print(\"2th layer b\" , a[2].bias.shape)\n",
        "print(\"2th layer h\" , a[2].h.shape)\n",
        "print(\"2th layer a\" , a[2].a.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_llvujQYBewv",
        "outputId": "e19f7d8e-5bd7-4c8c-aae4-9bed7fb20177"
      },
      "id": "_llvujQYBewv",
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-106-526b8f77b8ac>:12: RuntimeWarning: divide by zero encountered in divide\n",
            "  sig = np.where(self.m >= 0,1/(1 + np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) - 1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oth layer w (128, 784)\n",
            "oth layer b (128, 1)\n",
            "oth layer h (128, 60000)\n",
            "oth layer a (128, 60000)\n",
            "1th layer w (64, 128)\n",
            "1th layer b (64, 1)\n",
            "1th layer h (64, 60000)\n",
            "1th layer a (64, 60000)\n",
            "2th layer w (10, 64)\n",
            "2th layer b (10, 1)\n",
            "2th layer h (10, 60000)\n",
            "2th layer a (10, 60000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network = Feedforward(X_new,['sigmoid','relu','softmax'],\"Xavier_U\", a).Forward_prop()\n",
        "q = network[2].h\n",
        "print(q.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZUDWcZAAiTI",
        "outputId": "af1e264c-a000-44dd-b981-7386a7ce3246"
      },
      "id": "iZUDWcZAAiTI",
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "(10, 60000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z =callloss(loss_function=\"se\",y= y_new,y_pred= q).give_loss()\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiTnApEkZ-ZV",
        "outputId": "6c80eb9c-5470-4ec3-c1c1-49f957889cd8"
      },
      "id": "fiTnApEkZ-ZV",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09521316379404962"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        for param, grad in zip(params, grads):\n",
        "            param -= self.lr * grad\n",
        "\n",
        "class Momentum:\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.velocities = None\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.velocities is None:\n",
        "            self.velocities = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.velocities[i] = self.momentum * self.velocities[i] - self.lr * grad\n",
        "            param += self.velocities[i]\n",
        "\n",
        "\n",
        "class Adagrad:\n",
        "    def __init__(self, lr=0.01, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "        self.cache = None\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.cache is None:\n",
        "            self.cache = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.cache[i] += grad ** 2\n",
        "            param -= self.lr * grad / (np.sqrt(self.cache[i]) + self.epsilon)\n",
        "\n",
        "\n",
        "class RMSProp:\n",
        "    def __init__(self, lr=0.001, beta=0.9, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "        self.cache = None\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.cache is None:\n",
        "            self.cache = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.cache[i] = self.beta * self.cache[i] + (1 - self.beta) * grad ** 2\n",
        "            param -= self.lr * grad / (np.sqrt(self.cache[i]) + self.epsilon\n",
        "\n",
        "\n",
        "\n",
        "class Adam:\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        self.t = 0\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m = [np.zeros_like(param) for param in params]\n",
        "            self.v = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        self.t += 1\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n",
        "\n",
        "            # Bias correction\n",
        "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
        "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon\n",
        "\n",
        "\n",
        "class Nadam:\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        self.t = 0\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m = [np.zeros_like(param) for param in params]\n",
        "            self.v = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        self.t += 1\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            # Update biased moment estimates\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n",
        "\n",
        "            # Compute bias-corrected estimates with Nesterov acceleration\n",
        "            m_hat = (self.beta1 * self.m[i] / (1 - self.beta1 ** (self.t + 1)) +\n",
        "                    ((1 - self.beta1) * grad / (1 - self.beta1 ** self.t))\n",
        "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "ynXoTieZS-7O",
        "outputId": "a8531839-11dc-4430-ab84-e6a64ccb010f"
      },
      "id": "ynXoTieZS-7O",
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-118-229680b660aa>, line 56)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-118-229680b660aa>\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    class Adam:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Regularization Code\n",
        "class Regularisation:\n",
        "  def __init__(self, network, weight = 0):\n",
        "    self.network = network\n",
        "    self.weight = weight\n",
        "\n",
        "class L2_regularisation(Regularisation):\n",
        "\n",
        "  def Apply_L2(self):\n",
        "    # Function that returns L2 regularisation loss for the given network\n",
        "    L = len(self.network)\n",
        "    res = 0\n",
        "    for j in range(L-1):\n",
        "      n = np.mean(self.network[j].weight)\n",
        "      res += (1 / 2*n) * np.sum(self.network[j].weight ** 2)\n",
        "    return res\n",
        "\n",
        "  def Apply_L2_grad(self):\n",
        "  # Function that returns L2 regularisation gradient for the given Weight matrix / tensor\n",
        "    res_d = 2 * self.weight\n",
        "    return res_d\n",
        "\n",
        "\n",
        "class L1_regularisation(Regularisation):\n",
        "\n",
        "  def Apply_L1(self):\n",
        "    # Function that returns L1 regularisation loss for the given network\n",
        "    L = len(self.network)\n",
        "    res = 0\n",
        "    for j in range(L-1):\n",
        "      n = np.mean(self.network[j].weight)\n",
        "      res += (1 / 2*n) * np.sum(np.absolute(self.network[j].weight))\n",
        "    return res\n",
        "\n",
        "  def Apply_L1_grad(self):\n",
        "    # Function that returns L1 regularisation gradient for the given Weight matrix / tensor\n",
        "    res_d = np.sign(self.weight)\n",
        "    return res_d\n",
        "\n",
        "\n",
        "class ApplyReg(Regularisation):\n",
        "\n",
        "    def __init__(self,reg_function, network, weight = 0):\n",
        "        self.reg_function= reg_function\n",
        "        super().__init__(network, weight)\n",
        "\n",
        "    def do_reg(self):\n",
        "        if self.reg_function == 'L2':\n",
        "            return L2_regularisation(self.network).Apply_L2()\n",
        "        if self.reg_function == 'L1':\n",
        "            return L1_regularisation(self.network).Apply_L1()\n",
        "        if self.reg_function == 'L2_d':\n",
        "            return L2_regularisation(self.network, self.weight).Apply_L2_grad()\n",
        "        if self.reg_function == 'L1_d':\n",
        "            return L1_regularisation(self.network, self.weight).Apply_L1_grad()"
      ],
      "metadata": {
        "id": "H0NBqkSyhNWW"
      },
      "id": "H0NBqkSyhNWW",
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ApplyReg(\"L2\", a).do_reg()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r_OCAEvS04f",
        "outputId": "26f17829-fc85-4a20-e138-112d0f255e18"
      },
      "id": "_r_OCAEvS04f",
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.07999452224272594"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ApplyReg(\"L2_d\",a, a[0].weight).do_reg()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmvClNIqTNAe",
        "outputId": "b71c0fc4-8865-4888-86af-53b8c5567596"
      },
      "id": "cmvClNIqTNAe",
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.01583719,  0.06981665,  0.03334084, ..., -0.02371536,\n",
              "         0.11123681,  0.10318363],\n",
              "       [-0.12899401, -0.11148396, -0.06352633, ..., -0.05187817,\n",
              "        -0.13996567, -0.08795399],\n",
              "       [-0.0460761 , -0.02104272,  0.02950053, ..., -0.16192836,\n",
              "        -0.13957067, -0.08873795],\n",
              "       ...,\n",
              "       [ 0.10358064, -0.06248159,  0.15741003, ...,  0.09645185,\n",
              "         0.07039571, -0.01073643],\n",
              "       [ 0.14485504, -0.04998867, -0.07543037, ..., -0.0118866 ,\n",
              "        -0.03983624,  0.09731049],\n",
              "       [ 0.07494576, -0.06181764,  0.0576291 , ..., -0.1015957 ,\n",
              "        -0.07547197, -0.02643779]])"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, learning_rate=0.01, decay=0.0):\n",
        "        self.eta = learning_rate  # Learning rate\n",
        "        self.decay = decay        # Learning rate decay\n",
        "        self.iterations = 0       # Track steps for decay\n",
        "\n",
        "    def update(self, weights, gradients):\n",
        "        \"\"\"Update weights using gradients.\"\"\"\n",
        "        weights -= self.eta * gradients  # Core SGD update rule\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Decay learning rate after each epoch (optional).\"\"\"\n",
        "        if self.decay > 0:\n",
        "            self.eta = self.eta / (1 + self.decay * self.iterations)\n",
        "            self.iterations += 1"
      ],
      "metadata": {
        "id": "mQDo0hSwfYx4"
      },
      "id": "mQDo0hSwfYx4",
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CalculateAllLoss:\n",
        "  def __init__(self, X_train, y_predicted,network, y_true, primary_loss, weight_decay=0, regularisation_fn=None):\n",
        "    self.y_predicted = y_predicted\n",
        "    self.y_true = y_true\n",
        "    self.network = network\n",
        "    self.X_train = X_train\n",
        "    self.loss_value = primary_loss\n",
        "    self.weight_decay = weight_decay\n",
        "    self.regularisation_fn= regularisation_fn\n",
        "    self.calc_accuracy_loss()\n",
        "\n",
        "\n",
        "  #def overall_loss(self):\n",
        "    \"\"\"\n",
        "    Calculates the total loss of the network.\n",
        "\n",
        "    This includes:\n",
        "    - The primary loss (e.g., Cross-Entropy Loss)\n",
        "    - Optional regularization (like L2 regularization)\n",
        "\n",
        "    Parameters:\n",
        "    - network: The neural network model.\n",
        "    - Y_pred: The predicted output from the network.\n",
        "    - Y_true: The actual labels (ground truth).\n",
        "    - loss_fn: The loss function to be used (e.g., CrossEntropy_loss).\n",
        "    - weight_decay: A coefficient for regularization (default is 0, meaning no regularization).\n",
        "    - regularisation_fn: A function for computing the regularization term (optional).\n",
        "\n",
        "    Returns:\n",
        "    - Total loss value.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "  def calc_accuracy_loss(self):\n",
        "    \"\"\"\n",
        "    Computes the accuracy and loss for a given neural network.\n",
        "\n",
        "    Parameters:\n",
        "    - network: The neural network model.\n",
        "    - X: Input data (features for prediction).\n",
        "    - Y: Actual labels (ground truth values).\n",
        "    - loss_fn: The loss function to be used.\n",
        "    - weight_decay: Regularization strength (default is 0, meaning no regularization).\n",
        "    - regularisation_fn: A function to compute the regularization term (optional).\n",
        "\n",
        "    Returns:\n",
        "    - accuracy: The percentage of correctly classified examples.\n",
        "    - loss: The computed total loss.\n",
        "    \"\"\"\n",
        "    # Get network predictions self.y_prediction from model\n",
        "    total_loss = self.loss_value  # Compute primary loss\n",
        "\n",
        "    if self.weight_decay > 0 and self.regularisation_fn:\n",
        "        regularized_val = ApplyReg(self.regularisation_fn, self.network).do_reg()\n",
        "        print(f\"Reg value: {regularized_val}\")\n",
        "        total_loss += self.weight_decay * regularized_val # Add regularization term if applicable  # Compute total loss\n",
        "\n",
        "\n",
        "    # Ensure dimensions match between input and labels\n",
        "    assert self.X_train.shape[1] == self.y_true.shape[1], \"Mismatch in batch size between inputs and labels\"\n",
        "\n",
        "    # Compute accuracy by comparing predicted vs actual labels\n",
        "    batch_size = self.X_train.shape[1]  # Number of examples\n",
        "    correct_predictions = np.sum(np.argmax(self.y_predicted, axis=0) == np.argmax(self.y_true, axis=0))\n",
        "    accuracy = correct_predictions / batch_size  # Compute accuracy as a fraction\n",
        "\n",
        "    return accuracy, total_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "6B3z_WoEeGA5"
      },
      "id": "6B3z_WoEeGA5",
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k, l = CalculateAllLoss(X_train=X_new,y_predicted=q,network=a,y_true=y_new,primary_loss=z, weight_decay=0.1, regularisation_fn=\"L2\").calc_accuracy_loss()\n",
        "k, l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHlffnq-Y6Ym",
        "outputId": "2552ad54-9c32-472f-e23c-d5585252f2d0"
      },
      "id": "dHlffnq-Y6Ym",
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reg value: -0.07999452224272594\n",
            "Reg value: -0.07999452224272594\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.11486666666666667, 0.08721371156977703)"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3fZCFuv9YWra"
      },
      "id": "3fZCFuv9YWra",
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Optimizer:\n",
        "  def __init__(self, network, eta = 0.01, beta = 0.9):\n",
        "    self.network = network\n",
        "    self.eta = eta\n",
        "    self.beta = beta\n",
        "\n",
        "\n",
        "class SGD(Optimizer):\n",
        "\n",
        "  def Apply_sgd(self):\n",
        "    L = len(self.network)\n",
        "    for k in range(L):\n",
        "      self.network[k].weight -= self.eta * self.network[k].d_w\n",
        "      self.network[k].bias -= self.eta * self.network[k].d_b\n",
        "\n",
        "    return self.network\n",
        "\n",
        "class MomentumGD:\n",
        "  def __init__(self, gradient, eta = 0.01 , beta = 0.9):\n",
        "    self.grad = gradient\n",
        "    self.eta = eta\n",
        "    self.beta = beta\n",
        "    self.new_updates = 0\n",
        "    self.Apply_mgd()\n",
        "\n",
        "  def Apply_mgd(self):\n",
        "    self.new_updates = self.beta * self.new_updates + self.eta * self.grad\n",
        "    return self.new_updates\n",
        "\n",
        "class Nag:\n",
        "  def __init__(self, gradient, eta = 0.01, beta = 0.9 ):\n",
        "    self.new_updates = 0\n",
        "    self.gradient = gradient\n",
        "    self.eta = eta\n",
        "    self.beta = beta\n",
        "    self.Apply_nag()\n",
        "\n",
        "  def Apply_nag(self):\n",
        "    L = len(self.gradient_w_L)\n",
        "\n",
        "    for j in range (L -1):\n",
        "      self.look = self.beta * self.new_updates + (self.gradient[j] - self.beta * self.new_updates)\n",
        "      self.new_updates = self.eta * self.look\n",
        "      return self.new_updates\n",
        "\n",
        "class CallOptimizers(Optimizer):\n",
        "\n",
        "  def __int__(self, network, optimization_fn, eta = 0.01, beta = 0.9):\n",
        "    self.optimization_function = optimization_fn\n",
        "    super().__init__(network, eta, beta)\n",
        "\n",
        "  def apply_optimization(self):\n",
        "    if self.optimization_function == 'sgd':\n",
        "        return SGD(self.network, self.eta).Apply_sgd()\n",
        "    elif self.optimization_function == 'Momentum':\n",
        "        return MomentumGD(self.gradient_w, self.grad_b, self.eta, self.beta).Apply_mgd()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZE3u3I_l6nGy"
      },
      "id": "ZE3u3I_l6nGy",
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_norm= Normalize(X_train).Norm_reshape()\n",
        "y_norm =OneHotEncoder(X_train, y_train).onehot_encode()"
      ],
      "metadata": {
        "id": "2uMpqFSKffHb"
      },
      "id": "2uMpqFSKffHb",
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "class Backpropagation:\n",
        "\n",
        "    def __init__(self, loss_function, X_train, y_train, activation_fn, layers_dimensions ,method, batch_size, optimizer_fn, epochs,weight_decay,regularization_fn,grad_reglr_fn ,X_val=None, t_val=None, use_wandb=False, optim_params=None):\n",
        "        self.loss_function = loss_function\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.activation_fn = activation_fn\n",
        "        self.method = method\n",
        "        self.n = layers_dimensions\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.optimizer_fn = optimizer_fn\n",
        "        self.weight_decay = weight_decay\n",
        "        self.regularization_fn = regularization_fn\n",
        "        self.grad_reglr_fn = grad_reglr_fn\n",
        "        self.batches_number = self.X_train.shape[1]//self.batch_size\n",
        "        z = Weight_bias(self.n, self.activation_fn, self.y_train, self.method)\n",
        "        self.init_network = z.Init_network()\n",
        "        self.network = Feedforward(self.X_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "\n",
        "        # print(\"X_norm shape \",self.X_norm.shape)\n",
        "        # print(\"y_norm shape \",self.y_norm.shape)\n",
        "        # print(\"H shape \",self.H.shape)\n",
        "        # print(\"length self.network \",len(self.network))\n",
        "        # print(\"self.batch_no \",self.batches_number)\n",
        "        # print(\"self.batch_size \",self.batch_size)\n",
        "\n",
        "        #self.loss = map_losses[loss]\n",
        "        # self.use_wandb = use_wandb\n",
        "        # if t_val is not None:\n",
        "        #     self.X_val = X_val\n",
        "        #     self.layers[0].a_val = X_val\n",
        "        #     self.t_val = t_val\n",
        "        #self.param_init(optimizer, optim_params)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def backward_propagation(self):\n",
        "        L = len(self.network)\n",
        "        self.y_predicted = self.network[L-1].h\n",
        "        # Initialize variables neesed to keep track of loss\n",
        "        self.eta_hist = []\n",
        "        self.loss_history = []\n",
        "        self.accuracy_history = []\n",
        "        self.loss_hist_val = []\n",
        "        self.accuracy_hist_val = []\n",
        "\n",
        "        # Perform Backprop\n",
        "        for epochs in tqdm(range(self.epochs)):\n",
        "            print(f\"No of epoch rotation {epochs}\")\n",
        "\n",
        "\n",
        "            self.loss = callloss(self.loss_function, self.y_train, self.y_predicted).give_loss()\n",
        "            print(\"loss : \" , self.loss)\n",
        "            accuracy, overall_loss = CalculateAllLoss(self.X_train, self.y_predicted, self.network, self.y_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "            self.accuracy = accuracy\n",
        "            self.overall_loss = overall_loss\n",
        "            self.loss_history.append(self.overall_loss)\n",
        "            self.accuracy_history.append(self.accuracy)\n",
        "            print(\"Overall_loss_history : \" , self.loss_history)\n",
        "            print(\"accuracy_history : \" , self.accuracy_history)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            for batch in range(self.batches_number):\n",
        "                # print(f\"No of batch rotation {batch}\")\n",
        "\n",
        "                X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "                self.X_batch = X_batch\n",
        "                print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "                # print(\"y_predicted shape : \" , self.y_predicted.shape)\n",
        "\n",
        "                y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "                # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "                self.y_true_batch = y_true_batch\n",
        "                self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "                assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "                # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "                # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "\n",
        "\n",
        "\n",
        "                self.network[-1].d_a = callloss(self.loss_function,self.y_true_batch, self.y_pred_batch).give_gradloss()\n",
        "                print(\"network -[-1].d_a : \" , self.network[-1].d_a.shape)\n",
        "                # A_k = apply_activation(self.activation_fn[-1], self.network[-1].h[:, batch*self.batch_size : (batch+1)*self.batch_size]).do_activation_derivative()\n",
        "                # print(\"shape A_ k : \" , A_k.shape)\n",
        "                # self.network[-1].d_h = self.network[-1].d_a * A_k\n",
        "                # print(\"network -[-1].d_h : \" , self.network[-1].d_a.shape)\n",
        "\n",
        "                # self.network[-1].d_w = self.network[-1].d_h @ self.network[-2].a[:, batch*self.batch_size : (batch+1)*self.batch_size].T\n",
        "                # print(\"network -[-1].d_w : \" , self.network[-1].d_w.shape)\n",
        "                # d_b = -np.sum(self.network[-1].d_h, axis = 1)\n",
        "                # self.network[-1].d_b = d_b.reshape(-1 , 1)\n",
        "                # print(\"network -[-1].d_b : \" , self.network[-1].d_b.shape)\n",
        "\n",
        "\n",
        "                # op_prev_w =  CallOptimizers(optimization_function=self.optimizer_fn, gradient = self.network[-1].d_w, eta = 0.001, beta= 0.9)\n",
        "                # op_prev_b = CallOptimizers(optimization_function=self.optimizer_fn, gradient = self.network[-1].d_b, eta = 0.001, beta= 0.9)\n",
        "                # self.network[-1].Weight_updates =op_prev_w.apply_optimization()\n",
        "                # print(\"len network -[-1].weight_updates : \" , len(self.network[-1].Weight_updates), self.network[-1].Weight_updates.size )\n",
        "                # self.network[-1].bias_updates = op_prev_b.apply_optimization()\n",
        "                # print(\"len network -[-1].bias_updates : \" , len(self.network[-1].bias_updates), self.network[-1].bias_updates.size)\n",
        "\n",
        "                for k in range(L-1,0,-1):\n",
        "\n",
        "                  self.network[k].d_w = np.dot(self.network[k].d_a, self.network[k-1].h[:, batch*self.batch_size : (batch+1)*self.batch_size].T)\n",
        "                  print(f\"shape self.network-{k}.d_w : \" , self.network[k].d_w.shape)\n",
        "                  self.network[k].d_b =  np.sum(self.network[k].d_a, axis=1, keepdims = True)\n",
        "                  print(f\"shape self.network-{k}.d_b : \" , self.network[k].d_b.shape)\n",
        "\n",
        "\n",
        "                  self.network[k-1].d_h = np.matmul(self.network[k].weight.T, self.network[k].d_a)\n",
        "                  print(f\"shape self.network-{k-1}.d_h : \" , self.network[k-1].d_h.shape)\n",
        "                  act_derv =  apply_activation(self.activation_fn[k-1], self.network[k-1].a[:, batch*self.batch_size : (batch+1)*self.batch_size])\n",
        "                  self.network[k-1].d_a = self.network[k - 1].d_h * act_derv.do_activation_derivative()\n",
        "                  print(f\"shape self.network-{k-1}.d_a : \" , self.network[k-1].d_a.shape)\n",
        "                assert(k-1 == 0)\n",
        "\n",
        "                self.network[0].d_w = np.dot(self.network[0].d_a , self.X_batch.T)\n",
        "                print(\"network -[0].d_w : \" , self.network[0].d_w.shape)\n",
        "                self.network[0].d_b = np.sum(self.network[0].d_a, axis=1, keepdims = True)\n",
        "                print(\"network -[0].d_b : \" , self.network[0].d_b.shape)\n",
        "\n",
        "\n",
        "                # for k in range(L-2,0,-1):\n",
        "                #     # print(f\"No of layers rotation {k}\")\n",
        "                #     print(f\"shape self.network-{k+1}.d_h : \" , self.network[k+1].d_h.shape)\n",
        "                #     print(f\"shape self.network-{k+1}.weight : \" , self.network[k+1].weight.shape)\n",
        "                #     self.network[k].d_a = self.network[k + 1].weight.T @ self.network[k + 1].d_h\n",
        "                #     print(f\"shape self.network-{k}.d_a : \" , self.network[k].d_a.shape)\n",
        "                #     act_derv =  apply_activation(self.activation_fn[k], self.network[k].h[:, batch*self.batch_size : (batch+1)*self.batch_size])\n",
        "                #     self.network[k].d_h = self.network[k].d_a * act_derv.do_activation_derivative()\n",
        "                #     print(f\"shape self.network-{k}.d_h : \" , self.network[k].d_h.shape)\n",
        "\n",
        "                #     self.network[k].d_w = self.network[k].d_h @ self.network[k-1].a[:, batch*self.batch_size : (batch+1)*self.batch_size].T\n",
        "                #     print(f\"shape self.network-{k}.d_w : \" , self.network[k].d_w.shape)\n",
        "                #     derv_bias = -np.sum(self.network[k].d_h, axis=1)\n",
        "                #     self.network[k].d_b = derv_bias.reshape(-1 , 1)\n",
        "                #     print(f\"shape self.network-{k}.d_b : \" , self.network[k].d_b.shape)\n",
        "\n",
        "\n",
        "                    # op_w = CallOptimizers(optimization_function=self.optimizer_fn, gradient = self.network[k].d_w, eta = 0.001, beta= 0.9)\n",
        "                    # op_b = CallOptimizers(optimization_function=self.optimizer_fn, gradient = self.network[k].d_b, eta = 0.001, beta= 0.9)\n",
        "                    # self.network[k].Weight_updates =  op_w.apply_optimization()\n",
        "                    # print(\"len network -[-1].weight_updates : \" , len(self.network[k].Weight_updates), self.network[k].Weight_updates.size )\n",
        "                    # print(f\"len network -{k}.weight_updates : \" , len(self.network[k].Weight_updates))\n",
        "                    # self.network[k].bias_updates = op_b.apply_optimization()\n",
        "                    # print(\"len network -[-1].bias_updates : \" , len(self.network[k].bias_updates), self.network[k].bias_updates.size)\n",
        "                    # print(f\"len network -{k}.bias_updates : \" , len(self.network[k].bias_updates))\n",
        "\n",
        "\n",
        "\n",
        "                    # w_updates = CallOptimizers(optimization_function=self.optimizer_fn, gradient = grad_w_L, eta = 0.001, beta= 0.9).apply_optimization()\n",
        "                    # b_updates = CallOptimizers(optimization_function=self.optimizer_fn, gradient = grad_b_L, eta = 0.001, beta= 0.9).apply_optimization()\n",
        "                    # print(\"shape w_updates : \" ,len(w_updates))\n",
        "                    # print(\"shape b_updates : \" ,len(b_updates))\n",
        "                for i in range(L):\n",
        "                  print(f\"{i}th layer w\" , self.network[i].weight.shape)\n",
        "                  print(f\"{i}th layer b\" , self.network[i].bias.shape)\n",
        "                  print(f\"{i}th layer h\" , self.network[i].h.shape)\n",
        "                  print(f\"{i}th layer a\" , self.network[i].a.shape)\n",
        "                  print(f\"{i}th layer g\" , self.network[i].g.shape)\n",
        "                  print(f\"{i}th layer d_g\" , self.network[i].d_g.shape)\n",
        "                  print(f\"{i}th layer d_a\" , self.network[i].d_a.shape)\n",
        "                  print(f\"{i}th layer d_h\" , self.network[i].d_h.shape)\n",
        "                  print(f\"{i}th layer d_w\" , self.network[i].d_w.shape)\n",
        "                  print(f\"{i}th layer d_b\" , self.network[i].d_b.shape)\n",
        "                    # for i in range(L):\n",
        "                    #     print(f\"{i} th layer w\" , self.network[i].weight.shape)\n",
        "                    #     print(f\"{i} th layer b\" , self.network[i].bias.shape)\n",
        "                    #     print(f\"{i} th layer w_update\" , self.network[i].Weight_updates.shape)\n",
        "                    #     print(f\"{i} th layer b_update\" , self.network[i].bias_updates.shape)\n",
        "                    # for k in range(L):\n",
        "                    #   self.network[k].weight -= self.network[k].Weight_updates\n",
        "                    #   self.network[k].bias -= self.network[k].bias_updates\n",
        "                self.network_updated = CallOptimizers(self.network, optimization_fn = self.optimizer_fn ).apply_optimization()\n",
        "\n",
        "\n",
        "                # self.network = Feedforward(self.X_train,self.activation_fn,self.method, self.network_updated, self.batch_size).Forward_prop()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jHfISCRCqJDP"
      },
      "id": "jHfISCRCqJDP",
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Backpropagation(loss_function=\"se\",X_train= X_new, y_train= y_new,layers_dimensions= [784, 128, 64, 10],activation_fn=['relu', 'relu', 'softmax'], batch_size=1000,optimizer_fn=\"sgd\", epochs= 5,weight_decay=0.01,grad_reglr_fn= \"L1_d\", regularization_fn=\"L1\", method= \"Xavier_N\" ).backward_propagation()"
      ],
      "metadata": {
        "id": "_WFk5jtH19Ti",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d971bf71-331f-4428-cde6-d32bde093b15"
      },
      "id": "_WFk5jtH19Ti",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of epoch rotation 0\n",
            "loss :  0.09187630367383108\n",
            "Reg value: -0.299299735760645\n",
            "Reg value: -0.299299735760645\n",
            "Overall_loss_history :  [0.08888330631622463]\n",
            "accuracy_history :  [0.07641666666666666]\n",
            "X_batch shape :  (784, 1000)\n",
            "network -[-1].d_a :  (10, 1000)\n",
            "shape self.network-2.d_w :  (10, 64)\n",
            "shape self.network-2.d_b :  (10, 1)\n",
            "shape self.network-1.d_h :  (64, 1000)\n",
            "shape self.network-1.d_a :  (64, 1000)\n",
            "shape self.network-1.d_w :  (64, 128)\n",
            "shape self.network-1.d_b :  (64, 1)\n",
            "shape self.network-0.d_h :  (128, 1000)\n",
            "shape self.network-0.d_a :  (128, 1000)\n",
            "network -[0].d_w :  (128, 784)\n",
            "network -[0].d_b :  (128, 1)\n",
            "0th layer w (128, 784)\n",
            "0th layer b (128, 1)\n",
            "0th layer h (128, 60000)\n",
            "0th layer a (128, 60000)\n",
            "0th layer g (128, 60000)\n",
            "0th layer d_g (128, 60000)\n",
            "0th layer d_a (128, 1000)\n",
            "0th layer d_h (128, 1000)\n",
            "0th layer d_w (128, 784)\n",
            "0th layer d_b (128, 1)\n",
            "1th layer w (64, 128)\n",
            "1th layer b (64, 1)\n",
            "1th layer h (64, 60000)\n",
            "1th layer a (64, 60000)\n",
            "1th layer g (64, 60000)\n",
            "1th layer d_g (64, 60000)\n",
            "1th layer d_a (64, 1000)\n",
            "1th layer d_h (64, 1000)\n",
            "1th layer d_w (64, 128)\n",
            "1th layer d_b (64, 1)\n",
            "2th layer w (10, 64)\n",
            "2th layer b (10, 1)\n",
            "2th layer h (10, 60000)\n",
            "2th layer a (10, 60000)\n",
            "2th layer g (10, 60000)\n",
            "2th layer d_g (10, 60000)\n",
            "2th layer d_a (10, 1000)\n",
            "2th layer d_h (10, 60000)\n",
            "2th layer d_w (10, 64)\n",
            "2th layer d_b (10, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Optimizer.__init__() got an unexpected keyword argument 'optimization_fn'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-22d9cd68f029>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"se\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0my_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers_dimensions\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'softmax'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sgd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_reglr_fn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"L1_d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"Xavier_N\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-127-1aa7fe2a69ae>\u001b[0m in \u001b[0;36mbackward_propagation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m                     \u001b[0;31m#   self.network[k].weight -= self.network[k].Weight_updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0;31m#   self.network[k].bias -= self.network[k].bias_updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCallOptimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimization_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_fn\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Optimizer.__init__() got an unexpected keyword argument 'optimization_fn'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " a,b,c,d = Feedforward(X_new,[784, 128, 64, 10],['sigmoid','relu','softmax'],\"Xavier_U\").Forward_prop()"
      ],
      "metadata": {
        "id": "kGOmz4yXVtvE"
      },
      "id": "kGOmz4yXVtvE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c,d = Feedforward(X,[2,3,2,1],['sigmoid','relu','softmax'],\"Xavier_U\").Forward_prop()\n",
        "a,b,c,d"
      ],
      "metadata": {
        "id": "92hGBoNHLYP3"
      },
      "id": "92hGBoNHLYP3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_new.size"
      ],
      "metadata": {
        "id": "yx1WfEF9-dbd"
      },
      "id": "yx1WfEF9-dbd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = callloss(\"se\", y_new, d).give_loss()\n",
        "print(a)"
      ],
      "metadata": {
        "id": "QfbsEUn_-SnV"
      },
      "id": "QfbsEUn_-SnV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Backpropagation(loss_function=\"se\",X_train= X_train, y_train= y_train,layers_dimensions= [784, 128, 64 ,10],activation_fn=['sigmoid', 'sigmoid', 'sigmoid'], batch_size=10000,optimizer_fn=\"Adam\", epochs= 5,weight_decay=0,grad_reglr_fn=None, method= \"Xavier_U\" ).backward_propogation()"
      ],
      "metadata": {
        "id": "wss0tLWsslKV"
      },
      "id": "wss0tLWsslKV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MhJrZkPpp7Zv"
      },
      "id": "MhJrZkPpp7Zv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TMAZN5qaVr3t"
      },
      "id": "TMAZN5qaVr3t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Backprop(loss_function=\"se\",X_train= X_train, y_train= y_train,layers_dimensions= [784, 128, 64 ,10],activation_fn=['sigmoid', 'sigmoid', 'softmax'], batch_size=10000,optimizer_fn=\"Adam\", epochs= 5, method= \"Xavier_U\" ).backward_propogation()"
      ],
      "metadata": {
        "id": "CyW79eripCll"
      },
      "id": "CyW79eripCll",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new= Normalize(X_train).Norm_reshape()\n",
        "y_new =OneHotEncoder(X_train,y_train).onehot_encode()"
      ],
      "metadata": {
        "id": "lhnh7_96ftQs"
      },
      "id": "lhnh7_96ftQs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new.shape"
      ],
      "metadata": {
        "id": "gFlE6dbNAJE7"
      },
      "id": "gFlE6dbNAJE7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c,d = Feedforward(X,[2,3,3,1],['relu','relu','softmax'],\"Xavier_U\").Forward_prop()\n",
        "a,b,c,d"
      ],
      "metadata": {
        "id": "NV7gYASA0m8t"
      },
      "id": "NV7gYASA0m8t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(b[\"a\" + str(4-1)])"
      ],
      "metadata": {
        "id": "Xmxy4rN2cTM-"
      },
      "id": "Xmxy4rN2cTM-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(apply_activation_derivative([\"sigmoid\"], b[\"a\" + str(4-1)]).do_activation_derivative()))"
      ],
      "metadata": {
        "id": "X06X6gfoV1Ol"
      },
      "id": "X06X6gfoV1Ol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2064d83c-b27e-46e6-bef2-c24d9faf70ff",
      "metadata": {
        "id": "2064d83c-b27e-46e6-bef2-c24d9faf70ff"
      },
      "outputs": [],
      "source": [
        "Initilize(3,5,\"Xavier_N\").Init_weight(\"Xavier_N\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [relu,relu,sigmoid]\n",
        "print(x[1])"
      ],
      "metadata": {
        "id": "AIOXHXcuRF9-"
      },
      "id": "AIOXHXcuRF9-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weight_bias([4,5,3,1],['relu','relu','sigmoid'],\"Xavier_N\").Init_network([4,5,3,1])"
      ],
      "metadata": {
        "id": "4NKWIZNDdN7u"
      },
      "id": "4NKWIZNDdN7u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callloss(\"se\",y_new,d).give_loss()"
      ],
      "metadata": {
        "id": "BOwYYfPZCFzT"
      },
      "id": "BOwYYfPZCFzT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CmG8ToWcnOR7"
      },
      "id": "CmG8ToWcnOR7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}