{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "wOf-9eHQIIN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Question 1 (2 Marks)\n",
        "## Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use from keras.datasets import fashion_mnist for getting the fashion mnist dataset."
      ],
      "metadata": {
        "id": "KQwDjvooANHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "hgqEXnh5H_NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist"
      ],
      "metadata": {
        "id": "ybFbuM6D7N7q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Class to load and preprocess the Fashion-MNIST dataset.\"\"\"\n",
        "class FashionMNISTLoader:\n",
        "\n",
        "    def __init__(self):\n",
        "        (self.x_train, self.y_train), (_, _) = fashion_mnist.load_data()\n",
        "        self.class_labels = [\n",
        "            'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
        "        ]\n",
        "\n",
        "    def get_sample_images(self):\n",
        "        sample_images = []\n",
        "        for label in range(10):\n",
        "            idx = np.where(self.y_train == label)[0][0]\n",
        "            sample_images.append((self.x_train[idx], self.class_labels[label]))\n",
        "        return sample_images\n",
        "\n",
        "class ImagePlotter:\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_images(image_data):\n",
        "        fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "        fig.suptitle(\"Fashion-MNIST Class Samples\", fontsize=14, fontweight='bold')\n",
        "\n",
        "        for ax, (image, label) in zip(axes.flat, image_data):\n",
        "            ax.imshow(image, cmap='gray')\n",
        "            ax.set_title(label, fontsize=10)\n",
        "            ax.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "class FashionMNISTVisualizer:\n",
        "    def __init__(self):\n",
        "        self.data_loader = FashionMNISTLoader()\n",
        "        self.plotter = ImagePlotter()\n",
        "\n",
        "    def visualize_samples(self):\n",
        "        samples = self.data_loader.get_sample_images()\n",
        "        self.plotter.plot_images(samples)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    visualizer = FashionMNISTVisualizer()\n",
        "    visualizer.visualize_samples()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "id": "AjpkIvO_7RNo",
        "outputId": "706f5657-c4c9-4ce0-d9af-60f60a0ba7e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 10 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA94AAAHbCAYAAAA9P0KxAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAgetJREFUeJzt3Xd4VVX2//FPaElIQugQIIChF0GlqAFFRMRCURQbKo5tRETsZYaxjn2sozKjo2AbdVSwgwUBlY70IiBdeichIQQ4vz/8cb8E9trkXnIg6Pv1PDwPWeeuc849d5+yuWGtuCAIAgEAAAAAgFCUONI7AAAAAADA7xkTbwAAAAAAQsTEGwAAAACAEDHxBgAAAAAgREy8AQAAAAAIERNvAAAAAABCxMQbAAAAAIAQMfEGAAAAACBETLwBAAAAAAgRE28AOMpdddVViouLU1xcnE477bRC55122mmRvKuuuiq0/cPR44EHHoiMibp16x7p3UEUOJ8BoHhj4g0AURg9enTk4db3hwfforPvPyzs/TN06FDnay+99NIDXjt69OjI8v0/v5IlS2rWrFkF1pGdnV3gNQ888ECB5Qf7nOfNm6frrrtODRo0UGJiohISElSzZk0df/zxuuKKK/T0008rPz9fUsGJbmH/RDO2giDQl19+qSuuuEINGzZUuXLlVLp0aVWrVk2dOnXSE088odWrVxd6fUeTaD4HAADCVupI7wAA4Mjo27evunbtKklq3rz5Ed6b6Lzwwgvq2bNngdiqVav04YcfRrWePXv2aODAgfrkk0+KZL+GDx+u8847Tzt37jxg31atWqXp06fr7bff1jXXXKPy5csXyTYtK1as0GWXXaYff/zxgGXr1q3Td999p++++07z5s3TkCFDQt2Xw604fQ4AAEhMvAHgkFx88cVq3br1AfGjYSJ78cUXH+ldiNmYMWM0c+ZMtWjRIhJ7+eWXtWvXrqjX9emnn2rixIk68cQTD2mfdu/erWuvvTYy2atUqZIuuugipaenKycnRz///LO+//57rVu3LpJz5plnKjk5ucB6Bg0apMWLF0uSKlSooL/85S8FlhdmbK1du1YdOnTQkiVLIrFjjjlG3bt3V7Vq1bR582ZNmDDBOSk/2sXyOQAAELoAAFBoo0aNCiRF/gwePNj7+vz8/GDgwIHB2WefHWRkZASpqalBqVKlgooVKwbt27cPXnjhhWDnzp0H5H3//ffBeeedF9SoUSMoXbp0kJSUFNSpUyc466yzgvvvvz/YsmVL5LV9+vSJ7E+HDh2C9evXB3379g3S0tKCMmXKBI0bNw5eeeWVA7bRoUOHSF6fPn0OWD5//vzghhtuCBo2bBgkJiYGiYmJQYMGDYLrr78+mDdv3gGv338/Vq1aFVx33XVB9erVvftxMPuut0SJEpG/X3PNNZHX7NixI6hSpUogKShZsmSBz2jUqFGR1+3/+e39c/rpp0dek5WVVWDZ/fffX2B/9l2273GbMWNGgWWjR48+4L3s2bMnGDlyZLBjxw7z/e77udSpUyfq4xUEQXDJJZcU2Je+ffsG+fn5B7xuwYIFwdtvvx35+f777ze3PXTo0ODyyy8Pjj322KBq1aqRcdmkSZOgX79+wZIlSw5Y//r164Pbb789aNq0aVC2bNmgdOnSQbVq1YI2bdoE/fr1C8aPH1/g9Z988knQpUuXoGrVqkGpUqWClJSUICMjI+jRo0fw6KOPBrt37z7oez+Uz+G1114LevXqFTRu3DioVKlSZB9atmwZ3HXXXcH69esPWFedOnUKjJUvv/wyOOmkk4LExMSgZs2awV//+tfIOf7SSy8FjRs3DuLj44NjjjkmeOSRR4I9e/YUWN/+59HKlSuDPn36BFWrVg3i4+OD448/Pnj33XcP2I+Dnc+LFi0K+vfvHzRu3DgoW7ZskJCQEDRp0iS4++67ne8r2s8OAODHxBsAohDtxHv/SZzrzxlnnBHs2rUrkvPtt98eMHnc/8++E999H9QbNWoU1K1b15nz2muvFdg334P6//73vyAhIcHcfnx8/AEP//vuR0ZGRpCWllao/TiYfddbqVKloH379oGkIDExMdiwYUMQBEHw+uuvR15z/vnnF3riXb169cjfv/nmG+dnVtiJ908//VRg2fPPPx/V+9zrUCfeq1atCuLi4iLrOO644wo1YQ0C/8T7ggsu8I7JcuXKBTNnzoy8Pjc3N2jUqJE35+677468fvDgwQc9V3Jzcw/6Hg7lc2jVqpV3+zVr1gxWrlxZIGffiffxxx9f4NjvO0769+/vXOff/va3Auvbd7w3bNgwqFmzpjPv6aefLpDnO58//vjjoGzZst73NXfu3Mjro/3sAAAHx6+aA8AhGDFihDZs2HBA/OKLL1Z6erri4uKUkZGhk046STVr1lSFChWUn5+vn3/+WR988IF27dqlb7/9Vh999JEuuugiSdIrr7yi3bt3S5IaN26sXr16qVSpUlq+fLmmT5+uqVOnmvszf/58JSQkqG/fvkpMTNSgQYOUm5srSXryySd19dVXH/Q9/fLLL7riiiuUl5cn6bdf1e3Tp4/i4uL0xhtvaMOGDcrLy1OfPn3UqlUrNWjQ4IB1LF68+JD3wzJgwAD9+OOPys3N1auvvqp77rlHL7zwgiQpJSVFf/rTnzRs2LBCrevuu+/WnXfeqV27dukvf/mLzjjjjJj3q3HjxkpMTIy8zwEDBuiJJ55QZmamTjjhBLVr107t2rVTyZIlY95GYYwaNUpBEER+7tOnj0qUOPRaquXLl9eZZ56pJk2aqEKFCipTpozWrl2rYcOGafny5dq2bZvuvvtuffnll5H9mD9/viQpISFB11xzjWrWrKk1a9bol19+0ZgxYwqsf9CgQZG/t2nTRl27dtWuXbu0YsUKTZw4UfPmzSvUfh7K51C1alV169ZN9erVU8WKFVWyZEmtXLlS77//vjZu3KiVK1fq73//u15++WXntqdNm6ZmzZqpZ8+eGjFihCZPnixJeuONNyRJxx9/vLp27ar33ntPCxculCQ9//zzGjhwoMqUKXPA+hYsWKDU1FTdeuutiouL0+uvv64tW7ZIku655x51795d9evX9x6PJUuW6NJLL40cj2bNmun888/Xnj179M4772jZsmVauXKlLrjgAs2aNUslS5aM+rMDABTCkZ75A8DRxPpV5f3/7PstaxAEwdq1a4NPPvkkePnll4N//OMfwVNPPRU0b9488vqrr7468tru3btH4q5fKV29enWwffv2yM/7fkMmKfj4448jy5577rkCy7Zt2xZZZn1DNmDAgEi8RIkSwaxZsyLLZs2aVeDXvQcMGHDI+3Ew+3/jvWvXrqB27dqBpCA9PT347rvvIsv79+9/wGfk+8b7s88+C66//vrIz0OHDo35G2/X+9z/T7Vq1YKXXnrJ+34P9RvvJ598ssA2hw8fXuhc3zfeQRAEO3fuDL7//vvgtddeC5599tngqaeeCv70pz9FcuLj4yO/Vj106NBIvEuXLgesa8eOHcGvv/4a+blFixaR17t+jXnJkiWF/ub+UD6H7du3B99++23wyiuvBM8880zw1FNPBT169IjkZmRkFHj9vt94V6pUKdi6dWsQBL/9V419t1m1atUgOzs7CIIgGDFiRIFl+/6mwP7n0dixYyPLxo4dW2DZX//618gy63y+9dZbC3yDvu9vDaxatarAb9d88sknQRBE/9kBAA6Ob7wBIES5ubm68cYb9eabb2rPnj3m63799dfI30855RR9+umnkn5rpfXvf/9bDRs2VKNGjdSuXTu1bdtWcXFxzvXUqFFDPXr0iPzcqFGjAss3b96slJQU7z6PHz8+8vdWrVoVKObVvHlztWrVKvJN3r6vjWU/5syZo+HDhx+Q37x5c5111lnOdZcsWVL9+vXT3XffrRUrVuiKK66Q9Fubr/79+2vlypXe97e/++67T2+++aZ27NihgQMHqlOnTlHl72vAgAFKT0/XE088oUmTJh2wfO3aterXr5/Kli171LWce+edd3TLLbc4f8Njr7y8PG3YsEFpaWlq06aN4uPjlZeXp6+++krNmjVTixYt1LBhQx1//PHq1KmTatasGck95ZRTNHPmTElS586ddfLJJ6tBgwZq2rSpTj31VB177LGF3tdYP4dnnnlG999/v7Kzs81173uu7q9bt24qV66cJB3QB/3cc89VUlKSJKlevXoFlm3evNm5voyMDGVmZkZ+zszM1DHHHBMpmvfTTz+Z+7LX2LFjI39fsGCBEhMTzdeOGzdO3bt3j/qzAwAcHBNvADgEgwcP9k6g7r333kK1atr7a92SdMstt2jmzJn673//q7y8PI0ePbpAL+rmzZvr66+/Vlpa2gHr2f9hPz4+vsDPvsn/Xps2bYr8vVq1agcs3zdmTRgKux+TJ0/WnXfeeUB+nz59zIm3JF177bV68MEHlZOTE5lon3322WrQoEHUE++aNWuqX79+evrppzV37ly9/fbbUeXvr2fPnurZs6fWr1+v8ePHa/z48frkk08K/Kr0M888E9rEe/8J0c8//+w9loUxdepUXXnllYUaP3vHcq1atTRkyBD1799fGzZs0Ny5czV37tzI65KTk/Xqq6/qkksukSQ9+uijWrx4sYYPH67s7Gx98803+uabbyKv79Chg7744ovI5PVgov0cPv74Y91+++0HXe/+Lcr2VaNGjcjf9//V8X2XlSpV8PHLOq5Vq1Y9IFatWrXIxHvvr5377Hs+H8z69eslRf/ZAQAO7tD/0xcAwPT+++9H/n7sscdq9uzZys/PVxAE6tWrlzOnVKlSevPNN7V69Wp9/PHHeuKJJ3T11VerQoUKkqTZs2frnnvuceaWLl26wM/WN+M+FStWjPx97dq1ByzfN7Z3n8LYD5+KFSvq8ssvLxC7+eabY17fvffeG/mm8uGHHz6kfdurSpUq6t69ux577DHNmTOnwP8f3/v/e8PQsWPHAsf7YL9tURgffPBBZB1xcXF69913lZ2drSAI9MUXX5h5l1xyiVatWqUff/xRgwYN0m233abjjz9ekpSdna1rrrkm8u1yuXLl9OWXX2rFihX64IMP9Mgjj6h3794qW7aspN9ayD355JNR73thP4d9z9Xk5GR9/fXXys3NVRAEeumllwq1rf3H/b72n2wXhqvl2b7nX2F6kO97Pjdr1kxPPfWU+Wffa1I0nx0A4OD4xhsAQrRx48bI3zt27KhmzZpJ+u2bpX2/xd7X/PnzlZ6eripVqhT4de3mzZvrtttukyRvgbVDlZmZGfn13J9++klz5syJ7Pfs2bML/Hrrvr8GG4urrroq5m9+b775Zr3yyiuSfiuodeaZZ8a8H5UqVdJtt92mBx54QGvWrIlpHatWrdJjjz2mfv36qXHjxgWWxcXFRSaQUuEmTLFKS0vTRRddFJlITps2TQMGDNBzzz13QEGxhQsXatKkSerdu7d3nfuO49TUVF100UWRgm3/+9//nDmbNm1SVlaW6tSpEyloJv32WxJ7J4M5OTmaP3++WrVqpdmzZ6tRo0aqVauWLrzwwsh6BgwYECmeV5hxH+vnsO97zMjIUOfOnSX99m30hx9+eNDthmHx4sUaN25c5DwbN25cgd7srVq1Oug69j2fV69erUsvvfSA34rYtWuXPvvss0gv+2g/OwDAwTHxBoAQNWrUSLNnz5YkvfrqqypRooTKli2rt956K/Jrnft79tln9dZbb6lTp0465phjVK1aNW3atElvvvlm5DVhTtz69eunQYMGKS8vT3v27FGHDh0KVDXf+81nmTJl1K9fv9D242CaNWumr776Sjk5OapXr94hf6t+22236cUXX/T+H2afnTt36sUXX9SLL76o5s2bKzMzU+np6dq9e7fGjh1b4NemD/VXvw/m2Wef1YQJE7Rs2TJJ0osvvqjhw4erW7dukfE0ceJE/fDDD7ryyisPOvHe9//ob9myReeee64yMzP1448/6uuvv3bmLFiwQCeffLLatGmjli1bqkaNGipVqpRGjBhR4HV7x/Idd9yhSZMmqVOnTpF/eFq1apUGDx58wGt9Yv0cGjVqFFk2c+ZMXXrppWrSpImGDx+uCRMmHHS7YTnnnHN09dVXR6qa71WqVKlC/aNV//799a9//Us7duzQpk2bdNxxx6lXr15KT09Xdna25s6dq9GjR2vLli1asmSJKlSoEPVnBwA4OCbeABCiv/71r7r00ksl/VZo7bnnnpP027eSnTt3LjAJ2FdOTo4+++wz57ISJUoU6v+ixqp+/fp66623dOWVV2rHjh3auHGjnnnmmQKviY+P15AhQw7ayihsh/It9/5SUlJ07733FsmxnT17duQfXPZXt25dPfLII4e8DZ+0tDSNGTNGl156aaQA3qJFiyLjL1p/+tOf9Mwzz2jVqlWSfmujt3cS1qdPn0i7LJfJkydHivHtr2fPngUKjW3evNn8djkhISHq/04QzecwYMAAvfHGG8rKypIkvffee5J+m+D27t1b77zzTlTbLgpNmzZVTk6Onn322QOWPfLII4U6/zIyMvTuu+/q8ssv1/bt27Vhw4YCrdt8ovnsAAB+/B9vAAjRJZdcov/9739q2bKlSpcurUqVKuniiy/WhAkTChRb2tc111yju+++W6eeeqrS09OVkJCgMmXKKD09Xb169dKYMWN03nnnhbrfvXr10vTp03XDDTeofv36SkhIUEJCgurVq6frrrtO06ZN+10WVrrxxhtVq1atmHJr166tsWPH6uGHH1bnzp3VqFEjVahQQSVLllT58uV14okn6qGHHtL06dPNz74o1alTR2PHjtVnn32m3r17q379+kpKSlKpUqVUtWpVnXHGGXrppZcK9f+mK1asqB9//FE9e/ZUuXLllJiYqDZt2mjo0KHmt66NGjXS008/rZ49e6phw4ZKTU1VyZIlVaFCBbVr107PP/98ZHIrSXfeeacGDBgQ6XlfpkwZxcfHKyMjQ3369NGkSZPUpk2bg+5rrJ9D/fr19f333+vMM89U2bJllZycrA4dOmjkyJGH1N/9UFSpUkUTJkzQ1VdfrapVqyo+Pl7HHXec3nnnHd11112FXs95552n2bNn67bbbtOxxx6r5ORklSxZUpUqVdLJJ5+sO++8U2PHjo0URYz2swMAHFxcEATBkd4JAAAA/Fb3YO9vEHTo0MGsBQEAOLrwjTcAAAAAACFi4g0AAAAAQIiYeAMAAAAAECL+jzcAAAAAACHiG28AAAAAAELExBsAAAAAgBAx8QYAAAAAIERMvAEAAAAACBETbwAAAAAAQsTEGwAAAACAEDHxBgAAAAAgREy8AQAAAAAIERNvAAAAAABCxMQbAAAAAIAQMfEGAAAAACBETLwBAAAAAAgRE28AAAAAAELExBsAAAAAgBAx8QYAAAAAIERMvAEAAAAACBETbwAAAAAAQsTEGwAAAACAEDHxBgAAAAAgREy8Q/LAAw/ouOOO877mtNNO0y233HJY9gcAUPzVrVtXzz33XOTnuLg4ffzxx0dsfwAAQNFg4v3/xcXFef888MADRb7NoUOH6uGHH/a+ZunSpYqLi9P06dOdyx988EFdfvnlknhAw5FxJM4doLi66qqrImO/TJkyql+/vh566CHt2rXrSO8aUCzte86ULl1a1apVU+fOnfX6669rz549R3r3gGJhzZo16t+/vzIyMhQfH6/09HR169ZNI0eOLLJt7P8Pvyh6pY70DhQXq1evjvz9/fff13333af58+dHYsnJyUW+zYoVK3qX79y586Dr+OSTT3TPPfcU1S4BUYvm3AmCQLt371apUsXv0rNz506VKVPmSO8GfgfOOussDR48WHl5efryyy/Vr18/lS5dWvfee++R3rWYcG4gbHvPmd27d2vt2rUaMWKEBgwYoA8//FCffvqp856Rn5+v0qVLH4G9BQ6vpUuXql27dipfvryeeuopHXvsscrPz9dXX32lfv366eeffz7Su4hC4hvv/6969eqRP6mpqYqLiysQc028R48erbZt2yopKUnly5dXu3bttGzZsgKveeutt1S3bl2lpqbqkksuUVZWVmTZ/r9qXrduXT388MO68sorVa5cOV1//fU65phjJEnHH3+84uLidNppp0Vev2LFCs2ZM0dnnXWW6tatK0k6//zzFRcXF/lZkgYNGqR69eqpTJkyatSokd56660C+xgXF6dBgwbp7LPPVmJiojIyMvThhx/GeCTxR+M7d37++WelpKRo+PDhatWqleLj4/Xjjz8qLy9PN998s6pWraqEhAS1b99ekydPjqxzyJAhKl++fIHtfPzxx4qLi4v8PGPGDHXs2FEpKSkqV66cWrVqpSlTpkSW//jjjzrllFOUmJio9PR03Xzzzdq+fXtkuet8A4pCfHy8qlevrjp16qhv374644wz9Omnnzr/e9F5552nq666qtDrnjVrlk4//XQlJiaqUqVKuv7665WdnS1J+vrrr5WQkKAtW7YUyBkwYIBOP/30yM+cGyhu9p4zNWvW1AknnKC//OUv+uSTTzR8+HANGTJE0v89q3Tv3l1JSUl65JFHJP32BcQJJ5yghIQEZWRk6MEHH4z8hkkQBHrggQdUu3ZtxcfHq0aNGrr55psj23355ZfVoEEDJSQkqFq1arrwwgsP+3sHDubGG29UXFycJk2apAsuuEANGzZUs2bNdNttt2nChAmSpOXLl6tHjx5KTk5WuXLldNFFF2nt2rWRdSxatEg9evRQtWrVlJycrDZt2ujbb7+NLD/ttNO0bNky3XrrrZHfQEHRY+Ido127dum8885Thw4dNHPmTI0fP17XX399gYG6aNEiffzxx/r888/1+eefa8yYMXr88ce96/3HP/6hli1batq0afrb3/6mSZMmSZK+/fZbrV69WkOHDo28du+DXLly5SKTlsGDB2v16tWRn4cNG6YBAwbo9ttv1+zZs/XnP/9Zf/rTnzRq1KgC2/3b3/6mCy64QDNmzFDv3r11ySWXaN68eUVyrIB77rlHjz/+uObNm6cWLVrorrvu0kcffaQ33nhDU6dOVf369dWlSxdt2rSp0Ovs3bu3atWqpcmTJ+unn37SPffcE/n2Y9GiRTrrrLN0wQUXaObMmXr//ff1448/6qabbiqwjv3PNyAMiYmJhfoNpoPZvn27unTpogoVKmjy5Mn64IMP9O2330bGdadOnVS+fHl99NFHkZzdu3fr/fffV+/evSVxbuDocfrpp6tly5YFnnseeOABnX/++Zo1a5auvvpq/fDDD7ryyis1YMAAzZ07V//+9781ZMiQyKT8o48+0rPPPqt///vfWrhwoT7++GMde+yxkqQpU6bo5ptv1kMPPaT58+drxIgROvXUU4/IewUsmzZt0ogRI9SvXz8lJSUdsLx8+fLas2ePevTooU2bNmnMmDH65ptvtHjxYl188cWR12VnZ+ucc87RyJEjNW3aNJ111lnq1q2bli9fLum3//5aq1YtPfTQQ1q9enWB32ZEEQpwgMGDBwepqane12zcuDGQFIwePdq5/P777w/Kli0bbNu2LRK78847gxNPPDHyc4cOHYIBAwZEfq5Tp05w3nnnFVjPkiVLAknBtGnTDthG586dgxdffDHys6Rg2LBhBV6TmZkZXHfddQVivXr1Cs4555wCeTfccEOB15x44olB3759ne8NsOx/7owaNSqQFHz88ceRWHZ2dlC6dOngnXfeicR27twZ1KhRI3jyySed6wmCIBg2bFiw7yUrJSUlGDJkiHM/rrnmmuD6668vEPvhhx+CEiVKBLm5uUEQuM834FD16dMn6NGjRxAEQbBnz57gm2++CeLj44M77rjjgGt+EARBjx49gj59+kR+rlOnTvDss89Gft73uv7KK68EFSpUCLKzsyPLv/jii6BEiRLBmjVrgiAIggEDBgSnn356ZPlXX30VxMfHB5s3bw6CgHMDxc++58z+Lr744qBJkyZBEPx2Ltxyyy0Flnfq1Cl49NFHC8TeeuutIC0tLQiCIHj66aeDhg0bBjt37jxg3R999FFQrly5As9pQHEzceLEQFIwdOhQ8zVff/11ULJkyWD58uWR2Jw5cwJJwaRJk8y8Zs2aBf/85z8jP+9//0HR4xvvQli+fLmSk5Mjfx599FFVrFhRV111lbp06aJu3brp+eefP+Bfh+rWrauUlJTIz2lpaVq3bp13W61bty7UPm3btk1jxoxR9+7dva+bN2+e2rVrVyDWrl27A77NPvnkkw/4mW+8UVT2HdeLFi1Sfn5+gXFZunRptW3bNqoxd9ttt+naa6/VGWecoccff1yLFi2KLJsxY4aGDBlS4Lzt0qWL9uzZoyVLljj3Cygqn3/+uZKTk5WQkKCzzz5bF198cZEUGZw3b55atmxZ4FuPdu3aac+ePZG6Cr1799bo0aO1atUqSdI777yjc889N/JfNzg3cDQJgqDAbxLuPy5nzJihhx56qMB4vu6667R69Wrl5OSoV69eys3NVUZGhq677joNGzYs8mvonTt3Vp06dZSRkaErrrhC77zzjnJycg7r+wMOJgiCg75m3rx5Sk9PV3p6eiTWtGlTlS9fPvJclZ2drTvuuENNmjRR+fLllZycrHnz5kW+8cbhwcS7EGrUqKHp06dH/txwww2Sfvu17vHjxyszM1Pvv/++GjZsGPm/FpIOKPoRFxd30Aqdrl8jcRk+fLiaNm1a4CQDiqvCjuu9SpQoccDNJj8/v8DPDzzwgObMmaNzzz1X3333nZo2baphw4ZJ+u0G8+c//7nAeTtjxgwtXLhQ9erVi3m/gMLo2LGjpk+froULFyo3N1dvvPGGkpKSCjWuD1WbNm1Ur149vffee8rNzdWwYcMiv2YucW7g6DJv3rxIrRvpwHGZnZ2tBx98sMB4njVrlhYuXKiEhASlp6dr/vz5evnll5WYmKgbb7xRp556qvLz85WSkqKpU6fq3XffVVpamu677z61bNnygBoJwJHUoEEDxcXFHXIBtTvuuEPDhg3To48+qh9++EHTp0/XscceWyT/DQqFx8S7EEqVKqX69etH/uxbjfz444/Xvffeq3Hjxql58+b673//W6Tb3ltJdvfu3QXin3zyiXr06FEgVrp06QNe16RJE40dO7ZAbOzYsWratGmB2L7/YLD35yZNmhzSvgMuewv97Tsu8/PzNXny5Mi4rFKlirKysgoUfHK11GvYsKFuvfVWff311+rZs6cGDx4sSTrhhBM0d+7cAuft3j9UZ0bYkpKSVL9+fdWuXbtANeYqVaoU+M2o3bt3a/bs2YVeb5MmTTRjxowC58XYsWNVokQJNWrUKBLr3bu33nnnHX322WcqUaKEzj333Mgyzg0cLb777jvNmjVLF1xwgfmaE044QfPnz3eO5xIlfnvETUxMVLdu3fTCCy9o9OjRGj9+vGbNmiXpt+e7M844Q08++aRmzpyppUuX6rvvvjss7w8ojIoVK6pLly566aWXClz799qyZYuaNGmiFStWaMWKFZH43LlztWXLlshz1dixY3XVVVfp/PPP17HHHqvq1atr6dKlBdZVpkyZA+YRKFpMvGO0ZMkS3XvvvRo/fryWLVumr7/+WgsXLizyyWrVqlWVmJioESNGaO3atdq6dat27dql4cOHH/Br5nXr1tXIkSO1Zs0abd68WZJ05513asiQIRo0aJAWLlyoZ555RkOHDtUdd9xRIPeDDz7Q66+/rgULFuj+++/XpEmTDii2AxSFpKQk9e3bV3feeadGjBihuXPn6rrrrlNOTo6uueYaSdKJJ56osmXL6i9/+YsWLVqk//73v5HKtpKUm5urm266SaNHj9ayZcs0duxYTZ48OXL+3X333Ro3bpxuuummyDePn3zyCWMaR9Tpp5+uL774Ql988YV+/vln9e3bN6pv13r37q2EhAT16dNHs2fP1qhRo9S/f39dccUVqlatWoHXTZ06VY888oguvPBCxcfHR5ZxbqA4ysvL05o1a7Ry5UpNnTpVjz76qHr06KGuXbvqyiuvNPPuu+8+vfnmm3rwwQc1Z84czZs3T++9954GDhwo6bcOGa+99ppmz56txYsX6+2331ZiYqLq1Kmjzz//XC+88IKmT5+uZcuW6c0339SePXsK/CMWUBy89NJL2r17t9q2bauPPvpICxcu1Lx58/TCCy/o5JNP1hlnnKFjjz02cu2fNGmSrrzySnXo0CHy3zMaNGigoUOHRn7L6bLLLjvgt3Dr1q2r77//XitXrtSGDRuOxFv9/Tuy/8W8eCpMcbU1a9YE5513XpCWlhaUKVMmqFOnTnDfffcFu3fvDoLgt+JqLVu2LJDz7LPPBnXq1In87Cqu5ipq8Oqrrwbp6elBiRIlgg4dOgTffvttUKtWrQNe9+mnnwb169cPSpUqVWA7L7/8cpCRkRGULl06aNiwYfDmm28WyJMUvPTSS0Hnzp2D+Pj4oG7dusH777/vff+Ai1VcbW9hp71yc3OD/v37B5UrVw7i4+ODdu3aHVAAZNiwYUH9+vWDxMTEoGvXrsErr7wSKa6Wl5cXXHLJJUF6enpQpkyZoEaNGsFNN90UKQ4VBEEwadKkoHPnzkFycnKQlJQUtGjRInjkkUciyykigjD4CkXt3Lkz6Nu3b1CxYsWgatWqwWOPPRZVcbUgCIKZM2cGHTt2DBISEoKKFSsG1113XZCVlXXAttq2bRtICr777rsDlnFuoDjp06dPICmQFJQqVSqoUqVKcMYZZwSvv/565JkqCNwFZIMgCEaMGBFkZmYGiYmJQbly5YK2bdsGr7zyShAEv91HTjzxxKBcuXJBUlJScNJJJwXffvttEAS/FRXs0KFDUKFChSAxMTFo0aIFzz4otlatWhX069cvqFOnTlCmTJmgZs2aQffu3YNRo0YFQRAEy5YtC7p37x4kJSUFKSkpQa9evSJFN4Pgt2LNHTt2DBITE4P09PTgxRdfPGAeMn78+KBFixZBfHx8wBQxHHFBUIj/tY9i5eabb9auXbv08ssvF8n64uLiNGzYMJ133nlFsj4AAAAAwP8pdfCXoLhp3rz5AVXIAQAAAADFExPvo9D1119/pHcBAAAAAFBITLxRqB6BAAAAAIDYUNUcAAAAAIAQMfEGAAAAACBETLwBAAAAAAgRE28AAAAAAELExBsAAAAAgBAVuqp5XFxcmPtxSNs/XFW5Gzdu7Iy/+OKLZs4HH3zgjE+bNs3M2blzpzOen59v5jRv3twZP//8882cRYsWOeNPPfWUmbNlyxZz2dHoUMbOkT4nDpfWrVuby/r06eOMb9y40czJyspyxnft2mXmVK5c2Rn3fX7Lly93xlu2bGnmVKtWzRmvUqWKmdOxY0dz2dHoaDgnisP9oGrVqs746aefbuZce+21zrjvujpv3jxn3LpPSFL58uXNZZmZmc74hAkTzJy//OUvznhubq6ZEwvrcy0OnTdi3Yff232ibt26zvhpp51m5vTo0cMZ990n3n77bWd86tSpZo71jHbBBReYOZ06dXLGc3Jyot63V155xcz5vTka7hNFrUQJ93eVe/bsiXpdycnJ5rJmzZo5402bNjVzZs2a5Yzv2LHDzKlRo4a5bO3atc74jBkzzBxLcbhfHy6FeT984w0AAAAAQIiYeAMAAAAAECIm3gAAAAAAhIiJNwAAAAAAIWLiDQAAAABAiOKCQpaUK8oqhIerwt1xxx1nLrvkkkuccV/1y927dzvjSUlJZk5iYqIzXqlSJTOnKC1YsMBcZlVibNSokZljVTr86quvzJx//OMfzvjs2bPNnMPlj1iZM1p33nmnueycc85xxn1VPo855hhnPCUlxcyxqppv2rTJzNm6dasz7qsgbVXZrV+/vpljvZ+jVXE6J4qyyrU1hiRpwIABzvgZZ5xh5sTHxzvj27dvjzrHqsYs+c8Li68Dxq+//uqMr1692syx7mO+8+/77793xv/5z3+aOZs3bzaXHWm/x6rmZ599tjN+6623mjlWJfsyZcqYOVZlZd/Ytjq1WJ0nJGnp0qXOuK9jhjXurfuHZJ/HNWvWNHNGjhzpjN98881mTnFWnO4TxZn1PO0b+02aNHHGW7VqZeb88MMPzrjvGu3r1mKds1a3GEmaPn26ueyPgqrmAAAAAAAcYUy8AQAAAAAIERNvAAAAAABCxMQbAAAAAIAQMfEGAAAAACBETLwBAAAAAAjREWknFoty5cqZy958801nvEWLFmZOiRLuf3PIysoyc6zy+r7WLVYLstKlS5s5qampzrivTY3Vvqko27NJUkJCgjNutZuR7DYjVvsDSbriiiui27EY0RLj4B544AFzWXp6ujPua5dXsWJFZzyW42mNR9/6Ymkn1r59ezOnXbt2zrjV2qa4K07nRCztxOrVq+eMf/bZZ2aO1SbRuuZL9nXfuuZLUl5enjPua/mSnJxcZNuR7Ouxr7VMqVKlolqXb1lOTo6Z869//csZHzZsmJlzuByt7cSs80Gyr+3W+SBJZcuWdcatZyrJfj7xtfmy7i0+1nZ87S2ttmG+fbPOfd95bLUa892P7rjjDnPZkVac7hNHmu8cq1u3rjO+bNkyM6dnz57OuK916TvvvOOM+55DfPttPQuVL1/ezLHul1OmTDFzfm9oJwYAAAAAwBHGxBsAAAAAgBAx8QYAAAAAIERMvAEAAAAACBETbwAAAAAAQuQuVVoMDR061FxWp04dZ3zdunVmjlXl0qreKtlVLn0VGq31+XI2bNjgjJcsWdLMsfgqjcYiNzfXGfdV/7Wq/J166qlmTuPGjZ3xn3/+2bN3CEPDhg3NZVYlZKsSsyQlJSU541a1XElav369M+47J6zOAb4OCdb54utCYI3jo7WqeXESS+Xcxx57zBlfs2aNmWNVI/Z97ta++aohW9d93/liVSj3XXPj4+PNZdb55+vOYb0n3z5Y55KvEnq/fv2c8W+++cbMyc7ONpdBuv32281l1nXVx/pcfR0mrPHjO1eWLFnijFtVyH374Ktq7jtXLFZHAd/zo1XFunnz5mbOueee64x/8cUXnr3D4ear9G3dd3ydJ1asWOGM+7r9nH/++c64b6x8++235rJ58+Y5476OB9ZczNf1yJpT/J7xjTcAAAAAACFi4g0AAAAAQIiYeAMAAAAAECIm3gAAAAAAhIiJNwAAAAAAIWLiDQAAAABAiIpdO7FWrVo541aZesluv+Vr7WC1IfK1xKhZs6Yz7muDZLXe8LVusfbbamEh2W1qfO1wrFYeWVlZZs6vv/4a1bp8fO/n2muvdcbvuOOOqLeDQ1O5cmVzWUpKijNutSySpNTUVGfcaukk2eerr12ebx8sVmsZX9uyChUqRL0dHJq0tDRzWfXq1Z1xXxsiq8WV77pmXfd9484ar752R9Z10nf99N3HrP3zrc86Dr4cq82XrwWZtW/dunUzc959911zGaQhQ4aYy2699VZn3NdmzGonZN0LJP/zjmXnzp3OuO9+ZNm2bZu5rCjbGVn7LNn3Pat1lETbsCPB90yRkZHhjPvaQR533HHOuO9zX7VqlTNer149M8c6x3ztG605jSRlZmY647Vr1zZzrP2z5g2Sff325Rzt+MYbAAAAAIAQMfEGAAAAACBETLwBAAAAAAgRE28AAAAAAELExBsAAAAAgBAVu6rmHTt2dMatasO+Zb5KsVaV4ry8PDPn7rvvdsatCoSSXZmvRo0aZs7q1audcV+1Rauapu+4WZUYTzjhBDOnf//+zrhVWV6yq7T7Pp8LL7zQGaeq+eFnVWOV7LHqq3bcrFkzZ9xXHdxXCdniO18sOTk5zrjVNUCSmjZtGvV2cGh8Y8Wqau4bk1blV1+FcqvSt++aa13zfOPLt8ziq8Jvrc93PbZyfMe0SpUqzrjvXmF9Dp07dzZzqGruN2nSJHPZ+PHjnfHu3bubORMnTnTGfV1krA4AGzduNHOsZxrf+LHuE77OM9Z++yqhW2Pbx9qHe+65J+p1ITxW5XJJSk9Pd8Z9lfF/+eUXZ7xFixZmjnXOWh0FJKlu3brO+KmnnmrmTJ482VzWtm1bZ9xXjf27775zxn33iXbt2jnj8+fPN3OmT59uLjsa8I03AAAAAAAhYuINAAAAAECImHgDAAAAABAiJt4AAAAAAISIiTcAAAAAACFi4g0AAAAAQIiKXTsxq42U1bpFslun+ErYJyQkOONbt241c1599VVn/MwzzzRzrNZcgwcPNnP+/Oc/O+OzZ882cypWrOiM+9rKWK0Jnn32WTPnxhtvdMZ9rUSsY221bpKkxo0bO+MNGzY0cxYsWGAuw8FZbZBSUlLMHGtM5ufnR51Tvnx5M6dWrVrOuK/dk9UOxjfurFY1vvZVaWlp5jKEw9eKxbrmWW3GJLv1nK8lndW6yNdectGiRc740qVLzZzt27dHtX1fjmSfm1YrL8k+3l27djVzrP3znedWi0vfeY7YvfDCC874gAEDzJzly5c74+vXrzdzrPHouxZnZWWZyyzWue87H6xnl9KlS5s51r75Wm8OHz7cGfe1LcPh57s+rVu3LuqcIAic8a+//trMscZEt27dzJyvvvrKGffdw0aOHGkus+ZPvjlFpUqVnHHf+WedZ77nKqtFW3Z2tplTnPCNNwAAAAAAIWLiDQAAAABAiJh4AwAAAAAQIibeAAAAAACEiIk3AAAAAAAhKnZVzVu2bOmMr1ixwsyxqvZZVZp9ypUrF3XOiBEjzGVWNb+mTZuaOXfccYczPmzYMDPHqnboqzY+depUZ7xVq1ZmjlVd3ld11qqOuGfPHjPHqpx68sknmzlUNT80VmV8X6VIqwp45cqVzRzrvPSNIWusJCYmmjnjxo2Lal2SPb59FaTj4uLMZQjHe++9Zy774YcfnPHevXubOc2bN3fGH330UTPn559/NpdFq2zZsuYya4z7xr7vXLK6TPgqz7777rvO+L333mvmTJ482RmvVq2amWNVuc7IyDBz4Od7BrCud+3btzdzHnnkkaj3wfpcfd1qrPGdm5tr5ljv1XcM8vLynHFfNWiLL+ezzz6Len0IjzW+fN0drPHqu3Za1/YqVaqYOdY1etmyZWaOVR184sSJZo6vA4c1R/Gds9b49z0jWeem71yyutwU5T05THzjDQAAAABAiJh4AwAAAAAQIibeAAAAAACEiIk3AAAAAAAhYuINAAAAAECImHgDAAAAABCiI9JOzGrdIknr1693xn0l7EuWLOmM+0rYW60ENm7caOZYfO/HalWRlpZm5ljtOnzvJz8/P+ocX2sui9V+oGbNmmZOLO3ErJYhp5xyipnzxhtvmMtwcBUqVHDGrTEs2Z+hryWHtT7rPJakZs2aOeMrV640c2rXru2ML1261Myx2oZt27bNzLHOPYTnySefNJdZY3LUqFFmzrRp05xxX3tJq3WJ75prjSPffWfLli3OuG/cBUFgLrP2LzU11cyxzr9FixaZOVb7Nl97Qus4+K5B8PM9O1lWr15tLrM+82OOOcbMsa6rWVlZZo51HvtaO1otiHxjzmrrFEvbJF+7JxQvVstT3/XbGntW+y9J2rRpkzPua3dszU/Kly9v5lx77bVRbV/yt3a0joPvWmy1BvOdS1Yb2507d5o51n7TTgwAAAAAADDxBgAAAAAgTEy8AQAAAAAIERNvAAAAAABCxMQbAAAAAIAQHZGq5nfffbe5zKrm56tKaVXNttYl2dUJfdX3Wrdu7YxXqlTJzLEq9pUuXdrMsSr2+arYWu/HV13aqpB48cUXmzlW5WurCrlkV8v15Vj7bX0GOHRWZc6cnJyo1+Ub3ykpKc74hg0bzByrSrNV8Vmyz4k6deqYOVZVZd91wfdeEY6vvvrKXNapUydn/IILLjBzzjzzTGfc1ymhb9++zriv8mz9+vWd8eTkZDPHGvu+LgC+675VLdbXZeLtt992xn1Vqa37vK9a7ebNm53xnj17mjmZmZnOuK+aL2JnVfS2ruuSPbZ8lZ2tDgC+sW1d831jzhJLNfh169ZFnYMjwxp7vnmDdZ32PYckJSU549a8RbLHse9ZrHv37s74mDFjzBxfhxfrPmZVLpfse1LZsmXNHKvD0/Tp082c6tWrm8uOBnzjDQAAAABAiJh4AwAAAAAQIibeAAAAAACEiIk3AAAAAAAhYuINAAAAAECImHgDAAAAABCiI9JObNy4ceYyq0y81YZFksqVK+eMW2X8JWnhwoXOuK/E/4QJE5xxXxsWa5lvO1ZJfl8Z/7i4uKi3Y7UF8bWIWbBggTPuaxdgvR9r+5K0atUqZ/zjjz82c3BorLHqa/tm8X22W7dudcabNGkS9Xas9kOS3YLQOvclqXbt2s64r+2N73xBOB5//HFzmdV20bqmSNK8efOc8W7dupk59913n7nMYu1bXl6emWNdw602Y5K/FZJ1Pfa1xbPa6PjOv0mTJjnja9asMXNGjRrljPvOWdqGxc66TvueaX799VdnvEWLFlFvxzfurfHtG6fWuWK1ypTs+5vV0kmSKleu7IyvXLnSzLH4nutiaWmGwrHaZflaF1st83w51hj3jUmLr9XZyJEjnfEVK1aYOb59sMa/L8dq2+d7frJapPmuDdY+WPMgyX+/PNz4xhsAAAAAgBAx8QYAAAAAIERMvAEAAAAACBETbwAAAAAAQsTEGwAAAACAEB2RquaDBg2KelmFChXMnAYNGjjjffv2NXM6dOjgjPsqpM6ePdsZ37Jli5ljVeC0KssWNV+VP6vSqK+aZ2pqqjM+c+ZMM6d3797mMhQfsVS4tfhyrAqXVsVQn0WLFpnLWrZs6Yxblfklafv27c64Ne4lf+cAhGPo0KHmsk6dOjnjrVu3NnOGDx/ujH/66admTtWqVZ3x5cuXmzmxVBS3Krj6qiH7WJWSreqykl2t1uooIkl16tRxxm+55Zaoc0477TQzZ9q0ac749OnTzRzEbunSpc64r5NFmTJlnHHfc521HV+l70qVKjnjvur71vp8VZWt90oV8uLF123Het6wOk9IUkZGhjPuu3Za84NYqmz77hNWdxXfeembh8TSXcl67vNVY7c6BPiOj/W5Wue/JG3YsMFcdrjxjTcAAAAAACFi4g0AAAAAQIiYeAMAAAAAECIm3gAAAAAAhIiJNwAAAAAAIWLiDQAAAABAiI5IO7FY+NpBTJo0yRn3tYM4/fTTnXFfCXurJUZSUpKZY5Xkj6VFk681mLUslrZOVusYyW5tM27cODMHRwdrrPhapFhtNHJzc80cq32EryWHxdcaLDMz0xn3tctbu3atM16jRg0z53C1BsT/adq0qbnMGntr1qwxcyZMmOCMt2vXzsxp3ry5M+67h8QyVqzz0redWO4Vvn2z9sF3TP/73/864742X4sXL3bGV6xYYeb4rgEoetb5VdRtJ63xaD2D+Nbne3607kextLf0tXvC4ee7Rlpjxfc8b7VP9M01YmG17PK9H6tll+9ZzCc5OdkZ991brFZsDRs2NHNq1qzpjPvOJetZsVq1amYO7cQAAAAAAPiDYOINAAAAAECImHgDAAAAABAiJt4AAAAAAISIiTcAAAAAACEqdlXNrYp5vgp3VhVuXwXAbdu2OeO+yq67d++OejsWX2XAWNZXlGKpvLtly5Yi3U4slXwRDt8xt6pv+iqHW+dyLGNozpw5UedYVWwl+7xcv369mcOYPPwyMjLMZdaYrFWrlpljVef2Vdq3qv1nZWWZOSVKuP+t29c5wLpOWvejWPmq+VrVaqtUqWLmWMfOVy3a+ozKly9v5lSvXt0Ztyqk4//EUoncGqu+a6T1jOarNm7x5VjbsSo+S9K6deuccd/Yzs7ONpeh+PDNG7Zv3x51jvVcs3HjRjOnUqVKzngsz1W+eYM1Jn1VzX3v1brmW/vm47u3WNXGfc+DVmcD33lenPCNNwAAAAAAIWLiDQAAAABAiJh4AwAAAAAQIibeAAAAAACEiIk3AAAAAAAhYuINAAAAAECIil07MavEvlXa3mfRokXmMqudmK9UvtWqwsd6P0XdTsy3Pov1fnwtBizW8fSxWutIRd8qBwdnfR5lypQxc6y2Dr5zxWphE0uLlilTppjLrPcTSxu7+Ph4M8fXcgrh8F07rJYvvmuK1QKsbNmyZo41Vnzjy1rmu35b79V3DHzrs/bbtz7rGuB7r1abGJ+KFSs64777co0aNZxx2okdnPWZ+9qMWe3gKlSoYOZY10jr8/bxjSvrfE1NTTVzYnmus86vOnXqRL0uXytBHBrf9du6pviev63WYL7ng1jmANaY8F1vrXPZd45ZLdUkKS8vzxn3HVNr/3xjvFq1as641SZSstu3Wc+jxQ3feAMAAAAAECIm3gAAAAAAhIiJNwAAAAAAIWLiDQAAAABAiJh4AwAAAAAQomJX1dwSSwXs3NxcM8eqZOmrTmhV5vNVXLUqF/oqJ1o5sVS+9W0nlqqF1j5QmfPoZ40h37izxr6vwq21vrlz53r2zm3Lli1R5/jOCV/V0FjWh3DEUtHbV6l506ZNznhiYqKZE0t18FjGipUTyz1EsjuE+O591nnue69r1qxxxq2q85J9L/edl1aVbRyc75ywrF+/3hmfPXu2mbNixQpn3PesYY0TqwqyZD/XLV26NOrt+Cqhr1692hm3KuzjyLCqkEuxdWrwVQG3WM/Gvu5B1nXQemb3iaUDiGRfV33dQaz35OtEYN1jY9nv9PR0M6c44RtvAAAAAABCxMQbAAAAAIAQMfEGAAAAACBETLwBAAAAAAgRE28AAAAAAELExBsAAAAAgBAdNe3EYmnD4muVYZXE923HWuYre2/x7VssLY2s9jGxtLbx7Vssbcui3T6ODKu9hq+lktUmwtdWxWpbZLWc8cnKyjKXxdL+zxrfvnZ5VgsbHBnW9dN3XVu7dq0z7hv7sYil1VksrbxiabfmaxMTyz0plvPC2u+i3jfE7pRTTnHGFy9ebOYsW7bMGfe1M9q2bZszXq5cOTPHagEWS2vZtLQ0M8dSvXp1c1nVqlWd8XXr1pk51vkQSxu4PyJfyy7readBgwZmjnWtsVonSlLz5s2d8ezsbDMnISHBXGaJZUz42pNZz3CbN282c9q0aeOMb9261cyx7r2+toHWPaxy5cpmTnHCN94AAAAAAISIiTcAAAAAACFi4g0AAAAAQIiYeAMAAAAAECIm3gAAAAAAhOioqWpe1GrWrOmM+yr2WRUNfdW5raqUVlW+w8nat/z8fDPH2m8qy/5++SpF5uTkOONWxVDJrjT6yy+/RLdjB2FVPPftm1X91ldldPv27dHtGA5ZLB0RfNdc67rvq4obS1cIax98VfNj6SQRy/GJZR98x9SqCL9lyxYzJ5ZqvrHk/JH4KtxbYzU9Pd3Madq0qTPuq2pevnx5Z9x3b7HuB0lJSWbOMccc44z7xpyvSnq0fJWqL7vsMmf8ueeeM3OoXn5ofNc065nV93ywcePGqHOs66BvrFiSk5PNZVZ1fl+O1QXAtz7fuVS3bl1nfO7cuWbOxIkTnfGzzz7bzJk1a5Yz7rsfNW7c2Bn/+eefzZyw8I03AAAAAAAhYuINAAAAAECImHgDAAAAABAiJt4AAAAAAISIiTcAAAAAACFi4g0AAAAAQIiOmnZisbRH8fG1GbBYLQN2795t5ljl7X1l761lvmNg5fjaUVitcvLy8swcax98bXeiXReODGscly1b1sypVauWM+4bD9Z5NH/+fM/eRW/Tpk3OuNXaRrJbfBR16yYcHXytqqxrq+/aXpTtJWMdd1aeb31WaxlfqyqrjY6vbeBxxx0X1fal4tGasziLpSVVly5dzGVWayDfubJt2zZn3Go/JEkrV650xq22QJL9Xn/99Vczp0WLFs742rVrzZxKlSo54752tFYL2/r165s5Rd1i84/GNyat5x1fzg8//OCM+84xq+VqLC14ffMWax9KlYptmme1SfU9P8UyXq0WbVZcsu8HvnuYr3Xh4cY33gAAAAAAhIiJNwAAAAAAIWLiDQAAAABAiJh4AwAAAAAQIibeAAAAAACE6Kipal7UrMrdvkqDVkVBX45VadBXfc9an6+yq7U+X0VDK8eqwujjq3SI36+kpKSoc6wqxL6KsLGwKtk2adLEzLGuC74q7b7zEuHIysoyl1lj0leB22JV5pZiq6waS4Vpa32xdMaQ7PuLb7/z8/Oj3o51vJcvX27mtG7d2hn3ddqIpTow/KxK35I0c+ZMZ9z3OVidLOLj46PbsYNsx+I776xlO3bsMHPS09Odcat6u2+Zr7I7Vc0PjXXdkuwOJr7P3ZoDxHJv8bHOiy1btpg51nv1VWnfunWruczqWON7r4sXL3bGa9SoYeasX7/eGfc9W1rXkxUrVpg51ud9JPCNNwAAAAAAIWLiDQAAAABAiJh4AwAAAAAQIibeAAAAAACEiIk3AAAAAAAhYuINAAAAAECI/rDtxGJp62LxtVTxtWixWOX6fduJZfvW+nw5VjsFX9sdSyzHBoefr31E2bJlo4pLdhumom4ntm7dOme8cePGZo7VFs/XLm/lypXR7BaiYLUN8V07rPHqa/Vj8bWR87WqsVj7bb1PSdq9e7czHsv9QLJbTFrbkWJri2ltZ+nSpWaOdbx9++b7jOBntbJavXq1mWO1J/K167HGgvU8IcX2TGGtz/e8F0tLM6vlarVq1cwc6z5RpUqVqLePwomlraLvPmGNcV/rq1iu39Y49rUHtpb5xr6vNZ+1Pl9LM+tcqlq1qplj3fsmTZpk5ljHOzc318yhnRgAAAAAAH8QTLwBAAAAAAgRE28AAAAAAELExBsAAAAAgBAx8QYAAAAAIER/2KrmvkrN0Srq6tyHq6q5tZ1Yqpr7qljj6OarPGuNId94yMrKcsatauex2rhxY9Tbsd6rr+q07/jg0FjXoliqacdSfd5X9dXah1g6Zviu7dYyX45vH6wqu7G8V1+18ZSUFGd8wYIFZk4slXljre4OqXbt2s6473hbn5HvGmlVQveNH18FZ0uFChWccd812tqOb/tLlixxxhs0aGDmrF271hlPTU01cypWrOiMb9q0yczB/4nlc/eN4w0bNjjjrVu3jm7HDiIvL88Z912jY3l+sq7RkrRjxw5n3FfB3eKrKJ6enu6M++4Tp556qjNuHTfJ35nmcOMbbwAAAAAAQsTEGwAAAACAEDHxBgAAAAAgREy8AQAAAAAIERNvAAAAAABCxMQbAAAAAIAQHTXtxIq6ZZfFV64/FtZ+x9ICJZZ9i+W4+VqtxdKKBkcHq+XL9u3bzRxrHPvaia1atSq6HYvR0qVLnfHSpUubOVYLDZ/8/Pyoc3BoYmmTGEs7Md+10NoH3/iy1ue7fhZ1e7JYWoPFcr+y2iTNmTPHzLGOj+9zoJ1Y7Kxx5zveOTk5zrjvmm+dE74WSNa49537ycnJzrivrZTVgqhmzZpmzpQpU5xxq82RJK1evdoZ97Uts9qj0U4sPLE8A+Tm5prLrLHv+9yt8eq7RlvLfPcj37OLdT772olt3brVGd+2bZuZY+3fli1bzBzruuW7NsTyuYaFb7wBAAAAAAgRE28AAAAAAELExBsAAAAAgBAx8QYAAAAAIERMvAEAAAAACNFRU9U8liqtPlY1TV9lzlhYlTl9VWytioZFfQxiUZRVzQ/XPqNwrOqSvoqwZcqUiSou+atVFqV169Y5475xF0ul6liqTuPQxFLVfPny5VFvx6p4LEnr1693xrOysswc37lksa65vvtBLFXAfeuLj493xq1OCJJd/dZXXd7aB9855qsODL/KlSs7477rtzXumzdvbuZY48RX7djaB985lJKSEtW6JLvacYsWLcycL774whn33dusfbAql0uM7TBZ1zTffaJcuXLOeLNmzcycmTNnOuO+a6f1PO0bD1aOr3K57/6WmJgYdY51r/Ltg3UcYrlX+nKK07nEN94AAAAAAISIiTcAAAAAACFi4g0AAAAAQIiYeAMAAAAAECIm3gAAAAAAhIiJNwAAAAAAISo+9dWLCV8blqJs6+LbjrXM11LFtw8WqyWPb98ssbQTQ/ESSzsxi28M5ebmRr0+a3z72kpZbWKsVoKSfY772t5Y28Ghi6X1lcX3GVqsljO+Zb7WKRUrVnTGrXEnxdZe0sfK852z1rGzWoZJUo0aNZxx3/litVzytYLxtYqCn9VOzDcWNm7c6IynpqaaOdbnt3r1ajPH+lw3b95s5mzfvt0Zj+WZxic7O9sZ9+2b9fxm7bMkpaWlOePz58/37B32stp/SVJ6erozPn36dDOndu3aznjdunXNnBkzZjjjvmuadc33PWdb95BVq1aZOZUqVYp6fb7xal0DcnJyzJyqVas6475nO+sea13PJP899nDjG28AAAAAAELExBsAAAAAgBAx8QYAAAAAIERMvAEAAAAACBETbwAAAAAAQnTUVDX3VbiLhVXpr2HDhmaOVWnQV23cWmZVkPbl+LZjHR9fJT9fVcVotxNLVfOi/kwRDl+lVouv8mUsVc2tqrS+8b1hwwZn3Fel3TrHYqmejkNnXVd8lemtzzeWysYfffSRucyqmLtu3Tozx7rmxtI5wHf99lU8t5b57i/W/m3dutXMmTJlirks2u34jk9RV6z+I0lOTnbGfVWIK1SoEPV2EhISnHHfeWyN7ypVqpg569evd8Z91fet9fkqJNerV88Z951DsXSrSUlJMZfh4GbPnm0uW7JkiTPuu6ZZVcA/+eQTMycxMdFcZonlfpCXlxdVXJLKly9vLsvKynLGfeeS9Tzme+azjqmvW8WwYcOccd/54us2crhxxwIAAAAAIERMvAEAAAAACBETbwAAAAAAQsTEGwAAAACAEDHxBgAAAAAgREy8AQAAAAAI0VHTTqyoWWX0faXyrfYWvrYTVgsJXwsUX6uxaPnaLVmtelasWGHmlC1b1hm32mv4+I6Br8UGwmG1VfG1b9m4caMzbrWPkWJrvxVLOzGrJUd8fLyZY7UN87W2sFry4NBZrVh87bKsseJrnWJ57LHHos5B7Kzzz3eviOVzxW8aNGjgjFutliT/td1ifX7W84Rk3yfGjRtn5lx22WXOuK/13siRI51x35iL5Rpjtdj0HetRo0aZy3Bw27Zti2mZ5YQTTog6J5bnHd88xGI9C/labPmes619iOX89z0jWedm7dq1zZxffvnFGbdaoBU3fOMNAAAAAECImHgDAAAAABAiJt4AAAAAAISIiTcAAAAAACFi4g0AAAAAQIiOmqrmviq2ViVUn2nTpjnjc+fONXO2bNnijMdShdxXMTM7O9sZ971P6/hYlZ0lu6Lhzp07zZwKFSo445MmTTJzot0+joyZM2c645999pmZY439TZs2mTmxVGqNZaysWbPGGV+4cKGZY43vdevWmTmzZ8+ObsdQaNY4WrBggZnz66+/OuMTJ06Mevu++44llvsRfvPOO+844xkZGWbO1KlTw9qd370bb7zRGfc9N1jPLu+//76ZY3U9WbZsmZlTq1YtZ3zp0qVmzpQpU8xl0froo4+izvnggw+KbPs4MnwV8K0K5b7K5VYVcF+OdQ/xnZfWfvu241tf1apVnXHfs5BVvdxXQT43NzfqHMvR0imJb7wBAAAAAAgRE28AAAAAAELExBsAAAAAgBAx8QYAAAAAIERMvAEAAAAACBETbwAAAAAAQhQX0PsEAAAAAIDQ8I03AAAAAAAhYuINAAAAAECImHgDAAAAABAiJt4AAAAAAISIiTcAAAAAACFi4g0AAAAAQIiYeAMAAAAAECIm3gAAAAAAhIiJNwAAAAAAIWLiDQAAAABAiJh4AwAAAAAQIibeAAAAAACEiIn3Ueiqq67SeeedV+jXL126VHFxcZo+fXpo+wQUhbi4OH388cfm8tGjRysuLk5btmw5bPsEHG245gPA0euBBx7QcccdZy4fMmSIypcvf0jbiHYugaLBxPsQrF+/Xn379lXt2rUVHx+v6tWrq0uXLho7duyR3jWgWDrUcyYzM1OrV69Wamqq93XcUHCkcF8AisZVV12luLi4yJ9KlSrprLPO0syZM4/0rgFe48ePV8mSJXXuuece6V054k477TTdcsstR3o3io1SR3oHjmYXXHCBdu7cqTfeeEMZGRlau3atRo4cqY0bNx7pXQOKpUM9Z8qUKaPq1auby3fv3q24uLii2l0gar/X+0J+fr5Kly59pHcDfzBnnXWWBg8eLElas2aNBg4cqK5du2r58uVHeM8A22uvvab+/fvrtdde06pVq1SjRo0jvUsoJvjGO0ZbtmzRDz/8oCeeeEIdO3ZUnTp11LZtW917773q3r27JOmZZ57Rscceq6SkJKWnp+vGG29UdnZ2ZB17f1Xkq6++UpMmTZScnKyzzjpLq1evjrxm9+7duu2221S+fHlVqlRJd911l4IgKLAvI0aMUPv27SOv6dq1qxYtWnR4DgRQSIU5ZyRpw4YNOv/881W2bFk1aNBAn376aWTZ/r9qvvcc+vTTT9W0aVPFx8fr6quv1htvvKFPPvkk8k3J6NGjD/O7xR9RYcZ4XFyc/vOf/5hjXJJmz56ts88+W8nJyapWrZquuOIKbdiwIbI82mv+7t27dfXVV6tx48aRCcsnn3yiE044QQkJCcrIyNCDDz6oXbt2RXLi4uI0aNAgde/eXUlJSXrkkUeK8lABhbL3t0aqV6+u4447Tvfcc49WrFih9evXS5LuvvtuNWzYUGXLllVGRob+9re/KT8/v8A6/v73v6tq1apKSUnRtddeq3vuucf7a7zAocjOztb777+vvn376txzz9WQIUMKLN/7HDNy5Ei1bt1aZcuWVWZmpubPn2+uc9GiRcrIyNBNN910wBxgr4Nd0y0PPvigqlSponLlyumGG27Qzp07I8vy8vJ08803q2rVqkpISFD79u01efLkAvljxoxR27ZtFR8fr7S0NN1zzz2R7V511VUaM2aMnn/++cjz2NKlSw+6T79nTLxjlJycrOTkZH388cfKy8tzvqZEiRJ64YUXNGfOHL3xxhv67rvvdNdddxV4TU5Ojv7xj3/orbfe0vfff6/ly5frjjvuiCx/+umnNWTIEL3++uv68ccftWnTJg0bNqzAOrZv367bbrtNU6ZM0ciRI1WiRAmdf/752rNnT9G/cSBGhTlnpN9uAhdddJFmzpypc845R71799amTZvM1+fk5OiJJ57Qf/7zH82ZM0cvvPCCLrroosg/Yq1evVqZmZlhvCWggKIY41u2bNHpp5+u448/XlOmTNGIESO0du1aXXTRRZH8aK75eXl56tWrl6ZPn64ffvhBtWvX1g8//KArr7xSAwYM0Ny5c/Xvf/9bQ4YMOWBy/cADD+j888/XrFmzdPXVVxfRUQJik52drbffflv169dXpUqVJEkpKSkaMmSI5s6dq+eff16vvvqqnn322UjOO++8o0ceeURPPPGEfvrpJ9WuXVuDBg06Um8BfwD/+9//1LhxYzVq1EiXX365Xn/9dedk+a9//auefvppTZkyRaVKlTKvsTNnzlT79u112WWX6cUXX3T+Vl9hr+n7GzlypObNm6fRo0fr3Xff1dChQ/Xggw9Glt9111366KOP9MYbb2jq1KmqX7++unTpErlfrVy5Uuecc47atGmjGTNmaNCgQXrttdf097//XZL0/PPP6+STT9Z1110XeR5LT08v9LH8XQoQsw8//DCoUKFCkJCQEGRmZgb33ntvMGPGDPP1H3zwQVCpUqXIz4MHDw4kBb/88ksk9tJLLwXVqlWL/JyWlhY8+eSTkZ/z8/ODWrVqBT169DC3s379+kBSMGvWrCAIgmDJkiWBpGDatGkxvEug6BzsnJEUDBw4MPJzdnZ2ICkYPnx4EARBMGrUqEBSsHnz5iAI/u8cmj59eoHt9OnTx3uOAGE51DH+8MMPB2eeeWaBda5YsSKQFMyfP9+5Teua/8MPPwSdOnUK2rdvH2zZsiXy+k6dOgWPPvpogXW89dZbQVpaWoH9vOWWW2I8CsCh69OnT1CyZMkgKSkpSEpKCiQFaWlpwU8//WTmPPXUU0GrVq0iP5944olBv379CrymXbt2QcuWLcPabfzBZWZmBs8991wQBL89s1euXDkYNWpUZPne55hvv/02Evviiy8CSUFubm4QBEFw//33By1btgzGjh0bVKhQIfjHP/5RYBuDBw8OUlNTIz8X5pq+vz59+gQVK1YMtm/fHokNGjQoSE5ODnbv3h1kZ2cHpUuXDt55553I8p07dwY1atSIzEv+8pe/BI0aNQr27NkTec1LL70UWUcQBEGHDh2CAQMG+A7ZHwrfeB+CCy64QKtWrdKnn36qs846S6NHj9YJJ5wQ+bWSb7/9Vp06dVLNmjWVkpKiK664Qhs3blROTk5kHWXLllW9evUiP6elpWndunWSpK1bt2r16tU68cQTI8tLlSql1q1bF9iPhQsX6tJLL1VGRobKlSununXrShL/BwrFzsHOGUlq0aJF5O9JSUkqV65c5JxwKVOmTIEc4Eg61DE+Y8YMjRo1KvLteXJysho3bixJkV8nL+w1/9JLL9X27dv19ddfFyhIOGPGDD300EMFtrH3G4l970/732uAw61jx46aPn26pk+frkmTJqlLly46++yztWzZMknS+++/r3bt2ql69epKTk7WwIEDC5wH8+fPV9u2bQusc/+fgaIyf/58TZo0SZdeeqmk357ZL774Yr322msHvHbf+0BaWpokFXjWWb58uTp37qz77rtPt99+u3e7hb2m769ly5YqW7Zs5OeTTz5Z2dnZWrFihRYtWqT8/Hy1a9cusrx06dJq27at5s2bJ0maN2+eTj755ALfwrdr107Z2dn69ddfvfv8R8XE+xAlJCSoc+fO+tvf/qZx48bpqquu0v3336+lS5eqa9euatGihT766CP99NNPeumllySpwP+f2L9YTVxcnPn/NyzdunXTpk2b9Oqrr2rixImaOHHiAdsBigvrnNnLdU74/ttEYmIiBdVQrBzKGM/Ozla3bt0ik429fxYuXKhTTz1VUuGv+eecc45mzpyp8ePHF4hnZ2frwQcfLLD+WbNmaeHChUpISIi8LikpqegOChCDpKQk1a9fX/Xr11ebNm30n//8R9u3b9err76q8ePHq3fv3jrnnHP0+eefa9q0afrrX//Ksw+OmNdee027du1SjRo1VKpUKZUqVUqDBg3SRx99pK1btxZ47b73gb3PMPs+61SpUkVt27bVu+++q23btnm3W9hrOo48Jt5FrGnTptq+fbt++ukn7dmzR08//bROOukkNWzYUKtWrYpqXampqUpLS4s8VEnSrl279NNPP0V+3rhxo+bPn6+BAweqU6dOatKkiTZv3lxk7wcI295zpiiVKVNGu3fvLtJ1ArGKZoyfcMIJmjNnjurWrRuZcOz9k5SUFNU1v2/fvnr88cfVvXt3jRkzpsA25s+ff8D669evrxIleCxA8RUXF6cSJUooNzdX48aNU506dfTXv/5VrVu3VoMGDSLfhO/VqFGjA4pB7f8zUBR27dqlN998U08//XSBCfCMGTNUo0YNvfvuu1GtLzExUZ9//rkSEhLUpUsXZWVlma+N9Zo+Y8YM5ebmRn6eMGGCkpOTlZ6ernr16qlMmTIFWmHm5+dr8uTJatq0qSSpSZMmGj9+fIEvDMeOHauUlBTVqlVLEs9j+6OdWIw2btyoXr166eqrr1aLFi2UkpKiKVOm6Mknn1SPHj1Uv3595efn65///Ke6deumsWPH6l//+lfU2xkwYIAef/xxNWjQQI0bN9YzzzwTqegsSRUqVFClSpX0yiuvKC0tTcuXL9c999xThO8UKBoHO2eKUt26dfXVV19p/vz5qlSpklJTU2mFhNAVxRjv16+fXn31VV166aW66667VLFiRf3yyy9677339J///Cfqa37//v21e/dude3aVcOHD1f79u113333qWvXrqpdu7YuvPBClShRQjNmzNDs2bMjRXGA4iAvL09r1qyRJG3evFkvvvhi5LdCtm3bpuXLl+u9995TmzZt9MUXXxxQfLZ///667rrr1Lp1a2VmZur999/XzJkzlZGRcSTeDn7HPv/8c23evFnXXHNNgf/aI/32X5Bee+013XDDDVGtMykpSV988YXOPvtsnX322RoxYoSSk5MPeF2s1/SdO3fqmmuu0cCBA7V06VLdf//9uummm1SiRAklJSWpb9++uvPOO1WxYkXVrl1bTz75pHJycnTNNddIkm688UY999xz6t+/v2666SbNnz9f999/v2677bbIhL9u3bqaOHGili5dquTkZFWsWPGP/Q+8R/o/mR+tduzYEdxzzz3BCSecEKSmpgZly5YNGjVqFAwcODDIyckJgiAInnnmmSAtLS1ITEwMunTpErz55psHFIbatzhCEATBsGHDgn0/lvz8/GDAgAFBuXLlgvLlywe33XZbcOWVVxYoHPXNN98ETZo0CeLj44MWLVoEo0ePDiQFw4YNC4KA4mooHgpzzuw7bvdKTU0NBg8eHASBu7ja/udQEATBunXrgs6dOwfJycmBpAKFTYCwFMUYD4IgWLBgQXD++ecH5cuXDxITE4PGjRsHt9xyS6SATSzX/KeffjpISUkJxo4dGwRBEIwYMSLIzMwMEhMTg3LlygVt27YNXnnllcjrXfsJHE59+vQJJEX+pKSkBG3atAk+/PDDyGvuvPPOoFKlSkFycnJw8cUXB88+++wB94SHHnooqFy5cpCcnBxcffXVwc033xycdNJJh/nd4Peua9euwTnnnONcNnHixEBSMGPGjAOeY4IgCKZNmxZICpYsWRIEwf8VV9srKysryMzMDE499dQgOzvb+exzsGv6/vYWob3vvvsi59B1110X7NixI/Ka3NzcoH///kHlypWD+Pj4oF27dsGkSZMKrGf06NFBmzZtgjJlygTVq1cP7r777iA/Pz+yfP78+cFJJ50UJCYmFniPf1RxQRDlfygGAAAAjkKdO3dW9erV9dZbbx3pXQHwB8OvmgMAAOB3JycnR//617/UpUsXlSxZUu+++66+/fZbffPNN0d61wD8AfGNNwAAAH53cnNz1a1bN02bNk07duxQo0aNNHDgQPXs2fNI7xqAPyAm3gAAAAAAhOgPXFYOAAAAAIDwMfEGAAAAACBETLwBAAAAAAgRE28AAAAAAEJU6HZicXFxYe4HcEQcSm3BojwnSpSw/w1sz549RZbjU6ZMGWe8du3aZk6zZs2c8YkTJ5o5a9asiW7HilidOnXMZU2bNnXGR4wYYeYUZX3Kov5MY1FczgmgOIn1vOCcwO8R9wngQIU5L/jGGwAAAACAEDHxBgAAAAAgREy8AQAAAAAIERNvAAAAAABCxMQbAAAAAIAQxQWFLE1IFUL8HhWXypy+dVnLYqly/e9//9tcFh8f74zn5eWZOdWqVXPGU1JSzBzrmFtV1SVp2rRpznhiYqKZk5+f74xbldglKSsryxlfvHixmVO+fHln/NNPPzVzPvroI3OZxap4XtTVzovLOQEUJ1Q1B/4P9wngQFQ1BwAAAADgCGPiDQAAAABAiJh4AwAAAAAQIibeAAAAAACEiIk3AAAAAAAhYuINAAAAAECIaCcWMuu4Wa2BJLs9UCztG2L53A6lTUQ0MjMzzWXjxo1zxhs1amTmLFiwwBn3vZ/i0hIjlvHg89hjjznj9erVM3NWrVrljPvafO3evdsZT01NNXPS0tKc8aFDh5o5//rXv5zx8ePHmzlr1651xrdv327mbNiwwRkvWbKkmWN9dhUrVjRzJkyY4Iw/++yzZo61D9ZnEKvick4AxQntxID/w30COBDtxAAAAAAAOMKYeAMAAAAAECIm3gAAAAAAhIiJNwAAAAAAIWLiDQAAAABAiEod6R3AgYqyqvjhqlB+2mmnmcuOPfZYZ7xBgwZmzqOPPuqM+6phnnnmmc54Xl6emVNcxFLVPCMjw8xp3ry5M758+XIzJz4+3hn3jSFr31auXBn1durUqWPm9OrVyxnPyckxc9avX++MZ2VlmTlW5XBfZXmrqrhVJV6yPx9f9XRrO7HkADg8rHvW4bo3Hy6+e7P1XmPJ8V3vYukIE8vnc7hygH3Fcr74pKSkmMvat2/vjA8fPjzq7fj22zqfd+3aFfV2YnEkOj/xjTcAAAAAACFi4g0AAAAAQIiYeAMAAAAAECIm3gAAAAAAhIiJNwAAAAAAIaKqOQAAQAhiqYBrdT3wVfpNTk52xqdMmRL19mMRy/uMJaeoOzUcrv2mejkOla/7jXVe1K9f38y59tprzWW5ubnO+Pbt282cHTt2OOOTJk0yc2KpXm5VIvcdHysnlu37OisUBhPv/RR1uX4rp6hvHldeeaUzPmHCBDPnlFNOccZvvvlmM8dqkdSiRQszZ+HChc741KlTzZxbbrnFGZ8+fbqZczSL5eTv1KmTucxqq5KUlGTmWBfNUqWiv0xYD4GStHr1ame8cuXKZk63bt2c8WnTppk5VquMxMREM8c6bvn5+WaOdbH3XUvKlCnjjFvnpCSNHj066u0AAACgeOBXzQEAAAAACBETbwAAAAAAQsTEGwAAAACAEDHxBgAAAAAgREy8AQAAAAAIEVXNi6HGjRs7477q0qeddpoz3rp1azOnQoUKzviQIUPMnO+//94Z91Uob9WqlTPepk0bM2fnzp3OuK81wi+//GIu+z1q2rSpucyqdO2ram4d81gq/VvVwSWpdOnSznheXp6ZY7WwsKqD+9ZnbV+yuw1YFd8lKTU11RlPSEgwc6zjZrURkuyq5rFUxAdweJQtW9YZv+iii8yc7t27O+MzZ840c6xrrq9TwooVK5zx8uXLmzlWtwjf/dfqWLFhwwYzx+LbN+ua77sfWa2BfF02tmzZEtW6fPvmY90nfPcwa1l8fLyZY73XwYMHe/YOh5tvfFnPLqeffrqZc8YZZ5jLfv31V2fcN46sa13nzp3NnP/85z/O+Nq1a82couwW5evAY103cnJyot7OvvjGGwAAAACAEDHxBgAAAAAgREy8AQAAAAAIERNvAAAAAABCxMQbAAAAAIAQUdUcAAAgBN26dXPGjzvuODNn4MCBzrivQvlZZ53ljPs6MkyfPt0ZP+aYY8yc/Px8Z/ykk04yc6zq5dWrVzdzKlWq5Izn5uaaOevXr3fGGzVqZOZs2rQpqnVJUosWLaLeN6sSuq/a+amnnuqMW8dGsj/TefPmmTlWZecGDRqYOTj8rM4zPr7uQXXr1jWXWRXUS5Swv6/96quvnPHjjz/ezHnyySed8SlTppg5s2bNcsZ9Y7xt27bOuO/4jBs3zhkfP368mVMYTLz3Y5Wpj5VVXj8zM9PMWbNmjTO+bds2M+e1115zxm+99VYzZ9WqVc74s88+a+ZUrVrVGfcdt/nz5zvjVpsxyW4/4HuI+KO1E6tXr565zGox5WtDkpiY6Iz7jrn1EOZr62C1J/O1yrC242snZu2Dr/2WtczXQsNqOWEdT8k+BlWqVDFzAAAAcPTiV80BAAAAAAgRE28AAAAAAELExBsAAAAAgBAx8QYAAAAAIERMvAEAAAAACBFVzffjq6xsVS/2VfS22jT4KkU3b97cGT/ttNPMnD//+c/OuNViRLJL//usW7cu6hyrErrVxkOSatas6YxfffXVZs7YsWOd8dmzZ3v2rvizKpFnZ2ebOSkpKc64VR1cso/5ihUrzBxrHPtaTvjOMYuvqrjFqnhuncexsvatYsWKZo51TDMyMopknwAUDytXrnTGfd0VWrdu7Yz7Wt9s3bo1qrgkdejQwRkfM2aMmVOjRg1n/IorrjBzRowY4Yz72hlZ1+n33nvPzLGeNZKSkswcqzWXrytFkyZNnHFfm6GNGzc64w0bNjRzKlSo4Iz77uNW9xvr2EhS+/btnfHBgwebOQiP1fXEN9ewOgFZ1xJJysrKMpdZ54xvvFrLJk+ebOZY3YisuZMknXzyyc54z549zRzrnPHt27XXXuuM+1oAFgbfeAMAAAAAECIm3gAAAAAAhIiJNwAAAAAAIWLiDQAAAABAiJh4AwAAAAAQIqqaAwAQJavyrI9Vlbaou2mUKmXf2n3VtKPl615Q1N0DLFbXB9/79B27ota4cWNnvFatWmZO7dq1nXFfh4569eo5477K4S1atHDGR40aZeakpaU544sWLTJzKleu7Ixv377dzFm2bJm5zLJz505n3NeZw6pQ7vt8ypYtG92OSVq7dq0z3q1bt6hzfOOgfv36zrivunW5cuWccV9ldxROLPeJWDz88MPOuHW+How1xn3XVev8s6rmS/a49N0/pk6d6oxbFdIle7/79etn5lhdZi688EIzpzCYeO/H92HHcrPOzc11xn0PLKeffroz/vbbb5s5N9xwQ3Q7dhhZ7Tqsi70kTZkyxRn3lfG32jpZ2z9aWBdO383fGqu+Fg1W+6v58+ebOdY4jqWdmO/cs3J852QsNzxrH3zj7oQTTnDGfQ+V1mShfPny9s4BAADgqMWvmgMAAAAAECIm3gAAAAAAhIiJNwAAAAAAIWLiDQAAAABAiJh4AwAAAAAQIqqa76eo24xkZWU5499//72Z41tmsdo+7Nixw8yJ5b1alaJ967Kqcm/atMnMsY7b8OHDzZwaNWo443Xq1DFzjgZW1WyrMrZkf06+9iBW5XBf+whrH3wVymNpjxQLa32+fbOO2+7du80c6/ikpqaaOWvWrHHGN27caOZYbYGWLl1q5iA8RTlefRX4Y9lOUbYMk6S+ffs64wMHDjRzatasWaT7YMnPzz8s24mVdU5XqVLFzLGuD1bLMMnuJOHbjtWuymqjI0k9evRwxn/66Sczx2rNNXPmTDPH6u5yzDHHmDlWm602bdqYOePGjXPGO3ToYOZs2bLFGbfu1ZJ9D/F1ALGu+b7P1LrHW/vs2wffMwYK53C1Lty8ebMz7msnZnVdkuwuQb5WlVbXHN88xBqvvue0U045xRnPzMw0c6wxXrVqVTNnxIgR5rJDwTfeAAAAAACEiIk3AAAAAAAhYuINAAAAAECImHgDAAAAABAiJt4AAAAAAISIquYAABSRWCqUF3UV8ksvvdRcdvzxxzvjvXr1MnOs6rcbNmwwc959992o9y0WZcqUccbvuusuM+fvf/97ke6Dj1Xpd8mSJWbOjz/+6IyfddZZZo5VHfjnn382c7Zt2+aMV69e3cx5/vnnnfGOHTuaOVYV7k6dOpk51jGw4pJdSf/LL780c1q0aOGMN2nSxMx57733nHFfFWSrQrmvsvtJJ53kjFesWNHMscydO9dcZo0Rq+o9ip+yZcs6476q+b5lOTk5zvjWrVvNHKuDgzX2Jfue6LuPWvttHQPJ7irgq56enp5uLjsUTLyPEKt1k2QPBN9JYvHl+FokFSXrppudnW3mWCed77hZDzhF/VB7uFWrVs0Z912Y8vLynHHfA5X1EOZrKWK18vF9TtZ++8aqdXH2jWErx9d+yNo33zGwjrWvJc+CBQui2r4kHXfccc447cQAAACKP37VHAAAAACAEDHxBgAAAAAgREy8AQAAAAAIERNvAAAAAABCxMQbAAAAAIAQUdX8CImlorgvx2r34qsubYmlHY5PUlKSM96nTx8z5/PPP3fG//vf/5o5VpV0qy3C0aJevXrOuK/S9o4dO5zxSpUqmTlWpW1fuwXfPlis6uW+sWWNY99Ytfjej7UdXwV+K8d37lnv1VfZvVGjRuYyhCOWa2Es18j69euby6w2X5mZmWbOmWeeaS5btGiRM/7rr7+aOVbHA1+bmHPOOcdcVpQuueQSZ/zEE088LNs/GKsrxaZNm8wcq4NBuXLlzByrW4Mvx9q3li1bmjkjR450xn3dQ6xr1+23327mWPftyy+/3MypVauWMz548GAzZ8yYMc64rz3a/PnznXGrpZskXXjhhc54+fLlzZyFCxc64/Hx8WaO1VLNt29Wq7GUlBQzB4UTSxcX61nf6twjSTVq1HDGra4rB1tmjbGdO3eaOdY56xvjVgsyX2swq4VkVlaWmZOamuqM+9r5Wce7devWZk5h8I03AAAAAAAhYuINAAAAAECImHgDAAAAABAiJt4AAAAAAISIiTcAAAAAACGiqjkAoFiyKr/6KtNbFU991VgtsVQo91VwfeSRR5zxiy++2MyxKsWuXr3azJk0aZK5zOpE4Kt6/PPPPzvjVhVpSXr44YfNZZaqVas6477j88wzzzjjjRs3NnNatWrljP/000+evYuNtc7zzjvPzPnll1+ccd9n3qFDB2e8SpUqZs7zzz/vjFvVziXprrvucsZ9FZLvvPNOZ3zt2rVmzoABA5xxX2cOq7L7ySefbOZ8+umnzvg///lPM+e0005zxqtXr27mzJgxwxm3KqRLUteuXZ3x2rVrmzmzZ892xn0dSKwq9uPHjzdzUDjWPcTX9cSqau67Dlpjb/369WaO75pv3WOtLkWSlJ6e7oz77r1W9XTrXJakUqXcU1ff+7GuGy+99JKZY3WXsLZfWL/riXdRt8U6GvlakMXSaiyWNmgbNmxwxqdNm2bmWOX6//3vf5s5VtutcePGefau+EtLS3PGExISzJwtW7Y4474WDVYLMt9FJpbzyNdGw2Kdy74WNrGwHh6tyZwkbd682Rn3PehYx8B3U7PGAQAAAIo/ftUcAAAAAIAQMfEGAAAAACBETLwBAAAAAAgRE28AAAAAAELExBsAAAAAgBD9rqua/1Eql8cqlgrlFqvsvmS30XjvvffMHKuNRpcuXcwcq/L0ihUrzJyjgdUGwVc12+KrAp6bmxv1+qzq3L7K5UVZ1dx3DKx2GFb1dskeQ75rSXZ2trnMYu13uXLlzJwaNWpEvZ2jga/7hG+ZJZa2YZZOnTqZyy644AJn/LLLLjNzNm7c6IzPnTvXzLHOWd9Y8bVcss5zq22ZZHeZWLNmjZljHQertZRv32bNmmXmWO1ofF0fsrKyzGVFbfv27c742WefbebMmTPHGX/33XfNHOszr1ixoplj3Rt9Y9gad74WVxMnTnTGFy1aZOa89dZbznjPnj3NHOveMnXqVDMnIyPDGbfGlSRVqFDBGfe1ObQ+H193F+uzs7YvScOHD3fGr7rqKjPHasMUy/UXBVldYWK5T1mt4iS7I4vvGSmWlmZWy0fJfray7nuSvX++67fV/cXqMCNJv/76qzPuu9Y99dRTzviECRPMnMLgG28AAAAAAELExBsAAAAAgBAx8QYAAAAAIERMvAEAAAAACBETbwAAAAAAQvS7rmoOACjefBXji7Lzws0332wuu+GGG5zxatWqmTlWlVRfBW7r/fi2Y/FVUPYdU6vys29969evd8Z9ldUt48aNM5edf/75Ua9v4MCBzviNN95o5ixfvtwZv/zyy6Pe/sE0atTIGfdV2rbGSdOmTc2cH374wRm3qipLUrt27ZzxmTNnmjnbtm1zxps0aWLmWMe7d+/eZo513D7//HMzx6p23L59ezMnPz/fGZ8+fbqZY1Xft84Tye4acO6555o5CxYscMafe+45M6dhw4bOuG8cWOd+enq6mXO4+SqsW9W5fR1UrPVZ40HyXyMtvk4y0fryyy/NZVb3BF+3GquLi2TfQ3xj3PocfBXKfcc72hzf52PtW4sWLcycrVu3RrdjhcTE+3culnYBPnfffbcz7mtZMmjQIGf8iiuuMHOs9gO+C0+dOnWc8aJsL3QkxNLqw7rQVa5c2cyxLty+MRQL6+Lou0la++BrDRbtuiR733wtOawHKt+4sz4f340wljZsAAAAKB54kgMAAAAAIERMvAEAAAAACBETbwAAAAAAQsTEGwAAAACAEDHxBgAAAAAgRFQ1/53zVS6vW7euM/7AAw+YOVZFaF+LgQsvvNAZX7hwoZljtb6oUaOGmRNLW4LiIj4+PuocX4uGKlWqOOO+Filbtmxxxn2tjvLy8pzxWNo6+Kp2W5+tr0WKxddew1qf7/NZu3atM25ViZfs6vK+SvXWueyruF6czokTTjjBGe/cubOZY7UU8o196xqRnJxs5lhjf+XKlWZOampq1PtmLfO1/7Kq5vs+91jGUSzthnznktVxoG3btmbOqlWrnHHfZ2e1dfPdX8qWLeuMX3fddWZOrKz9sLpVSNKaNWuc8fnz55s5VpeQuXPnmjnz5s1zxq0WbZI0fvx4Z7x69epmzjnnnOOMW/cpSapdu7Yz7hsL1pi77LLLzJxPP/3UGffd96w2W1lZWWZOWlpaVNuX7PuOr+3exIkTnfGffvrJzOnRo4czbrUzC5P1fOB7li3Kll1F7dRTT3XGL7jgAjPHavNn3QskuxOQr1OK75pvHW/fPlifne/5qSjviT7WccjOzjZzevbs6Yx/9tlnUW9/X3zjDQAAAABAiJh4AwAAAAAQIibeAAAAAACEiIk3AAAAAAAhYuINAAAAAECImHgDAAAAABCiYtdOLJZWAkcj631KdisYX1sAq7x+48aNzZynnnrKGfe1YbHaaNx+++1mjq8tgOW4445zxjMyMswcq83J0aBChQpR5/jab6WkpDjjvjZfsbTmss5L32dujW9fC6RYWPvmO27WOWa1TZOkpKQkZ9zXTqxhw4bOuK/dm7VvVatWNXN87bDCcNNNN5nLrNYcvrZK1pjYuXOnmWO12fK1ILG242tdZJ1Lvs/dalsWSysvX9sy37lktXbx3ZOsz8i3D9bnsG3bNjPHagu0efPmqHN848q6PobB+mx/+OEHM8f6jDp27GjmtGrVyhm3WrRJdvutxYsXmzlWiz8f637w3XffmTnWZ+RrQWZdp2fPnm3mTJo0yRn3jR/r8/G1TbLOrxUrVpg5DRo0cMZ97cSs4zN06FAzx2qP5DvWYSnKZ/2KFSuay6y2k9Yx9+VY9zbJvtf7nimsZxTfPaxSpUrOeCznvxTb84Z1X7baN0rSuHHjnHHfvddq0eZ7vt26dasz7mu5etJJJ5nLDgXfeAMAAAAAECIm3gAAAAAAhIiJNwAAAAAAIWLiDQAAAABAiJh4AwAAAAAQomJX1TyWioaxVEOOpdJ2UfK9T6v6pa+iYc2aNZ1xX7Vxq6Kor5Jfr169zGVFyfp8fJV3fcenuCtfvry5zKqU6qvObVXaXrZsmZljVSi2qgZL9ufhqy5pfba+9xNLjm8fot2Or1qtVZ10zpw5Zk7t2rWdcV+1butYW5/1kfDWW2+ZyyZPnuyMZ2ZmmjnNmzd3xuvUqWPmWNWQfZ0DrMrTvuu0NfZ8lYCtZb6xan3uvi4XvirpvnPGkp2d7Yz7KrhbY9l3PbHeUyzVd337Zp2zX3zxhZlz1113mct8rErIvuru1jltVcWX7Mrdvu1cccUVzni1atXMnI0bNzrjubm5Zo51jvvGwsSJE51xX9cVqxL5P//5TzPHqgZvVYmW7O4TvnO/bt26zvjpp59u5gwfPtwZ/+mnn8wc61nC9+xkVVYv6k4jhWE9fz788MNmjnXcfc9V1rXdd5ys8883jrOyspxx373eOu6+c8yqDn7RRReZOVOmTDGXWfdRXzV2a4z7HHvssVFtX7LHq28OYF0bfNXTfc8Zh4JvvAEAAAAACBETbwAAAAAAQsTEGwAAAACAEDHxBgAAAAAgREy8AQAAAAAIERNvAAAAAABCVOzaicXiSLcG87HaAvj2OZaWag888IAzvmrVKjOnZcuWzvjFF18c9faLmnUMKleubOb42jMUd77WQPn5+c64r42U1f5qxIgRZo41HqztS7G1JrJaHVntzCT7s/W1TYql1Zm1D75jYB1rX9sbqy2fr7WFdQzKli1r5hxuvvYzVrsjq22Qj6+92zHHHOOM169f38yx2qBYraAke6z4joF1vvjG5IYNG5xxq8WXZLd8kuyWOL5WVdYyX3ubWNo7WtfBWNoaWcdNsluNhfEsYbXzslqASlJaWpoz7mv/Y93r69WrZ+asXr3aGV+6dKmZY50rvjZDo0ePdsZ997358+c74xUrVjRzNm3a5Iz72qOVLl3aGfedQ1abIV/O2rVrnXFfy6t27do549axkaQvv/zSGW/UqJGZY7VOs8bHofK17HrhhRecceuckOznRd+zdFFen3zb8V0jLampqc64r73V448/HvX2+/btay6zrie+1o4jR450xhcvXmzmNGjQwBn3tfOznoWsc1my772+Z7v169ebyw4F33gDAAAAABAiJt4AAAAAAISIiTcAAAAAACFi4g0AAAAAQIiYeAMAAAAAEKJiV9U8lirgVlVIXyVLq0KiVX0zVkVZJfXBBx80l+3atcsZb9GihZlz/vnnH/I+7eWrLm2x9tm3Pl9V86OZ71hYfJV+rfX5Kr9bx9yqFCvFVqXZ2o5VaViyq4b6qlv7llmsSsi+8zg9Pd0Z//HHH82crVu3OuO+qpxWFWurAuqR4KuMbVXh91WrjaWatTVefdf2WKrZW3wVe61x5OsOYO2bbzu+atGxdBWwqu1XqVLFzClXrpwz7hvj1vH23V+sqv5ZWVlRb2fZsmVmTqysa6HveJ988snOuFUBWLLHkK9q9rBhw5xxX1XzzMxMZ9zqWiBJs2bNcsZ91+jrrrvOGffdw6yq4r4OIF999ZUz7qsgf/fddzvjzZs3N3NeeeUVZ3zGjBlmzr333uuM+7otWOddrVq1zByrA0dY95Yrr7zSXGZV7l60aJGZY12ffF1CfNXxLda1y3ecVqxY4Yz7Og5Z1zSrMr4kvfHGG874eeedZ+Z89tln5jKre4HvmLZq1coZ79ixo5ljXbd857l13fDd9yy+ivTW52098xUW33gDAAAAABAiJt4AAAAAAISIiTcAAAAAACFi4g0AAAAAQIiYeAMAAAAAECIm3gAAAAAAhKjYtROLpf1W06ZNnXFfyfdt27Y541YZf0nKycmJbsdiVLNmTWfcauMh2a1JTjnllCLZp4PxfW6+1lLRrq927dpRr+to4Bt3VuubHTt2mDlWuwVfjtWKoXr16maO1bopMTHRzKlUqZIzvm7dOjOnQoUKzriv3ZPVTsjavmSPr1jaZPnOCeuYWm13JPuz8x3r4sRqF+drIxcL63j42lhZLUV8rVOsc8y3HYuvNZjVbiWWFoQH25bFOpd8LXGsVnC+1mDWsYul9aQvx7qX+95PrKwWQLm5uWbOvHnznHFf+y2rbdiXX35p5lgt9o4//ngzZ8KECc64r92TdX/zvR+rpZmvTax1LfZtx2qJ52sNZrVOs9qZSfY139f2bvHixc647xy22on5nsOsVpVWe81D5bvXW+23UlJSzJy8vLyo1iXZ13ZfSyrr2PparlotCn33Fuva4Ht+s653VstAyf+8YbUT87Vhs1qA+Z6frGc43/XbGsu+e6+V42tZao2Fhg0bmjmFwTfeAAAAAACEiIk3AAAAAAAhYuINAAAAAECImHgDAAAAABAiJt4AAAAAAISo0FXNrcpvsVQhL+rtjBs3rkj34Uh75ZVXnHFfJb1zzz03rN0pFKsqsOSvGhjt+ho3bhz1uo4GvkqaVhXu1NRUM8c6fr7KoNY55qsIa1We9FUbtypPWtVlJftztyrsSnblVKtCumRXkI7luK1Zs8bMWb16tTP+888/mzkNGjRwxn1j54/IqgjrqyJt2bx586HuDv7gGjVq5IxfcsklZo5VXd1XAXv9+vXO+GWXXWbm1KtXzxn3VTs+5phjnPFatWqZOV9//bUz7quebt33rArcPr5rfv369Z1xX4Vyq+K5b9+s9R133HFmTosWLZxxqyuPZFd29z2jWfeWk08+2cw5FCtXrjSXWffTX3/91cyx3nPlypXNHKvStq+Su3WO+To1xNL9wupS5HsOsZ5dfO+nSZMm5jKr24ivUrx1v/Q9Q1r753uGjOW50+p24uvas3XrVmfcd84WBt94AwAAAAAQIibeAAAAAACEiIk3AAAAAAAhYuINAAAAAECImHgDAAAAABAiJt4AAAAAAISo0O3EirptWFFux2pX9eWXX5o5NWvWdMYfe+wxM+fdd9+Nbsc87rvvPnPZWWed5Yw///zzZs7s2bMPeZ+KE6s9g68tyNEsOTk5pmUWq1XFiSeeaOZYrTLS09PNnJ07dzrjVnsPyW5rUrJkSTPHajnha99iHTdf649NmzY5482aNTNzrLYknTt3NnOs9hq+8Z2Xl+eMV6tWzcwBcGRZLcCsFluS3U7IamMl2deHiRMnRp1TtmxZM8dqaWS1+JGkVq1aOePWtVPy30Ms1v1gzpw5Zo51P0hLS4t6+75rcd26dZ1x331v+fLlznjFihXNHOszXbp0qZljLfO1tzwU06dPN5cNHTrUGb/66qvNHKv93uLFi82cHTt2OOO+5y3rucpqVSXZ7T59n7v1Gfpawllzp5ycHDPHamvqW59vH6xzyTrWkn28rWdLyb5u+K4nVqsx33XLap24du1aM6cw+MYbAAAAAIAQMfEGAAAAACBETLwBAAAAAAgRE28AAAAAAELExBsAAAAAgBAVuqr5aaed5oz7Ks9t27bNGd+8ebOZs337dmfcqvIn2RXzfJX06tWr54zffvvtZs7IkSOd8XXr1pk5Z555pjN+8803mzljxoxxxu+55x4zpziLpVJ9iRLufxPyfaZHsypVqpjLfvnlF2c8NTXVzLEqz65Zs8bMsSrp+s49q5qnr1Kk1YXA2r5kV6v1Vdi0KpD69m3r1q3OuK/SqXV8rGqmkn2da9y4sZlj7ffh6jgBIHrlypVzxq0uEpJdHbhTp05mzrRp05zxSZMmmTlWt4j27dubOdZzna8SutWtYdiwYWaOVQm9du3aZs6ePXuccavqtWS/H992rGu+dc+R7IrLvnvL/PnznXHrPiXZXXGs51fJvldZVZ3DZHUW8lVCv+OOO5xxq5K8ZI99X2Vs677tq1BuHVtfdxVrfdazk2Q/B/jGpG+Ztd++HN/+RZvjqxxunTO+av/WtaF69epmzsyZM53xt99+28x56623zGV78Y03AAAAAAAhYuINAAAAAECImHgDAAAAABAiJt4AAAAAAISIiTcAAAAAACFi4g0AAAAAQIgK3U7MKsvvK9dvtUiy2mtIUn5+vjO+adMmM8cqE79ixQoz55133nHGrfLxkt3KIzMz08xp0aKFMz527Fgzx2pp5mvdFh8f74z7WkEVZzk5Oc74119/fZj35PDwtZ6ylvnGg9Way9d6yjrm1tiSira9W/ny5c1lS5YsiXp9VpsK631KdhsPX8tA65haLdAkKSsryxn3tUezzmVfezQAR9acOXOccV/7Les68OGHH5o51rWradOmZs7q1audcV/bSesZqWvXrmaO1TqtWrVqZo7V5mvWrFlmjvWc6GuBZLXEXLlypZljHTff+7E+U1/7qlq1ajnjvvvRvHnznPGaNWuaOVbbsP/9739mzqGw2sVK9vP88OHDzRxrWceOHc0cq21ZnTp1zByrhavv/Vjnpa+dmO85wGKNCd8zn2+MW88bvucaX1s1i7V/1lxQsp/hfJ/DN99844xb54skjRs3zlx2KPjGGwAAAACAEDHxBgAAAAAgREy8AQAAAAAIERNvAAAAAABCxMQbAAAAAIAQxQW+knf7vtCoEFzUKlWq5Ixb1R0lqWLFilHnWO/HV9GwSZMmznhKSoqZ8+OPPzrj//3vf80cXzX2PwqrWv7UqVPNHGsc+BRy+DsV5Tlx8cUXm8vuuOMOZ3zp0qVmTv369Z3xX375xcxJTk52xrdv327mWBW1fVU5k5KSnPFYKtz6qu/GwqpwW6FCBTPHGkPLli0zc9q2beuMW+9TkjZs2OCMP/fcc2bOmDFjzGWW4nJOAMVJrOcF5wR+j/6I94nGjRs745UrVzZzrKr1vvmJ9Wznq/S9aNEicxkOn8KcF3zjDQAAAABAiJh4AwAAAAAQIibeAAAAAACEiIk3AAAAAAAhYuINAAAAAECImHgDAAAAABCiUkd6B/a3cePGqOL4fbLaKbz00kuHd0cOkzlz5pjLcnJynPEWLVqYOX/961+dcav9l2S38rPaWEl2+60GDRqYOd27d3fGfe3R9uzZ44w3bNjQzNm0aZMzXrp0aTPn66+/dsZLlLD/jTI1NdUZ9x03K6dVq1ZmjtWWZOzYsWYOAAA4dD///HORrWv27NlFti4cXfjGGwAAAACAEDHxBgAAAAAgREy8AQAAAAAIERNvAAAAAABCxMQbAAAAAIAQxQVBEBTqhXFxYe8LcNgVcvg7Ha5z4uyzz3bG27dvb+Y8+OCDzvjOnTuLZJ9waKyq5s8//7yZ8+OPPzrj//nPf4pkn/Y6Gs4J4HCL9bzgnMDvEfcJ4ECFOS/4xhsAAAAAgBAx8QYAAAAAIERMvAEAAAAACBETbwAAAAAAQsTEGwAAAACAEDHxBgAAAAAgRIVuJwYAAAAAAKLHN94AAAAAAISIiTcAAAAAACFi4g0AAAAAQIiYeAMAAAAAECIm3gAAAAAAhIiJNwAAAAAAIWLiDQAAAABAiJh4AwAAAAAQIibeAAAAAACE6P8BbuSO1Cr6dVUAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "RDPiii6aIDO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Question 2 (10 Marks)\n",
        "## Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes. Your code should be flexible such that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
      ],
      "metadata": {
        "id": "AJgpSSS-_woL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing code**"
      ],
      "metadata": {
        "id": "UUTKPOYqAz3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "h2o0HqEZIERN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bAOo8LX0relx"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This code defines classes for one-hot encoding labels, normalizing input features, and splitting data into training and validation sets,\n",
        "which are essential steps in preprocessing for machine learning tasks. Each class handles a specific transformation to prepare data for\n",
        "model training and evaluation.\n",
        "'''\n",
        "import numpy  as np\n",
        "\n",
        "class OneHotEncoder:\n",
        "\n",
        "    def __init__(self,x, y):\n",
        "        self.y = y\n",
        "        self.x = x\n",
        "        #self.num_class = num_class\n",
        "        self.onehot_encode()\n",
        "\n",
        "    def onehot_encode(self):\n",
        "        onehot = np.zeros((self.x.shape[0], 10))\n",
        "\n",
        "        for i, j in zip(range(len(self.x)), self.y):\n",
        "            onehot[i, j] = 1\n",
        "        return onehot.T\n",
        "\n",
        "class Normalize:\n",
        "\n",
        "    def __init__(self, unprocessed_X):\n",
        "        self.unprocessed_X = unprocessed_X\n",
        "        self.Norm_reshape()\n",
        "\n",
        "    def Norm_reshape(self):\n",
        "        X_norm = np.reshape(self.unprocessed_X,(self.unprocessed_X.shape[0],784)).T/255\n",
        "        X_norm= np.array(X_norm)\n",
        "\n",
        "        return X_norm\n",
        "\n",
        "\n",
        "class TrainValSplit:\n",
        "  def __init__(self, X_train, y_train, Val_split_ratio = 0.9):\n",
        "    self.X = X_train\n",
        "    self.y = y_train\n",
        "    self.vsr = Val_split_ratio\n",
        "\n",
        "  def Apply_split(self):\n",
        "    np.random.seed(0)\n",
        "\n",
        "    i = np.random.permutation(len(self.X))\n",
        "    split = int(self.X.shape[0] * (1 - self.vsr))\n",
        "\n",
        "    train = i[:split]\n",
        "    val = i[split:]\n",
        "\n",
        "    train_X = self.X[train]\n",
        "    val_X = self.X[val]\n",
        "\n",
        "    train_y = self.y[train]\n",
        "    val_y = self.y[val]\n",
        "\n",
        "    return train_X , train_y, val_X, val_y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Activation functions and their gradient calculation code**"
      ],
      "metadata": {
        "id": "4img7vaRBMXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CalActivation:\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "\n",
        "# Sigmoid (for hidden layers)\n",
        "class sigmoid(CalActivation):\n",
        "    def use_sigmoid(self):\n",
        "        return 1 / (1 + np.exp(-np.clip(self.m, -250, 250)))  # Clip inputs\n",
        "\n",
        "    def sigmoid_d(self):\n",
        "        sig = self.use_sigmoid()\n",
        "        return sig * (1 - sig)\n",
        "\n",
        "\n",
        "class relu(CalActivation):\n",
        "    def use_relu(self):\n",
        "        return np.maximum(0, self.m)\n",
        "\n",
        "    def relu_d(self):\n",
        "        return np.where(self.m > 0, 1, 0)\n",
        "\n",
        "class Leakyrelu(CalActivation):\n",
        "    def use_relu(self):\n",
        "        return np.maximum(0, self.m)\n",
        "\n",
        "    def relu_d(self):\n",
        "        return np.where(self.m > 0, 1, 0.01)\n",
        "\n",
        "\n",
        "class softmax(CalActivation):\n",
        "    def use_softmax(self):\n",
        "        shifted = self.m - np.max(self.m, axis=0, keepdims=True)\n",
        "        exps = np.exp(shifted)\n",
        "        return exps / np.sum(exps, axis=0, keepdims=True)\n",
        "\n",
        "    def softmax_d(self):\n",
        "        z=self.m - np.max(self.m,axis=0)\n",
        "        soft=np.exp(z)/np.sum(np.exp(z),axis=0)\n",
        "        return soft*(1-soft)\n",
        "\n",
        "class tanh(CalActivation):\n",
        "    def use_tanh(self):\n",
        "        return np.tanh(self.m)  # Built-in is stable\n",
        "\n",
        "    def tanh_d(self):\n",
        "        return 1 - (self.use_tanh() ** 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# This function call different activation function\n",
        "class apply_activation(CalActivation):\n",
        "\n",
        "    def __init__(self, activation_function, m):\n",
        "        super().__init__(m)\n",
        "        self.activation_function = activation_function.lower()\n",
        "\n",
        "    def do_activation(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.m).use_sigmoid()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.m).use_relu()\n",
        "        elif self.activation_function == 'lrelu':\n",
        "            return Leakyrelu(self.m).use_relu()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.m).use_tanh()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.m).use_softmax()\n",
        "\n",
        "    def do_activation_derivative(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.m).sigmoid_d()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.m).relu_d()\n",
        "        elif self.activation_function == 'lrelu':\n",
        "            return Leakyrelu(self.m).relu_d()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.m).tanh_d()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.m).softmax_d()\n",
        "        else:\n",
        "           raise ValueError(\"Unknown activation function\")\n"
      ],
      "metadata": {
        "id": "_jniGnSv6P9J"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initilization of weights and biases code using Xavier / He / Random**"
      ],
      "metadata": {
        "id": "aKmIysBmBWES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Initilize:\n",
        "    def __init__(self, layer_dimension, activation_function, y_train, method = \"Xavier_U\"):\n",
        "        self.n = layer_dimension\n",
        "        self.activation_fn = activation_function\n",
        "        self.y = y_train\n",
        "        self.Init_method = method\n",
        "\n",
        "class InitializeWeights(Initilize):\n",
        "    def __init__(self, ip_size, op_size, activation_function, batch_size, method):\n",
        "        super().__init__([ip_size, op_size], activation_function, batch_size, method)  # Properly inherit Initilize attributes\n",
        "        self.i_size = ip_size\n",
        "        self.o_size = op_size\n",
        "        self.Init_weights()\n",
        "\n",
        "\n",
        "    def Init_weights(self):\n",
        "        np.random.seed(0)\n",
        "\n",
        "        if self.Init_method == \"Xavier_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(1 / self.i_size)\n",
        "          self.weight = np.random.randn(self.o_size,self.i_size)*a\n",
        "\n",
        "        elif self.Init_method == \"Xavier_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / (self.o_size + self.i_size))\n",
        "          self.weight = np.random.uniform((-a), a,( self.o_size,self.i_size))\n",
        "\n",
        "        elif self.Init_method == \"He_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(2 / self.i_size)\n",
        "          self.weight = np.random.randn(self.o_size,self.i_size)*a\n",
        "\n",
        "        elif self.Init_method == \"He_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / self.i_size)\n",
        "          self.weight = np.random.uniform(-a, a, (self.o_size,self.i_size))\n",
        "\n",
        "        elif self.Init_method == \"Random\":\n",
        "          np.random.seed(0)\n",
        "          self.weight = np.random.randn(self.o_size,self.i_size)*0.01\n",
        "        else:\n",
        "          raise ValueError(f\"Unknown initialization method: {self.Init_method}\")\n",
        "\n",
        "\n",
        "        # Initialize biases and activations\n",
        "        self.bias = np.zeros((self.o_size, 1))\n",
        "        self.a = np.zeros((self.o_size, len(self.y[1])))\n",
        "        self.h = np.zeros((self.o_size, len(self.y[1])))\n",
        "\n",
        "        # Activation function and its derivative\n",
        "        self.g = apply_activation(self.activation_fn, self.a).do_activation()\n",
        "        self.d_g = apply_activation(self.activation_fn, self.a).do_activation_derivative()\n",
        "\n",
        "        # Gradients\n",
        "        self.d_a = np.zeros_like(self.a)\n",
        "        self.d_h = np.zeros_like(self.h)\n",
        "        self.d_w = np.zeros_like(self.weight)\n",
        "        self.d_b = np.zeros_like(self.bias)\n",
        "        self.Weight_updates = np.zeros_like(self.weight)\n",
        "        self.bias_updates = np.zeros_like(self.bias)\n",
        "\n",
        "\n",
        "\n",
        "class Weight_bias(Initilize):\n",
        "    def __init__(self, layer_dimension, activation_function, y_train, method=\"Xavier_U\"):\n",
        "        super().__init__(layer_dimension, activation_function, y_train, method)\n",
        "        self.network = []\n",
        "\n",
        "\n",
        "    def Init_network(self):\n",
        "\n",
        "        for i in range(1, len(self.n)):\n",
        "          self.network.append(InitializeWeights( self.n[i-1], self.n[i], self.activation_fn[i-1], self.y, self.Init_method))\n",
        "\n",
        "        return self.network\n"
      ],
      "metadata": {
        "id": "PATIBefL65DZ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feedforward Code for Prediction**"
      ],
      "metadata": {
        "id": "nDDchVi6BiFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Pre_Feedforward:\n",
        "\n",
        "  def __init__(self, inputs, w, b, activation_fn):\n",
        "    self.ip= inputs\n",
        "    self.w = w\n",
        "    self.b = b\n",
        "    self.activation_fn = activation_fn\n",
        "    self.Preactivation_cal()\n",
        "\n",
        "\n",
        "  def Preactivation_cal(self):\n",
        "\n",
        "    A = np.dot(self.w, self.ip) + self.b\n",
        "    A_norm = (A - np.mean(A, axis=1, keepdims=True)) / np.std(A, axis=1, keepdims=True) #Activation Normalization\n",
        "    H = apply_activation(self.activation_fn, A_norm).do_activation()\n",
        "    #cache = (H, A)\n",
        "    #self.cache[\"Input\"] = self.Prev_layer_H\n",
        "    #self.cache[\"Pre_act\"] = H\n",
        "    return A , H\n",
        "\n",
        "class Feedforward:\n",
        "\n",
        "  def __init__(self,X_train, activation_fn, method, network):\n",
        "    self.input = X_train\n",
        "    self.activation_fn = activation_fn\n",
        "    self.method = method\n",
        "    self.network = network\n",
        "\n",
        "  def Forward_prop(self):\n",
        "\n",
        "    L = len(self.network)\n",
        "    # print(L)\n",
        "\n",
        "    for i in range(L):\n",
        "      # print(i)\n",
        "      # print(\"oth layer input\" , self.input.shape)\n",
        "\n",
        "      self.network[i].a, self.network[i].h = Pre_Feedforward(self.input, self.network[i].weight, self.network[i].bias, self.activation_fn[i]).Preactivation_cal()\n",
        "      self.input = self.network[i].a\n",
        "      # print(\"oth layer w\" , a[i].weight.shape)\n",
        "      # print(\"oth layer b\" , a[i].bias.shape)\n",
        "      # print(\"oth layer h\" , a[i].h.shape)\n",
        "      # print(\"oth layer a\" , a[i].a.shape)\n",
        "\n",
        "\n",
        "    return self.network\n"
      ],
      "metadata": {
        "id": "FftS8rFt9VWK"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_new= Normalize(X_train).Norm_reshape()\n",
        "y_new =OneHotEncoder(X_train,y_train).onehot_encode()"
      ],
      "metadata": {
        "id": "p4VcmSf38u1V"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Here is the answer of question 2 where we have to predict the output of 10 class in probability disturbution form**"
      ],
      "metadata": {
        "id": "6FSYr70tAbvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Initialise the network with Xavier Uniform initilised weight and biases\n",
        "'''\n",
        "\n",
        "Initial_network = Weight_bias(layer_dimension = [784, 128, 64, 10],\n",
        "                              activation_function = ['sigmoid','relu','softmax'],\n",
        "                              y_train = y_new,\n",
        "                              method = \"Xavier_U\").Init_network()\n",
        "\n",
        "'''\n",
        "Predicting the output of 10 class with Xavier Uniform initilised weight and biases\n",
        "'''\n",
        "\n",
        "Predict = Feedforward(X_train = X_new,\n",
        "                     activation_fn= ['sigmoid','relu','softmax'],\n",
        "                     method = \"Xavier_U\",\n",
        "                     network= Initial_network).Forward_prop()\n",
        "\n",
        "\n",
        "print(\" Here is the prediction of output of 10 class in probability disturbution form \\n with Xavier Uniform initilised weight and biases: \\n\\n\", Predict[2].h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fEvrBBF8wnE",
        "outputId": "ecd66423-7cd3-470c-db51-793443baf871"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Here is the prediction of output of 10 class in probability disturbution form \n",
            " with Xavier Uniform initilised weight and biases: \n",
            "\n",
            " [[0.12972934 0.02736321 0.05606109 ... 0.02924843 0.13919559 0.28173024]\n",
            " [0.08958929 0.10297096 0.09014636 ... 0.05940065 0.04886422 0.07818933]\n",
            " [0.0826621  0.25909435 0.15104963 ... 0.21367701 0.08127471 0.02110413]\n",
            " ...\n",
            " [0.24521651 0.17669521 0.11148419 ... 0.03436621 0.05244138 0.01646606]\n",
            " [0.05550834 0.21928251 0.08714714 ... 0.15697124 0.05925401 0.05232998]\n",
            " [0.20670031 0.04619039 0.04105517 ... 0.00847484 0.07039559 0.03174267]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "IeM0PGqhINGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Question 3 (24 Marks)\n",
        "##Implement the backpropagation algorithm with support for the following optimisation functions\n",
        "\n",
        "##sgd\n",
        "##momentum based gradient descent\n",
        "##nesterov accelerated gradient descent\n",
        "##rmsprop\n",
        "##adam\n",
        "##nadam (Not required as aked)\n",
        "##We will check the code for implementation and ease of use (e.g., how easy it is to add a new optimisation algorithm such as Eve). Note that the code should be flexible enough to work with different batch sizes."
      ],
      "metadata": {
        "id": "sjL6Lhp3B5e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "ax8lGGyzIOal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L2 / L1 Norm Regularization Code and their Gradient Calling functions :**"
      ],
      "metadata": {
        "id": "FEACWx0UDJWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Regularisation:\n",
        "    def __init__(self, network, weight=0):\n",
        "        self.network = network\n",
        "        self.weight = weight\n",
        "\n",
        "class L2_regularisation(Regularisation):\n",
        "    def Apply_L2(self):\n",
        "        \"\"\"Returns L2 regularization loss for the given network.\"\"\"\n",
        "        L = len(self.network)\n",
        "        res = 0\n",
        "        for j in range(L):\n",
        "            if np.isnan(self.network[j].weight).any():\n",
        "                print(f\"Warning: NaN detected in network weights at layer {j}\")\n",
        "                return 0\n",
        "            res += 0.5 * np.sum(self.network[j].weight ** 2)\n",
        "        return res\n",
        "\n",
        "    def Apply_L2_grad(self, weight):\n",
        "        \"\"\"Returns L2 regularization gradient for the given weight matrix/tensor.\"\"\"\n",
        "        return 2 * weight\n",
        "\n",
        "class L1_regularisation(Regularisation):\n",
        "    def Apply_L1(self):\n",
        "        \"\"\"Returns L1 regularization loss for the given network.\"\"\"\n",
        "        L = len(self.network)\n",
        "        res = 0\n",
        "        for j in range(L):\n",
        "            if np.isnan(self.network[j].weight).any():\n",
        "                print(f\"Warning: NaN detected in network weights at layer {j}\")\n",
        "                return 0\n",
        "            res += (1 / 2) * np.sum(np.abs(self.network[j].weight))\n",
        "        return res\n",
        "\n",
        "    def Apply_L1_grad(self, weight):\n",
        "        \"\"\"Returns L1 regularization gradient for the given weight matrix/tensor.\"\"\"\n",
        "        return np.sign(weight)\n",
        "\n",
        "class ApplyReg(Regularisation):\n",
        "    def __init__(self, reg_function, network, weight=0):\n",
        "        self.reg_function = reg_function\n",
        "        super().__init__(network, weight)\n",
        "\n",
        "    def do_reg(self):\n",
        "        if self.reg_function == 'L2':\n",
        "            return L2_regularisation(self.network).Apply_L2()\n",
        "        if self.reg_function == 'L1':\n",
        "            return L1_regularisation(self.network).Apply_L1()\n",
        "        if self.reg_function == 'L2_d':\n",
        "            return L2_regularisation(self.network).Apply_L2_grad(self.weight)\n",
        "        if self.reg_function == 'L1_d':\n",
        "            return L1_regularisation(self.network).Apply_L1_grad(self.weight)\n"
      ],
      "metadata": {
        "id": "kj4ZlQjV9Jem"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overall Loss and Accuracy Calling functions code :**"
      ],
      "metadata": {
        "id": "ASwu48SQD_Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CalculateAllLoss:\n",
        "  def __init__(self, X_train, y_predicted,network, y_train, primary_loss, weight_decay=0, regularisation_fn=None):\n",
        "    self.y_predicted = y_predicted\n",
        "    self.y_true = y_train\n",
        "    self.network = network\n",
        "    self.X_train = X_train\n",
        "    self.loss_value = primary_loss\n",
        "    self.weight_decay = weight_decay\n",
        "    self.regularisation_fn= regularisation_fn\n",
        "    self.calc_accuracy_loss()\n",
        "\n",
        "\n",
        "  def overall_loss(self):\n",
        "    \"\"\"\n",
        "    Calculates the total loss of the network.\n",
        "    - Total loss value.\n",
        "    \"\"\"\n",
        "\n",
        "    total_loss = self.loss_value\n",
        "\n",
        "    if self.weight_decay > 0 and self.regularisation_fn:\n",
        "        regularized_val = ApplyReg(self.regularisation_fn, self.network).do_reg()\n",
        "        print(f\"Reg value: {regularized_val}\")\n",
        "        total_loss += self.weight_decay * regularized_val\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def calc_accuracy_loss(self):\n",
        "    \"\"\"\n",
        "    Computes the accuracy and loss for a given neural network.\n",
        "    \"\"\"\n",
        "\n",
        "    total_loss = self.loss_value\n",
        "\n",
        "    if self.weight_decay > 0 and self.regularisation_fn:\n",
        "        regularized_val = ApplyReg(self.regularisation_fn, self.network).do_reg()\n",
        "        print(f\"Reg value: {regularized_val}\")\n",
        "        total_loss += self.weight_decay * regularized_val\n",
        "\n",
        "\n",
        "    assert self.X_train.shape[1] == self.y_true.shape[1], \"Mismatch in batch size between inputs and labels\"\n",
        "\n",
        "\n",
        "    batch_size = self.X_train.shape[1]\n",
        "    correct_predictions = np.sum(np.argmax(self.y_predicted, axis=0) == np.argmax(self.y_true, axis=0))\n",
        "\n",
        "    accuracy = correct_predictions / batch_size\n",
        "\n",
        "    return accuracy , total_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "lc5p7TWsCycT"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All Loss functions and their Gradient Calling functions code :**"
      ],
      "metadata": {
        "id": "fvCL3kjNEO6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CalLoss:\n",
        "    def __init__(self, y, y_pred):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_pred\n",
        "        if y.shape != y_pred.shape:\n",
        "            raise ValueError(f\"Shape mismatch: y shape is {self.y.shape}, y_predicted shape is {self.y_predicted.shape}\")\n",
        "\n",
        "class CrossEntropy(CalLoss):\n",
        "    def give_celoss(self):\n",
        "        epsilon = 1e-8  # Small value to prevent log(0)\n",
        "        return -np.mean(self.y * np.log(self.y_predicted + epsilon))\n",
        "\n",
        "    def Give_cegrad(self):\n",
        "        epsilon = 1e-8  # Prevent division by zero\n",
        "        grad = -self.y / (self.y_predicted + epsilon)\n",
        "        return grad\n",
        "\n",
        "class SquaredError(CalLoss):\n",
        "    def give_seloss(self):\n",
        "        return np.mean((self.y - self.y_predicted) ** 2)\n",
        "\n",
        "    def Give_segrad(self):\n",
        "        grad = -2 * (self.y - self.y_predicted)\n",
        "        return np.clip(grad, -1, 1)  # Clip to prevent exploding gradients\n",
        "\n",
        "class callloss(CalLoss):\n",
        "    def __init__(self, loss_function, y, y_pred):\n",
        "        self.loss_function = loss_function.lower()\n",
        "        super().__init__(y, y_pred)\n",
        "\n",
        "    def give_loss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return CrossEntropy(self.y, self.y_predicted).give_celoss()\n",
        "        elif self.loss_function == 'se':\n",
        "            return SquaredError(self.y, self.y_predicted).give_seloss()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss function: {self.loss_function}\")\n",
        "\n",
        "    def give_gradloss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return CrossEntropy(self.y, self.y_predicted).Give_cegrad()\n",
        "        elif self.loss_function == 'se':\n",
        "            return SquaredError(self.y, self.y_predicted).Give_segrad()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss function: {self.loss_function}\")\n"
      ],
      "metadata": {
        "id": "qPTuWBjAC2Ij"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Below we have the answers of question 3 : Proper Backprop code and along with that we ahve all 5 optimiser codes"
      ],
      "metadata": {
        "id": "Bb3D9CW3Fb10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backpropagation codes which returns the network :**"
      ],
      "metadata": {
        "id": "eGducKvQEYyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "class Backpropagation:\n",
        "\n",
        "    def __init__(self, loss_function, X_train, y_train, y_pred, network, weight_decay, batch, batch_size, activation_fn):\n",
        "        self.loss_function = loss_function\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.y_predicted = y_pred\n",
        "        self.network = network\n",
        "        self.weight_decay = weight_decay\n",
        "        self.batch = batch\n",
        "        self.batch_size = batch_size\n",
        "        self.activation_fn = activation_fn\n",
        "\n",
        "\n",
        "    def backward_propagation(self):\n",
        "      L = len(self.network)\n",
        "\n",
        "      assert(self.y_train.shape[1] == self.y_predicted.shape[1])\n",
        "\n",
        "\n",
        "      self.network[-1].d_a = callloss(self.loss_function,self.y_train, self.y_predicted).give_gradloss()\n",
        "      # print(\"network -[-1].d_a : \" , self.network[-1].d_a.shape)\n",
        "      A_k = apply_activation(self.activation_fn[-1], self.network[-1].h).do_activation_derivative()\n",
        "      # print(\"shape A_ k : \" , A_k.shape)\n",
        "      self.network[-1].d_h = self.network[-1].d_a * A_k\n",
        "      # print(\"network -[-1].d_h : \" , self.network[-1].d_a.shape)\n",
        "\n",
        "      self.network[-1].d_w = self.network[-1].d_h @ self.network[-2].a.T  + self.weight_decay * self.network[-1].weight\n",
        "      # print(\"network -[-1].d_w : \" , self.network[-1].d_w.shape)\n",
        "      d_b = -np.sum(self.network[-1].d_h, axis = 1)\n",
        "      self.network[-1].d_b = d_b.reshape(-1 , 1)\n",
        "      # print(\"network -[-1].d_b : \" , self.network[-1].d_b.shape)\n",
        "\n",
        "\n",
        "\n",
        "      for k in range(L-2,0,-1):\n",
        "          # print(f\"No of layers rotation {k}\")\n",
        "\n",
        "          self.network[k].d_h = self.network[k + 1].weight.T @ self.network[k + 1].d_a\n",
        "          # print(f\"shape self.network-{k}.d_h : \" , self.network[k].d_h.shape)\n",
        "          act_derv =  apply_activation(self.activation_fn[k], self.network[k].a)\n",
        "          self.network[k].d_a = self.network[k].d_h * act_derv.do_activation_derivative()\n",
        "          # print(f\"shape self.network-{k}.d_a : \" , self.network[k].d_a.shape)\n",
        "\n",
        "          self.network[k].d_w = self.network[k].d_a @ self.network[k-1].h.T  + self.weight_decay * self.network[k].weight\n",
        "          # print(f\"shape self.network-{k}.d_w : \" , self.network[k].d_w.shape)\n",
        "          derv_bias = -np.sum(self.network[k].d_a, axis=1)\n",
        "          self.network[k].d_b = derv_bias.reshape(-1 , 1)\n",
        "          # print(f\"shape self.network-{k}.d_b : \" , self.network[k].d_b.shape)\n",
        "\n",
        "      # print(f\"shape self.network-{0}.d_a : \" , self.network[0].d_a.shape)\n",
        "      d_a = self.network[0].d_a[:, self.batch*self.batch_size : (self.batch+1)*self.batch_size]\n",
        "      # print(f\"shape self.network-{0}.d_a : \" , d_a.shape)\n",
        "      self.network[0].d_w = np.dot(d_a , self.X_train.T) + self.weight_decay * self.network[0].weight\n",
        "      # print(\"network -[0].d_w : \" , self.network[0].d_w.shape)\n",
        "      self.network[0].d_b = np.sum(self.network[0].d_a, axis=1, keepdims = True)\n",
        "\n",
        "      return self.network\n"
      ],
      "metadata": {
        "id": "SAoaQhHmC4QT"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All 5 Optimisers ( SGD / Momentum / NAG / RMSProp / ADAM ) Calling functions :**"
      ],
      "metadata": {
        "id": "U0RoKfJ5Eyp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "class Optimizer:\n",
        "  def __init__(self, loss_function, X_train, y_train, activation_fn, layers_dimensions ,method, batch_size, epochs,validX_train = None, validy_train = None,weight_decay = 0, eta = 0.01, beta = 0.9, beta2 = 0.999, regularization_fn = \"L2\",grad_reglr_fn = \"L2_d\" , use_wandb=False):\n",
        "      self.loss_function = loss_function\n",
        "      self.X_train = X_train\n",
        "      self.y_train = y_train\n",
        "      self.validX_train = validX_train\n",
        "      self.validy_train = validy_train\n",
        "      self.activation_fn = activation_fn\n",
        "      self.method = method\n",
        "      self.n = layers_dimensions\n",
        "      self.batch_size = batch_size\n",
        "      self.epochs = epochs\n",
        "      self.weight_decay = weight_decay\n",
        "      self.eta = eta\n",
        "      self.epsilon =1e-10\n",
        "      self.beta = beta\n",
        "      self.beta2 = beta2\n",
        "      self.regularization_fn = regularization_fn\n",
        "      self.grad_reglr_fn = grad_reglr_fn\n",
        "      self.batches_number = self.X_train.shape[1]//self.batch_size\n",
        "      z = Weight_bias(self.n, self.activation_fn, self.y_train, self.method)\n",
        "      self.init_network = z.Init_network()\n",
        "      self.min_eta = 1e-4\n",
        "      self.use_wandb = use_wandb\n",
        "      # wandb.init(project= \"DA6401_Assignment_1\" , name = \"RMSProp with validation data of 1/3rd of 60k rows\")\n",
        "\n",
        "\n",
        "class SGD(Optimizer):\n",
        "\n",
        "  def Gradient_descent(self):\n",
        "\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "    L = len(self.n) - 1\n",
        "    # Initialize variables neesed to keep track of loss\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    overall_loss = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "\n",
        "    for epochs in range(self.epochs):\n",
        "\n",
        "      for batch in tqdm(range(self.batches_number)):\n",
        "        print(f\"   batch no: {batch+1}     Train acc: {accuracy}     val acc: {val_accuracy}    Train Loss: {train_loss}\" )\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:  # Ensure we have enough data points\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)  # Compute standard deviation of recent losses\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "\n",
        "        for j in range(L):\n",
        "          self.fw_network[j].weight -= self.eta * self.bp_network[j].d_w\n",
        "          self.fw_network[j].bias -= self.eta * self.bp_network[j].d_b\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 4)\n",
        "        train_loss = round(loss , 4)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 4)\n",
        "          val_loss = round(v_loss , 4)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "    return self.loss_history, self.training_history, self.eta_history , self.validation_history\n",
        "\n",
        "\n",
        "\n",
        "class MGD(Optimizer):\n",
        "\n",
        "  def momentum_GD(self):\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "    L = len(self.n) - 1\n",
        "    # Initialize variables neesed to keep track of loss\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "\n",
        "    u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "    for epochs in range(self.epochs):\n",
        "\n",
        "      for batch in tqdm(range(self.batches_number)):\n",
        "        print(f\"   batch no: {batch+1}     Train acc: {accuracy}     val acc: {val_accuracy}    Train Loss: {train_loss}\" )\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:  # Ensure we have enough data points\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)  # Compute standard deviation of recent losses\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "        # self.beta = min(1 - 2 ** (-1 - math.log((self.calls / 250.0) + 1, 2)), self.beta)\n",
        "\n",
        "        for j in range(L):\n",
        "          u_w[j] = u_w[j] * self.beta + self.bp_network[j].d_w * self.eta\n",
        "          u_b[j] = u_b[j] * self.beta + self.bp_network[j].d_b * self.eta\n",
        "\n",
        "          self.fw_network[j].weight -= u_w[j]\n",
        "          self.fw_network[j].bias -= u_b[j]\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 4)\n",
        "        train_loss = round(loss , 4)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 4)\n",
        "          val_loss = round(v_loss , 4)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "    return self.loss_history, self.training_history, self.eta_history , self.validation_history\n",
        "\n",
        "\n",
        "\n",
        "class NAG(Optimizer):\n",
        "\n",
        "  def Nesterov_AGD(self):\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "    L = len(self.n) - 1\n",
        "    # Initialize variables neesed to keep track of loss\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "    u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "    lookleap_network = self.init_network[:]\n",
        "\n",
        "    for epochs in range(self.epochs):\n",
        "\n",
        "      for batch in tqdm(range(self.batches_number)):\n",
        "        print(f\"   batch no: {batch+1}     Train acc: {accuracy}     val acc: {val_accuracy}    Train Loss: {train_loss}\" )\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:  # Ensure we have enough data points\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)  # Compute standard deviation of recent losses\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "\n",
        "        for j in range(L):\n",
        "          u_w[j] = u_w[j] * self.beta + self.bp_network[j].d_w * self.eta\n",
        "          u_b[j] = u_b[j] * self.beta + self.bp_network[j].d_b * self.eta\n",
        "\n",
        "          self.fw_network[j].weight -= u_w[j]\n",
        "          self.fw_network[j].bias -= u_b[j]\n",
        "\n",
        "          lookleap_network[j].weight -= (self.eta * self.bp_network[j].d_w + self.beta * u_w[j])\n",
        "          lookleap_network[j].bias -= (self.eta * self.bp_network[j].d_b + self.beta * u_b[j])\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 4)\n",
        "        train_loss = round(loss , 4)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 4)\n",
        "          val_loss = round(v_loss , 4)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "    return self.loss_history, self.training_history, self.eta_history , self.validation_history\n",
        "\n",
        "\n",
        "\n",
        "class RMSProp(Optimizer):\n",
        "\n",
        "  def rms_GD(self):\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "        assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "\n",
        "    L = len(self.n) - 1  # Number of layers\n",
        "\n",
        "    # Initialize loss and training tracking variables\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "\n",
        "    overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "\n",
        "    # Initialize squared gradient accumulators for RMSProp\n",
        "    u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "    for epoch in range(self.epochs):\n",
        "        print(f\"Epoch {epoch+1}/{self.epochs} | Train Acc: {accuracy} | Val Acc: {val_accuracy} | Train Loss: {train_loss}\")\n",
        "\n",
        "        for batch in range(self.batches_number):\n",
        "\n",
        "            # Mini-batch selection\n",
        "            X_batch = self.X_train[:, batch * self.batch_size:(batch + 1) * self.batch_size]\n",
        "            y_true_batch = self.y_train[:, batch * self.batch_size:(batch + 1) * self.batch_size]\n",
        "\n",
        "            self.X_batch = X_batch\n",
        "            self.y_true_batch = y_true_batch\n",
        "\n",
        "            # Forward propagation\n",
        "            fw_network = Feedforward(self.X_batch, self.activation_fn, self.method, self.init_network).Forward_prop()\n",
        "            self.fw_network = fw_network\n",
        "            self.y_predicted = self.fw_network[L-1].h\n",
        "            self.y_pred_batch = self.y_predicted\n",
        "\n",
        "            # Compute loss\n",
        "            self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "            overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch,\n",
        "                                            self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "            self.loss_history.append(overall_loss)\n",
        "\n",
        "            # Backpropagation\n",
        "            bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch,\n",
        "                                         self.fw_network, self.weight_decay, self.batch, self.batch_size,\n",
        "                                         self.activation_fn).backward_propagation()\n",
        "            self.bp_network = bp_network\n",
        "\n",
        "            # **Dynamic Learning Rate Adjustment (Based on Loss Trends)**\n",
        "            if len(self.loss_history) > 5:\n",
        "                recent_losses = self.loss_history[-5:]\n",
        "                loss_std = np.std(recent_losses)\n",
        "\n",
        "                if self.loss_history[-1] > self.loss_history[-2]:\n",
        "                    self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "            self.eta_history.append(self.eta)\n",
        "\n",
        "            for j in range(L):\n",
        "                # Update moving average of squared gradients (RMSProp update)\n",
        "                u_w[j] = self.beta * u_w[j] + (1 - self.beta) * (self.bp_network[j].d_w) ** 2\n",
        "                u_b[j] = self.beta * u_b[j] + (1 - self.beta) * (self.bp_network[j].d_b) ** 2\n",
        "\n",
        "                # **Numerically Stable RMSProp Update**\n",
        "                denom_w = np.maximum(np.sqrt(u_w[j]) + self.epsilon, 1e-8)\n",
        "                denom_b = np.maximum(np.sqrt(u_b[j]) + self.epsilon, 1e-8)\n",
        "\n",
        "                self.fw_network[j].weight -= (self.eta / denom_w) * self.bp_network[j].d_w\n",
        "                self.fw_network[j].bias -= (self.eta / denom_b) * self.bp_network[j].d_b\n",
        "\n",
        "            # **Gradient Clipping (Optional)**\n",
        "            max_grad_norm = 1.0\n",
        "            for j in range(L):\n",
        "                self.bp_network[j].d_w = np.clip(self.bp_network[j].d_w, -max_grad_norm, max_grad_norm)\n",
        "                self.bp_network[j].d_b = np.clip(self.bp_network[j].d_b, -max_grad_norm, max_grad_norm)\n",
        "\n",
        "            # Compute training accuracy & loss\n",
        "            acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch,\n",
        "                                         self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "            accuracy = round(acc, 4)\n",
        "            train_loss = round(loss, 4)\n",
        "            self.training_history.append((accuracy, train_loss))\n",
        "\n",
        "            # **Validation Step (if available)**\n",
        "            if self.validX_train is not None:\n",
        "                network = Feedforward(self.validX_train, self.activation_fn, self.method, self.init_network).Forward_prop()\n",
        "                self.network = network\n",
        "                self.valy_predicted = self.network[L-1].h\n",
        "\n",
        "                val_acc, val_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network,\n",
        "                                                     self.validy_train, self.loss, self.weight_decay,\n",
        "                                                     self.regularization_fn).calc_accuracy_loss()\n",
        "                val_accuracy = round(val_acc, 4)\n",
        "                val_loss = round(val_loss, 4)\n",
        "                val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network,\n",
        "                                                    self.validy_train, self.loss, self.weight_decay,\n",
        "                                                    self.regularization_fn).overall_loss()\n",
        "                self.validation_history.append((val_accuracy, val_loss))\n",
        "\n",
        "    return self.loss_history, self.training_history, self.eta_history, self.validation_history , self.init_network\n",
        "\n",
        "\n",
        "\n",
        "class ADAM(Optimizer):\n",
        "\n",
        "    def adam_GD(self):\n",
        "\n",
        "        assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "        if self.validy_train is not None:\n",
        "            assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "\n",
        "        L = len(self.n) - 1  # Number of layers\n",
        "        self.eta_history = []\n",
        "        self.loss_history = []\n",
        "        self.training_history = []\n",
        "        self.valid_loss_history = []\n",
        "        self.validation_history = []\n",
        "        self.batch = 0\n",
        "        accuracy = 0\n",
        "        val_accuracy = 0\n",
        "        val_overall_loss = 0\n",
        "        train_loss = 0\n",
        "        i = 0  # Iteration counter\n",
        "\n",
        "        # Initialize moment estimates\n",
        "        u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "        u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "        v_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "        v_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            print(f\"Epoch {epoch+1}/{self.epochs} | Train Acc: {accuracy} | Val Acc: {val_accuracy} | Train Loss: {train_loss}\")\n",
        "\n",
        "            for batch in range(self.batches_number):\n",
        "\n",
        "                # Mini-batch selection\n",
        "                X_batch = self.X_train[:, batch * self.batch_size:(batch + 1) * self.batch_size]\n",
        "                y_true_batch = self.y_train[:, batch * self.batch_size:(batch + 1) * self.batch_size]\n",
        "\n",
        "                self.X_batch = X_batch\n",
        "                self.y_true_batch = y_true_batch\n",
        "\n",
        "                # Forward propagation\n",
        "                fw_network = Feedforward(self.X_batch, self.activation_fn, self.method, self.init_network).Forward_prop()\n",
        "                self.fw_network = fw_network\n",
        "                self.y_predicted = self.fw_network[L-1].h\n",
        "                self.y_pred_batch = self.y_predicted\n",
        "\n",
        "                # Compute loss\n",
        "                self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "                overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch,\n",
        "                                                self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "                self.loss_history.append(overall_loss)\n",
        "\n",
        "                # Backpropagation\n",
        "                bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch,\n",
        "                                             self.fw_network, self.weight_decay, self.batch, self.batch_size,\n",
        "                                             self.activation_fn).backward_propagation()\n",
        "                self.bp_network = bp_network\n",
        "\n",
        "                # **Dynamic Learning Rate Adjustment (Cosine Annealing)**\n",
        "                self.eta = self.min_eta + 0.5 * (self.eta - self.min_eta) * (1 + np.cos((epoch / self.epochs) * np.pi))\n",
        "                self.eta_history.append(self.eta)\n",
        "\n",
        "                for j in range(L):\n",
        "                    # Compute moment estimates\n",
        "                    u_w[j] = self.beta * u_w[j] + (1 - self.beta) * self.bp_network[j].d_w\n",
        "                    u_b[j] = self.beta * u_b[j] + (1 - self.beta) * self.bp_network[j].d_b\n",
        "                    v_w[j] = self.beta2 * v_w[j] + (1 - self.beta2) * (self.bp_network[j].d_w) ** 2\n",
        "                    v_b[j] = self.beta2 * v_b[j] + (1 - self.beta2) * (self.bp_network[j].d_b) ** 2\n",
        "\n",
        "                    # Bias correction\n",
        "                    u_w_pred = u_w[j] / (1 - np.power(self.beta, i + 1))\n",
        "                    u_b_pred = u_b[j] / (1 - np.power(self.beta, i + 1))\n",
        "                    v_w_pred = v_w[j] / (1 - np.power(self.beta2, i + 1))\n",
        "                    v_b_pred = v_b[j] / (1 - np.power(self.beta2, i + 1))\n",
        "\n",
        "                    # **AdamW-style update (Decoupled Weight Decay)**\n",
        "                    denom_w = np.maximum(np.sqrt(v_w_pred) + self.epsilon, 1e-8)\n",
        "                    denom_b = np.maximum(np.sqrt(v_b_pred) + self.epsilon, 1e-8)\n",
        "\n",
        "                    self.fw_network[j].weight -= (self.eta / denom_w) * u_w_pred + self.eta * self.weight_decay * self.fw_network[j].weight\n",
        "                    self.fw_network[j].bias -= (self.eta / denom_b) * u_b_pred  # No decay for biases\n",
        "\n",
        "                # Update model with new weights\n",
        "                self.init_network = self.fw_network\n",
        "\n",
        "                # Compute training accuracy & loss\n",
        "                acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch,\n",
        "                                             self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "                accuracy = round(acc, 4)\n",
        "                train_loss = round(loss, 4)\n",
        "                self.training_history.append((accuracy, train_loss))\n",
        "\n",
        "                i += 1  # Update iteration counter\n",
        "\n",
        "                # **Validation Step (if available)**\n",
        "                if self.validX_train is not None:\n",
        "                    network = Feedforward(self.validX_train, self.activation_fn, self.method, self.init_network).Forward_prop()\n",
        "                    self.network = network\n",
        "                    self.valy_predicted = self.network[L-1].h\n",
        "\n",
        "                    val_acc, val_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network,\n",
        "                                                         self.validy_train, self.loss, self.weight_decay,\n",
        "                                                         self.regularization_fn).calc_accuracy_loss()\n",
        "                    val_accuracy = round(val_acc, 4)\n",
        "                    val_loss = round(val_loss, 4)\n",
        "                    val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network,\n",
        "                                                        self.validy_train, self.loss, self.weight_decay,\n",
        "                                                        self.regularization_fn).overall_loss()\n",
        "                    self.validation_history.append((val_accuracy, val_loss))\n",
        "\n",
        "        return self.loss_history, self.training_history, self.eta_history, self.validation_history , self.init_network\n",
        "\n",
        "\n",
        "\n",
        "class GiveOptimizers(Optimizer):\n",
        "    def __init__(self, optimization_fn, loss_function, X_train, y_train, activation_fn, layers_dimensions ,method, batch_size, epochs,  validX_train , validy_train ,weight_decay = 0, eta = 0.01, beta = 0.9, beta2 = 0.999, regularization_fn = \"L2\",grad_reglr_fn = \"L2_d\", use_wandb=False):\n",
        "      self.optimization_function = optimization_fn.lower()\n",
        "      super().__init__( loss_function, X_train, y_train, activation_fn, layers_dimensions ,method, batch_size, epochs, validX_train, validy_train ,weight_decay = 0, eta = 0.01, beta = 0.9, beta2 = 0.999, regularization_fn = \"L2\",grad_reglr_fn = \"L2_d\" , use_wandb=False)\n",
        "\n",
        "    def apply_optimization(self):\n",
        "      if self.optimization_function == 'sgd':\n",
        "          return SGD(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs,  self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn, self.use_wandb).Gradient_descent()\n",
        "      elif self.optimization_function == 'momentum':\n",
        "          return MGD(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn, self.use_wandb).momentum_GD()\n",
        "      elif self.optimization_function == 'nag':\n",
        "          return NAG(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn, self.use_wandb).Nesterov_AGD()\n",
        "      elif self.optimization_function == 'rmsp':\n",
        "          return RMSProp(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn, self.use_wandb).rms_GD()\n",
        "      elif self.optimization_function == 'adam':\n",
        "          return ADAM(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn, self.use_wandb).adam_GD()"
      ],
      "metadata": {
        "id": "876QHHBCC54L"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4 (10 Marks)\n",
        "##Use the standard train/test split of fashion_mnist (use (X_train, y_tr(X_test, y_test) = fashion_mnist.load_data()).\n",
        "##Keep 10% of the training data aside as validation data for this hyperparameter search. Here are some suggestions for different values to try for hyperparameters.\n",
        "##As you can quickly see that this leads to an exponential number of combinations. You will have to think about strategies to do this hyperparameter search efficiently.\n",
        "##Check out the options provided by wandb.sweep and write down what strategy you chose and why.\n",
        "\n",
        "##number of epochs: 5, 10\n",
        "##number of hidden layers: 3, 4, 5\n",
        "##size of every hidden layer: 32, 64, 128\n",
        "##weight decay (L2 regularisation): 0, 0.0005, 0.5\n",
        "##learning rate: 1e-3, 1 e-4\n",
        "##optimizer: sgd, momentum, nesterov, rmsprop, adam, nadam\n",
        "##batch size: 16, 32, 64\n",
        "##weight initialisation: random, Xavier\n",
        "##activation functions: sigmoid, tanh, ReLU\n",
        "##wandb will automatically generate the following plots.\n",
        "##Paste these plots below using the \"Add Panel to Report\" feature.\n",
        "##Make sure you use meaningful names for each sweep (e.g. hl_3_bs_16_ac_tanh to indicate that there were 3 hidden layers, batch size was 16 and activation function was ReLU)\n",
        "##instead of using the default names (whole-sweep, kind-sweep) given by wandb."
      ],
      "metadata": {
        "id": "6WFF4wSQGiDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "AldeNbMPHtYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing the TrainValSplit, OneHotEncoder, Normalize classes to preprocess the data :**"
      ],
      "metadata": {
        "id": "3Rv-_i_PHQfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "X_split, y_split, valX_split, valy_split = TrainValSplit(X_train, y_train,Val_split_ratio= 0.1).Apply_split()\n",
        "\n",
        "X_new= Normalize(X_split).Norm_reshape()\n",
        "y_new =OneHotEncoder(X_split,y_split).onehot_encode()\n",
        "valX_new= Normalize(valX_split).Norm_reshape()\n",
        "valy_new =OneHotEncoder(valX_split,valy_split).onehot_encode()\n"
      ],
      "metadata": {
        "id": "R7etNqrmGHY9"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trying different hyperparametrs to tune my model"
      ],
      "metadata": {
        "id": "dZvV1jFhMHFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model2 = GiveOptimizers(\n",
        "    optimization_fn=\"adam\",  # Using \"adam\" or \"rmsp\" for best performance\n",
        "    loss_function=\"ce\",  # Cross-Entropy Loss\n",
        "    X_train=X_new,  # Normalised and splited X_train\n",
        "    y_train=y_new,  # Normalised and splited y_train\n",
        "    activation_fn=[\"sigmoid\"] * 3 + [\"softmax\"],  # 3 Hidden layers with Tanh, Softmax for output\n",
        "    layers_dimensions=[784, 128, 64, 10],  # Input layer (784), 3 hidden layers, output (10)\n",
        "    method=\"Xavier_U\",  # Initilise function of weight and biases for best result\n",
        "    batch_size=9000,  #batch size for better generalization\n",
        "    epochs=10,  # Train for 10 epochs\n",
        "    weight_decay=0,  # L2 Regularization to prevent overfitting\n",
        "    eta=0.001,  # Learning rate\n",
        "    beta=0.9,   #  beta 1\n",
        "    beta2=0.999,  #  beta 2\n",
        "    regularization_fn=\"L2\", #  L2 norm Regularization\n",
        "    grad_reglr_fn=\"L2_d\",   # gradient of L2 norm\n",
        "    validX_train=valX_new,  # Normalised and splited Validation X\n",
        "    validy_train=valy_new,  # Normalised and splited Validation y\n",
        "    use_wandb=False\n",
        ").apply_optimization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLY9VsxYLu_o",
        "outputId": "883ff784-4e80-44da-a596-fb69ce44de72"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Train Acc: 0 | Val Acc: 0 | Train Loss: 0\n",
            "Epoch 2/10 | Train Acc: 0.4039 | Val Acc: 0.436 | Train Loss: 0.0422\n",
            "Epoch 3/10 | Train Acc: 0.5036 | Val Acc: 0.5057 | Train Loss: 0.042\n",
            "Epoch 4/10 | Train Acc: 0.5076 | Val Acc: 0.4982 | Train Loss: 0.0428\n",
            "Epoch 5/10 | Train Acc: 0.5023 | Val Acc: 0.4917 | Train Loss: 0.043\n",
            "Epoch 6/10 | Train Acc: 0.501 | Val Acc: 0.4912 | Train Loss: 0.0431\n",
            "Epoch 7/10 | Train Acc: 0.5013 | Val Acc: 0.4907 | Train Loss: 0.0431\n",
            "Epoch 8/10 | Train Acc: 0.501 | Val Acc: 0.4903 | Train Loss: 0.0431\n",
            "Epoch 9/10 | Train Acc: 0.5006 | Val Acc: 0.4905 | Train Loss: 0.0431\n",
            "Epoch 10/10 | Train Acc: 0.5008 | Val Acc: 0.4892 | Train Loss: 0.0431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model3 = GiveOptimizers(\n",
        "    optimization_fn=\"rmsp\",  # Using \"adam\" or \"rmsp\" for best performance\n",
        "    loss_function=\"ce\",  # Cross-Entropy Loss\n",
        "    X_train=X_new,  # Normalised and splited X_train\n",
        "    y_train=y_new,  # Normalised and splited y_train\n",
        "    activation_fn=[\"sigmoid\"] * 3 + [\"softmax\"],  # 3 Hidden layers with Tanh, Softmax for output\n",
        "    layers_dimensions=[784, 128, 64, 10],  # Input layer (784), 3 hidden layers, output (10)\n",
        "    method=\"Xavier_U\",  # Initilise function of weight and biases for best result\n",
        "    batch_size=9000,  #batch size for better generalization\n",
        "    epochs=10,  # Train for 10 epochs\n",
        "    weight_decay=0,  # L2 Regularization to prevent overfitting\n",
        "    eta=0.001,  # Learning rate\n",
        "    beta=0.9,   #  beta 1\n",
        "    beta2=0.999,  #  beta 2\n",
        "    regularization_fn=\"L2\", #  L2 norm Regularization\n",
        "    grad_reglr_fn=\"L2_d\",   # gradient of L2 norm\n",
        "    validX_train=valX_new,  # Normalised and splited Validation X\n",
        "    validy_train=valy_new,  # Normalised and splited Validation y\n",
        "    use_wandb=False\n",
        ").apply_optimization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voLNsfLVN9dq",
        "outputId": "4a7c8ec4-0b9d-471b-a85e-8459393a048b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Train Acc: 0 | Val Acc: 0 | Train Loss: 0\n",
            "Epoch 2/10 | Train Acc: 0.4908 | Val Acc: 0.4887 | Train Loss: 0.0422\n",
            "Epoch 3/10 | Train Acc: 0.4878 | Val Acc: 0.4702 | Train Loss: 0.0431\n",
            "Epoch 4/10 | Train Acc: 0.4702 | Val Acc: 0.4527 | Train Loss: 0.0434\n",
            "Epoch 5/10 | Train Acc: 0.4572 | Val Acc: 0.4418 | Train Loss: 0.0435\n",
            "Epoch 6/10 | Train Acc: 0.4474 | Val Acc: 0.4342 | Train Loss: 0.0436\n",
            "Epoch 7/10 | Train Acc: 0.4423 | Val Acc: 0.427 | Train Loss: 0.0436\n",
            "Epoch 8/10 | Train Acc: 0.4374 | Val Acc: 0.4245 | Train Loss: 0.0435\n",
            "Epoch 9/10 | Train Acc: 0.4349 | Val Acc: 0.4215 | Train Loss: 0.0435\n",
            "Epoch 10/10 | Train Acc: 0.4326 | Val Acc: 0.421 | Train Loss: 0.0435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "dEda5peLQUML"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MFcqrHCsQbZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}