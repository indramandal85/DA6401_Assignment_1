{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "EuDCYeTFOWhL"
      },
      "id": "EuDCYeTFOWhL",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([[8,8,1],[7,9,1],[6,10,0],[5,5,0]], columns=['cgpa', 'profile_score', 'placed'])"
      ],
      "metadata": {
        "id": "fJ8rJlerOYGB"
      },
      "id": "fJ8rJlerOYGB",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['cgpa', 'profile_score']].values[0].reshape(2,1) # Shape(no of features, no. of training example)\n",
        "y = df[['placed']].values[0][0]"
      ],
      "metadata": {
        "id": "9724OEXSOX35"
      },
      "id": "9724OEXSOX35",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "aNIlLeY_gnbE",
        "outputId": "90dedc47-05f2-4e67-923a-6461c42ccfb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "aNIlLeY_gnbE",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8],\n",
              "       [8]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "IOeMXNH9g0hp",
        "outputId": "afa96990-0d4d-435a-be10-38bed79e3d63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IOeMXNH9g0hp",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This function calculates sigmoid activation function\n",
        "class sigmoid:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.use_sigmoid()\n",
        "\n",
        "    def use_sigmoid(self):\n",
        "        sig = np.where(self.m >= 0,1/(1 - np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) - 1))\n",
        "        return sig\n",
        "        #if self.m >= 0:\n",
        "            #return 1/(1 - np.exp(-self.m))\n",
        "        #else:\n",
        "            #return np.exp(self.m)/(np.exp(-self.m) - 1)\n",
        "\n",
        "\n",
        "# This function calculates tanh activation function\n",
        "class tanh:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.use_tanh()\n",
        "\n",
        "    def use_tanh(self):\n",
        "        z = (np.exp(self.m) - np.exp(-self.m))/(np.exp(self.m) + np.exp(-self.m))\n",
        "        return z\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates relu activation function\n",
        "class relu:\n",
        "\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "        self.use_relu()\n",
        "\n",
        "    def use_relu(self):\n",
        "        if np.all(self.m > 0):\n",
        "            return self.m\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates softmax activation function\n",
        "class softmax:\n",
        "\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "        self.use_softmax()\n",
        "\n",
        "    def use_softmax(self):\n",
        "        x = np.copy(self.m)\n",
        "        max_exp = np.max(x)\n",
        "\n",
        "        x = np.exp(x - max_exp)\n",
        "        x = x / np.sum(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# This function call different activation function\n",
        "class apply_activation:\n",
        "\n",
        "    def __init__(self, activation_function, value):\n",
        "        self.activation_function = activation_function.lower()\n",
        "        self.value = value\n",
        "        self.do_activation()\n",
        "\n",
        "\n",
        "    def do_activation(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.value).use_sigmoid()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.value).use_relu()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.value).use_tanh()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.value).use_softmax()\n",
        "\n",
        "\n",
        "\n",
        "class sigmoid_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.sigmoid_d()\n",
        "\n",
        "    def sigmoid_d(self):\n",
        "        sig_d = np.where(self.m >= 0,1/(1 - np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) - 1))\n",
        "        return sig_d * (1 - sig_d)\n",
        "\n",
        "\n",
        "\n",
        "class relu_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.relu_d()\n",
        "\n",
        "    def relu_d(self):\n",
        "        if self.m > 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "\n",
        "\n",
        "class tanh_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.tanh_d()\n",
        "\n",
        "    def tanh_d(self):\n",
        "        z = (np.exp(self.m) - np.exp(-self.m))/(np.exp(self.m) + np.exp(-self.m))\n",
        "        return (1 - (z)**2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class apply_activation_derivative:\n",
        "\n",
        "    def __init__(self,activation_function, value):\n",
        "        self.activation_function = activation_function.lower()\n",
        "        self.value = value\n",
        "        self.do_activation_derivative()\n",
        "\n",
        "    def do_activation_derivative(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid_derv(self.value).sigmoid_d()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu_derv(self.value).relu_d()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh_derv(self.value).tanh_d()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "01ZCcJdxviK6"
      },
      "id": "01ZCcJdxviK6",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class crossentropy:\n",
        "\n",
        "    def __init__(self, y, y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.give_celoss()\n",
        "\n",
        "    def give_celoss(self):\n",
        "        threshold = 10**(-8)\n",
        "        res = -np.sum(np.sum(self.y*np.log(self.y_predicted)))\n",
        "        return res\n",
        "\n",
        "class squarderror:\n",
        "\n",
        "    def __init__(self, y, y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.give_seloss()\n",
        "\n",
        "    def give_seloss(self):\n",
        "        res = np.sum((self.y - self.y_predicted)**2)\n",
        "        return np.sum(res)\n",
        "\n",
        "\n",
        "class callloss:\n",
        "\n",
        "    def __init__(self,loss_function, value1, value2):\n",
        "        self.loss_function = loss_function.lower()\n",
        "        self.value1 = value1\n",
        "        self.value2 = value2\n",
        "        self.give_loss()\n",
        "\n",
        "    def give_loss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return crossentropy(self.value1, self.value2).give_celoss()\n",
        "        if self.loss_function == 'se':\n",
        "            return squarderror(self.value1, self.value2).give_seloss()\n",
        "\n",
        "class crossentropy_grad:\n",
        "\n",
        "    def __init__(self,y ,y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.Give_cegrad()\n",
        "\n",
        "    def Give_cegrad(self):\n",
        "        grad = -self.y/(self.y_predicted)\n",
        "        return grad\n",
        "\n",
        "\n",
        "class squarederror_grad:\n",
        "\n",
        "    def __init__(self,y , y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.Give_segrad()\n",
        "\n",
        "    def Give_segrad(self):\n",
        "        res = -np.sum(self.y - self.y_predicted)\n",
        "        return res\n",
        "\n",
        "class call_lossgrad:\n",
        "\n",
        "    def __init__(self,loss_function, value1, value2):\n",
        "        self.loss_function = loss_function.lower()\n",
        "        self.value1 = value1\n",
        "        self.value2 = value2\n",
        "        self.give_gradloss()\n",
        "\n",
        "    def give_gradloss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return crossentropy_grad(self.value1, self.value2).Give_cegrad()\n",
        "        if self.loss_function == 'se':\n",
        "            return squarederror_grad(self.value1, self.value2).Give_segrad()"
      ],
      "metadata": {
        "id": "iA7A6vN-l5K1"
      },
      "id": "iA7A6vN-l5K1",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "895d572d-2ffd-466a-a260-e57859a6449e",
      "metadata": {
        "id": "895d572d-2ffd-466a-a260-e57859a6449e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Initilize:\n",
        "\n",
        "    def __init__(self, i_size, o_size, method = \"Xavier_U\"):\n",
        "        self.i_size = i_size\n",
        "        self.o_size = o_size\n",
        "        self.w, self.b = self.Init_weight(method)\n",
        "\n",
        "    def Init_weight(self, init_method):\n",
        "\n",
        "\n",
        "        if init_method == \"Xavier_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(1 / self.i_size)\n",
        "          w = np.random.randn(self.i_size,self.o_size)*a\n",
        "\n",
        "        elif init_method == \"Xavier_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / (self.o_size + self.i_size))\n",
        "          w = np.random.uniform((-a), a,( self.i_size, self.o_size))\n",
        "\n",
        "        elif init_method == \"He_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(2 / self.i_size)\n",
        "          w = np.random.randn(self.i_size, self.o_size)*a\n",
        "\n",
        "        elif init_method == \"He_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / self.i_size)\n",
        "          w = np.random.uniform(-a, a, (self.i_size, self.o_size))\n",
        "\n",
        "        elif init_method == \"Random\":\n",
        "          np.random.seed(0)\n",
        "          w = np.random.randn(self.i_size, self.o_size)*0.01\n",
        "\n",
        "        b = np.zeros((self.o_size,1))\n",
        "\n",
        "        return w, b\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Weight_bias:\n",
        "  def __init__(self, layer_dimension, activation_fn, method):\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.method = method\n",
        "\n",
        "  def Init_network(self, layer_dimension):\n",
        "    self.n = layer_dimension\n",
        "    self.network = {}\n",
        "\n",
        "    for i in range(1, len(self.n)):\n",
        "      self.network[\"w\" + str(i)] = Initilize(self.n[i-1], self.n[i], self.method).Init_weight(self.method)[0]\n",
        "      self.network[\"b\" + str(i)] = Initilize(self.n[i-1], self.n[i], self.method).Init_weight(self.method)[1]\n",
        "      self.network [\"h\"+ str(i)] = str(self.activation_fn[i-1])\n",
        "      self.network [\"neuronns in layer \"+ str(i)] = self.n[i]\n",
        "\n",
        "    return self.network\n",
        "\n",
        "\n",
        "\n",
        "class Pre_Feedforward:\n",
        "\n",
        "  def __init__(self, Prev_layer_H, w, b, activation_fn):\n",
        "    self.Prev_layer_H= Prev_layer_H\n",
        "    self.w = w\n",
        "    self.b = b\n",
        "    self.activation_fn = activation_fn\n",
        "    self.Preactivation_cal()\n",
        "\n",
        "\n",
        "  def Preactivation_cal(self):\n",
        "\n",
        "    A = np.dot(self.w.T, self.Prev_layer_H) + self.b\n",
        "    H = apply_activation(self.activation_fn, A).do_activation()\n",
        "    cache = (self.Prev_layer_H, A)\n",
        "    #self.cache[\"Input\"] = self.Prev_layer_H\n",
        "    #self.cache[\"Pre_act\"] = H\n",
        "    return cache , H\n",
        "\n",
        "class Feedforward:\n",
        "\n",
        "  def __init__(self,X_train, layer_dimension, activation_fn, method):\n",
        "    self.X_train = X_train\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.method = method\n",
        "\n",
        "\n",
        "  def Forward_prop(self):\n",
        "    network = Weight_bias(self.n, self.activation_fn,self.method).Init_network(self.n)\n",
        "    H = self.X_train\n",
        "    L = len(self.n) - 1\n",
        "    Inter_layers = {}\n",
        "\n",
        "    for i in range(1,L+1):\n",
        "      Prev_layer_H = H\n",
        "\n",
        "      w_l = network[\"w\"+  str(i)]\n",
        "      b_l = network[\"b\"+  str(i)]\n",
        "      Act_fn_l = network[\"h\" + str(i)]\n",
        "\n",
        "      cache, H = Pre_Feedforward(Prev_layer_H, w_l,b_l,Act_fn_l).Preactivation_cal()\n",
        "\n",
        "\n",
        "      #A = np.dot(w_l.T, Prev_layer_H) + b_l\n",
        "      #H = apply_activation(Act_fn_l, A).do_activation()\n",
        "\n",
        "      Inter_layers[\"h\" + str(i-1)] = cache[0]\n",
        "      Inter_layers[\"a\" + str(i)] = cache[1]\n",
        "      #cache.append((Prev_layer_H, A))\n",
        "\n",
        "      #cache , New_H = Pre_Feedforward(Prev_layer_H, w_l, b_l, Act_fn_l).Preactivation_cal()\n",
        "      #cache.appand()\n",
        "\n",
        "    return cache,Inter_layers,network , H\n",
        "\n",
        "\n",
        "class Findloss:\n",
        "  def __init__(self,X_train, y_train, layer_dimension, activation_fn, method, loss_function):\n",
        "    self.X_train = X_train\n",
        "    self.y_train = y_train\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.loss_function = loss_function\n",
        "    self.method = method\n",
        "    self.y_pred = Feedforward(self.X_train, self.n, self.activation_fn, self.method).Forward_prop()[-1]\n",
        "\n",
        "  def loss(self):\n",
        "    return callloss(self.loss_function, self.y_train, self.y_pred).give_loss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Backprop:\n",
        "\n",
        "  def __init__(self, X_train ,y_train, layer_dimension,activation_fn,grad_activation_fn, method,loss_function, grad_loss_function):\n",
        "\n",
        "    self.X_train = X_train\n",
        "    self.y_train = y_train\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.grad_activation_fn = grad_activation_fn\n",
        "    self.loss_function = loss_function\n",
        "    self.method = method\n",
        "    self.grad_loss_function = grad_loss_function\n",
        "\n",
        "  def Backpropagation(self):\n",
        "\n",
        "    y_pred = Feedforward(self.X_train, self.n, self.activation_fn, self.method).Forward_prop()[-1]\n",
        "    loss = Findloss(self.X_train, self.y_train, self.n, self.activation_fn, self.method, self.loss_function).loss()\n",
        "\n",
        "    L = len(self.n)\n",
        "\n",
        "    grad_L_yhat = call_lossgrad(self.loss_function, y_train, y_pred).give_gradloss()\n",
        "    grad_yhat_a_L = grad_L_yhat *\n",
        "\n",
        "    for i in range()\n",
        "\n"
      ],
      "metadata": {
        "id": "MhJrZkPpp7Zv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "308b8c2a-ab16-4a5c-fc50-fcee76f63f69"
      },
      "id": "MhJrZkPpp7Zv",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "expected ':' (<ipython-input-56-fcc5a8ffadee>, line 16)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-56-fcc5a8ffadee>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    for i in range()\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Findloss(X,y,[2,3,2,1], ['sigmoid','sigmoid','softmax'],\"Xavier_U\",\"se\").loss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PFRmUzwVmaO",
        "outputId": "4e277771-bd11-4980-b4d9-2bb980280d14"
      },
      "id": "3PFRmUzwVmaO",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "lhnh7_96ftQs",
        "outputId": "e7be339f-eba4-426e-ec42-14766c680488",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lhnh7_96ftQs",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Feedforward(X,[2,3,1],['sigmoid','sigmoid','sigmoid','softmax'],\"Xavier_U\").Forward_prop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV7gYASA0m8t",
        "outputId": "4f3b3d39-1498-490e-b54c-19d2bbf7fcbb"
      },
      "id": "NV7gYASA0m8t",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([[1.23999836],\n",
              "         [1.09616172],\n",
              "         [1.01296675]]),\n",
              "  array([[0.98103753]])),\n",
              " {'h0': array([[8],\n",
              "         [8]]),\n",
              "  'a1': array([[1.64223325],\n",
              "         [2.43353861],\n",
              "         [4.35825011]]),\n",
              "  'h1': array([[1.23999836],\n",
              "         [1.09616172],\n",
              "         [1.01296675]]),\n",
              "  'a2': array([[0.98103753]])},\n",
              " {'w1': array([[ 0.10694503,  0.47145628,  0.22514328],\n",
              "         [ 0.09833413, -0.16726395,  0.31963799]]),\n",
              "  'b1': array([[0.],\n",
              "         [0.],\n",
              "         [0.]]),\n",
              "  'h1': 'sigmoid',\n",
              "  'neuronns in layer 1': 3,\n",
              "  'w2': array([[0.11956818],\n",
              "         [0.52710415],\n",
              "         [0.25171784]]),\n",
              "  'b2': array([[0.]]),\n",
              "  'h2': 'sigmoid',\n",
              "  'neuronns in layer 2': 1},\n",
              " array([[1.5998001]]))"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2064d83c-b27e-46e6-bef2-c24d9faf70ff",
      "metadata": {
        "id": "2064d83c-b27e-46e6-bef2-c24d9faf70ff"
      },
      "outputs": [],
      "source": [
        "Initilize(3,5,\"Xavier_N\").Init_weight(\"Xavier_N\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [relu,relu,sigmoid]\n",
        "print(x[1])"
      ],
      "metadata": {
        "id": "AIOXHXcuRF9-"
      },
      "id": "AIOXHXcuRF9-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weight_bias([4,2,3,1],['relu','relu','sigmoid'],\"Xavier_N\").Init_network([4,2,3,1])"
      ],
      "metadata": {
        "id": "4NKWIZNDdN7u"
      },
      "id": "4NKWIZNDdN7u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eAhzWxfwt_ub"
      },
      "id": "eAhzWxfwt_ub",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}