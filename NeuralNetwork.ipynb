{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "EuDCYeTFOWhL"
      },
      "id": "EuDCYeTFOWhL",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([[8,8,1],[7,9,1],[6,10,0],[5,5,0]], columns=['cgpa', 'profile_score', 'placed'])\n",
        "X = df[['cgpa', 'profile_score']].values[0].reshape(2,1) # Shape(no of features, no. of training example)\n",
        "y = df[['placed']].values[0][0]"
      ],
      "metadata": {
        "id": "H2q--JEjvU-Q"
      },
      "id": "H2q--JEjvU-Q",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNIlLeY_gnbE",
        "outputId": "804c26e7-9a92-4a74-ad2f-6874bfadaddb"
      },
      "id": "aNIlLeY_gnbE",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62rXWMGhyBbp",
        "outputId": "1e4b85c5-9157-4c31-9293-c672896c2e79"
      },
      "id": "62rXWMGhyBbp",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47040000"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy  as np\n",
        "\n",
        "class OneHotEncoder:\n",
        "\n",
        "    def __init__(self,x, y):\n",
        "        self.y = y\n",
        "        self.x = x\n",
        "        #self.num_class = num_class\n",
        "        self.onehot_encode()\n",
        "\n",
        "    def onehot_encode(self):\n",
        "        onehot = np.zeros((self.x.shape[0], 10))\n",
        "\n",
        "        for i, j in zip(range(len(self.x)), self.y):\n",
        "            onehot[i, j] = 1\n",
        "        return onehot.T\n",
        "\n",
        "class Normalize:\n",
        "\n",
        "    def __init__(self, unprocessed_X):\n",
        "        self.unprocessed_X = unprocessed_X\n",
        "        self.Norm_reshape()\n",
        "\n",
        "    def Norm_reshape(self):\n",
        "        X_norm = np.reshape(self.unprocessed_X,(self.unprocessed_X.shape[0],784)).T/255\n",
        "        X_norm= np.array(X_norm)\n",
        "\n",
        "        return X_norm"
      ],
      "metadata": {
        "id": "BW34ZLm_wv9o"
      },
      "id": "BW34ZLm_wv9o",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new= Normalize(X_train).Norm_reshape()"
      ],
      "metadata": {
        "id": "RUTyqFl2zbjL"
      },
      "id": "RUTyqFl2zbjL",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new= Normalize(X_train).Norm_reshape()\n",
        "y_new =OneHotEncoder(X_train,y_train).onehot_encode()"
      ],
      "metadata": {
        "id": "K4hGhlz9w6pa"
      },
      "id": "K4hGhlz9w6pa",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOeMXNH9g0hp",
        "outputId": "8bedb42a-7d02-475a-8de8-a52f82afb11f"
      },
      "id": "IOeMXNH9g0hp",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# This function calculates sigmoid activation function\n",
        "class sigmoid:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.result =self.use_sigmoid()\n",
        "\n",
        "    def use_sigmoid(self):\n",
        "        sig = np.where(self.m >= 0,1/(1 - np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) - 1))\n",
        "        return sig\n",
        "        #if self.m >= 0:\n",
        "            #return 1/(1 - np.exp(-self.m))\n",
        "        #else:\n",
        "            #return np.exp(self.m)/(np.exp(-self.m) - 1)\n",
        "\n",
        "\n",
        "# This function calculates tanh activation function\n",
        "class tanh:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.result =self.use_tanh()\n",
        "\n",
        "    def use_tanh(self):\n",
        "        z = (np.exp(self.m) - np.exp(-self.m))/(np.exp(self.m) + np.exp(-self.m))\n",
        "        return z\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates relu activation function\n",
        "class relu:\n",
        "\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "        self.result =self.use_relu()\n",
        "\n",
        "    def use_relu(self):\n",
        "        return np.where(self.m > 0, self.m, 0)\n",
        "        #     return self.m\n",
        "        # else:\n",
        "        #     return 0\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates softmax activation function\n",
        "class softmax:\n",
        "\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "        self.result =self.use_softmax()\n",
        "\n",
        "    def use_softmax(self):\n",
        "        x = np.copy(self.m)\n",
        "        max_exp = np.max(x)\n",
        "\n",
        "        x = np.exp(x - max_exp)\n",
        "        x = x / np.sum(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# This function call different activation function\n",
        "class apply_activation:\n",
        "\n",
        "    def __init__(self, activation_function, value):\n",
        "        self.activation_function = activation_function.lower()\n",
        "        self.value = value\n",
        "        self.result = self.do_activation()\n",
        "\n",
        "\n",
        "    def do_activation(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.value).use_sigmoid()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.value).use_relu()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.value).use_tanh()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.value).use_softmax()\n",
        "\n",
        "\n",
        "\n",
        "class sigmoid_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.result = self.sigmoid_d()\n",
        "\n",
        "    def sigmoid_d(self):\n",
        "        sig_d = 1 / (1 + np.exp(-self.m))  # Compute sigmoid\n",
        "        # sig_d = np.where(self.m >= 0,1/(1 + np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) + 1))\n",
        "        return sig_d * (1 - sig_d)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class relu_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.result = self.relu_d()\n",
        "\n",
        "    def relu_d(self):\n",
        "        return np.where(self.m >0 , 1 ,0)\n",
        "\n",
        "\n",
        "\n",
        "class tanh_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.result =self.tanh_d()\n",
        "\n",
        "    def tanh_d(self):\n",
        "        z = (np.exp(self.m) - np.exp(-self.m))/(np.exp(self.m) + np.exp(-self.m))\n",
        "        return (1 - (z)**2)\n",
        "\n",
        "\n",
        "class softmax_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.result =self.softmax_d()\n",
        "\n",
        "    def softmax_d(self):\n",
        "        z=self.m - np.max(self.m,axis=0)\n",
        "        soft=np.exp(z)/np.sum(np.exp(z),axis=0)\n",
        "        return soft*(1-soft)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class apply_activation_derivative:\n",
        "\n",
        "    def __init__(self,activation_function, value):\n",
        "        self.activation_function= activation_function\n",
        "        self.value = value\n",
        "        self.do_activation_derivative()\n",
        "\n",
        "    def do_activation_derivative(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid_derv(self.value).sigmoid_d()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu_derv(self.value).relu_d()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh_derv(self.value).tanh_d()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax_derv(self.value).softmax_d()\n",
        "\n",
        "        # else:\n",
        "        #   raise ValueError(\"Unknown activation function\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "01ZCcJdxviK6"
      },
      "id": "01ZCcJdxviK6",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class crossentropy:\n",
        "\n",
        "    def __init__(self, y, y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.give_celoss()\n",
        "\n",
        "    def give_celoss(self):\n",
        "        threshold = 10**(-8)\n",
        "        res = -np.sum(np.sum(self.y*np.log(self.y_predicted)))\n",
        "        return res\n",
        "\n",
        "class squarderror:\n",
        "\n",
        "    def __init__(self, y, y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.give_seloss()\n",
        "        if self.y.shape != self.y_predicted.shape:\n",
        "          raise ValueError(f\"Shape mismatch: y shape is {self.y.shape} and y_predicted shape is {self.y_predicted.shape}\")\n",
        "\n",
        "    def give_seloss(self):\n",
        "      if self.y_predicted.shape != self.y.shape:\n",
        "        self.y_predicted = self.y_predicted.T\n",
        "        res = np.sum((self.y - self.y_predicted)**2)\n",
        "        return np.sum(res)\n",
        "\n",
        "\n",
        "class callloss:\n",
        "\n",
        "    def __init__(self,loss_function, value1, value2):\n",
        "        self.loss_function = loss_function.lower()\n",
        "        self.value1 = value1\n",
        "        self.value2 = value2\n",
        "        self.give_loss()\n",
        "\n",
        "    def give_loss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return crossentropy(self.value1, self.value2).give_celoss()\n",
        "        if self.loss_function == 'se':\n",
        "            return squarderror(self.value1, self.value2).give_seloss()\n",
        "\n",
        "class crossentropy_grad:\n",
        "\n",
        "    def __init__(self,y ,y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.Give_cegrad()\n",
        "\n",
        "    def Give_cegrad(self):\n",
        "        grad = -self.y/(self.y_predicted)\n",
        "        return grad\n",
        "\n",
        "\n",
        "class squarederror_grad:\n",
        "\n",
        "    def __init__(self,y , y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.Give_segrad()\n",
        "\n",
        "    def Give_segrad(self):\n",
        "        res = -(self.y - self.y_predicted)\n",
        "        return res\n",
        "\n",
        "class call_lossgrad:\n",
        "\n",
        "    def __init__(self,loss_function, value1, value2):\n",
        "        self.loss_function = loss_function.lower()\n",
        "        self.value1 = value1\n",
        "        self.value2 = value2\n",
        "        self.give_gradloss()\n",
        "\n",
        "    def give_gradloss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return crossentropy_grad(self.value1, self.value2).Give_cegrad()\n",
        "        if self.loss_function == 'se':\n",
        "            return squarederror_grad(self.value1, self.value2).Give_segrad()"
      ],
      "metadata": {
        "id": "iA7A6vN-l5K1"
      },
      "id": "iA7A6vN-l5K1",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "895d572d-2ffd-466a-a260-e57859a6449e",
      "metadata": {
        "id": "895d572d-2ffd-466a-a260-e57859a6449e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Initilize:\n",
        "\n",
        "    def __init__(self, i_size, o_size, method = \"Xavier_U\"):\n",
        "        self.i_size = i_size\n",
        "        self.o_size = o_size\n",
        "        self.w, self.b = self.Init_weight(method)\n",
        "\n",
        "    def Init_weight(self, init_method):\n",
        "\n",
        "\n",
        "        if init_method == \"Xavier_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(1 / self.i_size)\n",
        "          w = np.random.randn(self.i_size,self.o_size)*a\n",
        "\n",
        "        elif init_method == \"Xavier_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / (self.o_size + self.i_size))\n",
        "          w = np.random.uniform((-a), a,( self.i_size, self.o_size))\n",
        "\n",
        "        elif init_method == \"He_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(2 / self.i_size)\n",
        "          w = np.random.randn(self.i_size, self.o_size)*a\n",
        "\n",
        "        elif init_method == \"He_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / self.i_size)\n",
        "          w = np.random.uniform(-a, a, (self.i_size, self.o_size))\n",
        "\n",
        "        elif init_method == \"Random\":\n",
        "          np.random.seed(0)\n",
        "          w = np.random.randn(self.i_size, self.o_size)*0.01\n",
        "\n",
        "        b = np.zeros((self.o_size,1))\n",
        "\n",
        "        return w, b\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Weight_bias:\n",
        "  def __init__(self, layer_dimension, activation_fn, method):\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.method = method\n",
        "\n",
        "  def Init_network(self, layer_dimension):\n",
        "    self.n = layer_dimension\n",
        "    self.network = {}\n",
        "\n",
        "    for i in range(1, len(self.n)):\n",
        "      self.network[\"w\" + str(i)] = Initilize(self.n[i-1], self.n[i], self.method).Init_weight(self.method)[0]\n",
        "      self.network[\"b\" + str(i)] = Initilize(self.n[i-1], self.n[i], self.method).Init_weight(self.method)[1]\n",
        "      self.network [\"h\"+ str(i)] = str(self.activation_fn[i-1])\n",
        "      self.network [\"neuronns in layer \"+ str(i)] = self.n[i]\n",
        "\n",
        "    return self.network\n",
        "\n",
        "\n",
        "\n",
        "class Pre_Feedforward:\n",
        "\n",
        "  def __init__(self, Prev_layer_H, w, b, activation_fn):\n",
        "    self.Prev_layer_H= Prev_layer_H\n",
        "    self.w = w\n",
        "    self.b = b\n",
        "    self.activation_fn = activation_fn\n",
        "    self.Preactivation_cal()\n",
        "\n",
        "\n",
        "  def Preactivation_cal(self):\n",
        "    self.cache = {}\n",
        "\n",
        "    A = np.dot(self.w.T, self.Prev_layer_H) + self.b\n",
        "    H = apply_activation(self.activation_fn, A).do_activation()\n",
        "    cache = (self.Prev_layer_H, A)\n",
        "    #self.cache[\"Input\"] = self.Prev_layer_H\n",
        "    #self.cache[\"Pre_act\"] = H\n",
        "    return cache , H\n",
        "\n",
        "class Feedforward:\n",
        "\n",
        "  def __init__(self,X_train, layer_dimension, activation_fn, method):\n",
        "    self.X_train = X_train\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.method = method\n",
        "\n",
        "\n",
        "  def Forward_prop(self):\n",
        "    network = Weight_bias(self.n, self.activation_fn,self.method).Init_network(self.n)\n",
        "    H = self.X_train\n",
        "    L = len(self.n) -1\n",
        "    Inter_layers = {}\n",
        "\n",
        "    for i in range(1,L+1):\n",
        "      Prev_layer_H = H\n",
        "\n",
        "      w_l = network[\"w\"+  str(i)]\n",
        "      b_l = network[\"b\"+  str(i)]\n",
        "      Act_fn_l = network[\"h\" + str(i)]\n",
        "\n",
        "      cache, H = Pre_Feedforward(Prev_layer_H, w_l,b_l,Act_fn_l).Preactivation_cal()\n",
        "\n",
        "\n",
        "      #A = np.dot(w_l.T, Prev_layer_H) + b_l\n",
        "      #H = apply_activation(Act_fn_l, A).do_activation()\n",
        "\n",
        "      Inter_layers[\"h\" + str(i-1)] = cache[0]\n",
        "      Inter_layers[\"a\" + str(i)] = cache[1]\n",
        "      #cache.append((Prev_layer_H, A))\n",
        "\n",
        "      #cache , New_H = Pre_Feedforward(Prev_layer_H, w_l, b_l, Act_fn_l).Preactivation_cal()\n",
        "      #cache.appand()\n",
        "\n",
        "    return cache,Inter_layers,network , H\n",
        "\n",
        "\n",
        "class Findloss:\n",
        "  def __init__(self,X_train, y_train, layer_dimension, activation_fn, method, loss_function):\n",
        "    self.X_train = X_train\n",
        "    self.y_train = y_train\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.loss_function = loss_function\n",
        "    self.method = method\n",
        "    self.y_pred = Feedforward(self.X_train, self.n, self.activation_fn, self.method).Forward_prop()[-1]\n",
        "\n",
        "  def loss(self):\n",
        "    return callloss(self.loss_function, self.y_train, self.y_pred).give_loss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c,d = Feedforward(X,[2,4,3,1],['sigmoid','relu','softmax'],\"Xavier_U\").Forward_prop()\n",
        "a,b,c,d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeaxRE4jdJ5C",
        "outputId": "098ebe9c-d01c-479a-bb38-96d82e21b0f3"
      },
      "id": "IeaxRE4jdJ5C",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([[0.        ],\n",
              "         [2.38736017],\n",
              "         [2.35077871]]),\n",
              "  array([[1.85012037]])),\n",
              " {'h0': array([[8],\n",
              "         [8]]),\n",
              "  'a1': array([[-0.44050715],\n",
              "         [ 5.77733567],\n",
              "         [ 0.6456094 ],\n",
              "         [ 6.98649894]]),\n",
              "  'h1': array([[1.1629916 ],\n",
              "         [1.00310658],\n",
              "         [2.10235488],\n",
              "         [1.00092513]]),\n",
              "  'a2': array([[-0.2705021 ],\n",
              "         [ 2.38736017],\n",
              "         [ 2.35077871]]),\n",
              "  'h2': array([[0.        ],\n",
              "         [2.38736017],\n",
              "         [2.35077871]]),\n",
              "  'a3': array([[1.85012037]])},\n",
              " {'w1': array([[ 0.09762701,  0.43037873,  0.20552675,  0.08976637],\n",
              "         [-0.1526904 ,  0.29178823, -0.12482558,  0.783546  ]]),\n",
              "  'b1': array([[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]]),\n",
              "  'h1': 'sigmoid',\n",
              "  'neuronns in layer 1': 4,\n",
              "  'w2': array([[ 0.09038505,  0.39845328,  0.1902808 ],\n",
              "         [ 0.08310751, -0.14136384,  0.2701434 ],\n",
              "         [-0.11556603,  0.72542264,  0.85853661],\n",
              "         [-0.21582437,  0.54016981,  0.05350299]]),\n",
              "  'b2': array([[0.],\n",
              "         [0.],\n",
              "         [0.]]),\n",
              "  'h2': 'relu',\n",
              "  'neuronns in layer 2': 3,\n",
              "  'w3': array([[0.11956818],\n",
              "         [0.52710415],\n",
              "         [0.25171784]]),\n",
              "  'b3': array([[0.]]),\n",
              "  'h3': 'softmax',\n",
              "  'neuronns in layer 3': 1},\n",
              " array([[1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        for param, grad in zip(params, grads):\n",
        "            param -= self.lr * grad\n",
        "\n",
        "class Momentum:\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.velocities = None\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.velocities is None:\n",
        "            self.velocities = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.velocities[i] = self.momentum * self.velocities[i] - self.lr * grad\n",
        "            param += self.velocities[i]\n",
        "\n",
        "\n",
        "class Adagrad:\n",
        "    def __init__(self, lr=0.01, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "        self.cache = None\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.cache is None:\n",
        "            self.cache = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.cache[i] += grad ** 2\n",
        "            param -= self.lr * grad / (np.sqrt(self.cache[i]) + self.epsilon)\n",
        "\n",
        "\n",
        "class RMSProp:\n",
        "    def __init__(self, lr=0.001, beta=0.9, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "        self.cache = None\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.cache is None:\n",
        "            self.cache = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.cache[i] = self.beta * self.cache[i] + (1 - self.beta) * grad ** 2\n",
        "            param -= self.lr * grad / (np.sqrt(self.cache[i]) + self.epsilon\n",
        "\n",
        "\n",
        "\n",
        "class Adam:\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        self.t = 0\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m = [np.zeros_like(param) for param in params]\n",
        "            self.v = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        self.t += 1\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n",
        "\n",
        "            # Bias correction\n",
        "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
        "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon\n",
        "\n",
        "\n",
        "class Nadam:\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        self.t = 0\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m = [np.zeros_like(param) for param in params]\n",
        "            self.v = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        self.t += 1\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            # Update biased moment estimates\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n",
        "\n",
        "            # Compute bias-corrected estimates with Nesterov acceleration\n",
        "            m_hat = (self.beta1 * self.m[i] / (1 - self.beta1 ** (self.t + 1)) +\n",
        "                    ((1 - self.beta1) * grad / (1 - self.beta1 ** self.t))\n",
        "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "ynXoTieZS-7O",
        "outputId": "5df398c1-8bf3-4ca2-9f83-7d93919255df"
      },
      "id": "ynXoTieZS-7O",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-55-0ba97b7d964c>, line 56)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-55-0ba97b7d964c>\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    class Adam:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Regularization Code\n",
        "\n",
        "class L2_regularisation:\n",
        "\n",
        "  def __init__(self,layers_dimensions , network):\n",
        "    self.layers_dimensions = layers_dimensions\n",
        "    sellf.network = network\n",
        "\n",
        "  def Apply_L2(self):\n",
        "    # Function that returns L2 regularisation loss for the given network\n",
        "    L = len(self.layers_dimensions)\n",
        "    res = 0\n",
        "    for j in range(L):\n",
        "      ans += np.sum(self.network[\"w\" + str(j)] ** 2)\n",
        "\n",
        "    return res\n",
        "\n",
        "class L1_regularisation:\n",
        "\n",
        "  def __init__(self,layers_dimensions , network):\n",
        "    self.layers_dimensions = layers_dimensions\n",
        "    sellf.network = network\n",
        "\n",
        "  def Apply_L1(self):\n",
        "    # Function that returns L1 regularisation loss for the given network\n",
        "    L = len(self.layers_dimensions)\n",
        "    res = 0\n",
        "    for j in range(L):\n",
        "      res += np.sum(np.absolute(self.network[\"w\" + str(j)]))\n",
        "\n",
        "    return res\n",
        "\n",
        "class L2_regularisation_grad:\n",
        "\n",
        "  def __init__(self, weight):\n",
        "    sellf.weight = weight\n",
        "\n",
        "  def Apply_L2_grad(self):\n",
        "    # Function that returns L2 regularisation gradient for the given Weight matrix / tensor\n",
        "    return 2 * self.weight\n",
        "\n",
        "class L1_regularisation_grad:\n",
        "\n",
        "  def __init__(self, weight):\n",
        "    sellf.weight = weight\n",
        "\n",
        "  def Apply_L1_grad(self):\n",
        "    # Function that returns L1 regularisation gradient for the given Weight matrix / tensor\n",
        "    return np.sign(self.weight)\n",
        "\n",
        "class ApplyReg:\n",
        "\n",
        "    def __init__(self,reg_function, layer_dimension, network):\n",
        "        self.reg_function= reg_function\n",
        "        self.layer_dimension = layer_dimension\n",
        "        self.network = network\n",
        "        self.do_reg()\n",
        "\n",
        "    def do_reg(self):\n",
        "        if self.reg_function == 'L2':\n",
        "            return L2_regularisation(self.layer_dimension, self.network).Apply_L2()\n",
        "        if self.reg_function == 'L1':\n",
        "            return L1_regularisation(self.layer_dimension, self.network).Apply_L1()\n",
        "\n",
        "class ApplyRegDerv:\n",
        "\n",
        "    def __init__(self,reg_function, weight):\n",
        "        self.reg_function= reg_function\n",
        "        self.weight = weight\n",
        "        self.do_reg_derv()\n",
        "\n",
        "    def do_reg_derv(self):\n",
        "        if self.reg_function == 'L2':\n",
        "            return L2_regularisation_grad(self.weight).Apply_L2_grad()\n",
        "        if self.reg_function == 'L1':\n",
        "            return L1_regularisation_grad(self.weight).Apply_L1_grad()"
      ],
      "metadata": {
        "id": "H0NBqkSyhNWW"
      },
      "id": "H0NBqkSyhNWW",
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, learning_rate=0.01, decay=0.0):\n",
        "        self.eta = learning_rate  # Learning rate\n",
        "        self.decay = decay        # Learning rate decay\n",
        "        self.iterations = 0       # Track steps for decay\n",
        "\n",
        "    def update(self, weights, gradients):\n",
        "        \"\"\"Update weights using gradients.\"\"\"\n",
        "        weights -= self.eta * gradients  # Core SGD update rule\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Decay learning rate after each epoch (optional).\"\"\"\n",
        "        if self.decay > 0:\n",
        "            self.eta = self.eta / (1 + self.decay * self.iterations)\n",
        "            self.iterations += 1"
      ],
      "metadata": {
        "id": "mQDo0hSwfYx4"
      },
      "id": "mQDo0hSwfYx4",
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "class Backpropagation:\n",
        "\n",
        "      def __init__(self, loss_function,X_train , y_train, activation_fn , layers_dimensions ,method, batch_size, optimizer_fn, epochs,weight_decay,grad_reglr_fn = None ,X_val=None, t_val=None, use_wandb=False, optim_params=None):\n",
        "\n",
        "        self.loss_function = loss_function\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.activation_fn = activation_fn\n",
        "        self.method = method\n",
        "        self.n = layers_dimensions\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.optimizer_fn = optimizer_fn\n",
        "        self.weight_decay = weight_decay\n",
        "        self.grad_reglr_fn = grad_reglr_fn\n",
        "        self.X_norm= Normalize(self.X_train).Norm_reshape()\n",
        "        self.y_norm =OneHotEncoder(self.X_train, self.y_train).onehot_encode()\n",
        "        self.batches_number = self.X_norm.shape[1]//self.batch_size\n",
        "        cache, Inter_layers, network, H = Feedforward(self.X_norm,self.n,self.activation_fn,self.method).Forward_prop()\n",
        "        self.inter_layer = Inter_layers\n",
        "        self.H = H\n",
        "        self.network = network\n",
        "        print(\"X_norm shape \",self.X_norm.shape)\n",
        "        print(\"y_norm shape \",self.y_norm.shape)\n",
        "        print(\"H shape \",self.H.shape)\n",
        "        print(\"length self.network \",len(self.network))\n",
        "        print(\"self.batch_no \",self.batches_number)\n",
        "        print(\"self.batch_size \",self.batch_size)\n",
        "\n",
        "        #self.loss = map_losses[loss]\n",
        "        # self.use_wandb = use_wandb\n",
        "        # if t_val is not None:\n",
        "        #     self.X_val = X_val\n",
        "        #     self.layers[0].a_val = X_val\n",
        "        #     self.t_val = t_val\n",
        "        #self.param_init(optimizer, optim_params)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      def backward_propagation(self):\n",
        "        L = len(self.n)\n",
        "        self.y_predicted = self.H\n",
        "        # Initialize variables neesed to keep track of loss\n",
        "        self.eta_hist = []\n",
        "        self.loss_hist = []\n",
        "        self.accuracy_hist = []\n",
        "        self.loss_hist_val = []\n",
        "        self.accuracy_hist_val = []\n",
        "        self.loss = callloss(self.loss_function, self.y_norm.T, self.y_predicted).give_loss()\n",
        "\n",
        "        # Perform Backprop\n",
        "        for epochs in tqdm(range(self.epochs)):\n",
        "\n",
        "          for batch in tqdm(range(self.batches_number)):\n",
        "\n",
        "            X_batch = self.X_norm[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "            self.X_batch = X_batch\n",
        "            print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "            cache, self.inter_layers, self.network, self.H = Feedforward(self.X_batch,self.n,self.activation_fn,self.method).Forward_prop()\n",
        "            self.y_predicted = self.H\n",
        "            print(\"y_predicted shape : \" , self.y_predicted.shape)\n",
        "\n",
        "            y_true_batch = self.y_norm[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "            # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "            self.y_true_batch = y_true_batch\n",
        "            self.y_pred_batch = self.y_predicted\n",
        "            print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "            print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "            assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "\n",
        "            grad_a_k_L = call_lossgrad(self.loss_function,self.y_true_batch, self.y_pred_batch).give_gradloss()\n",
        "            print(\"grad_a_k_L : \" , grad_a_k_L.shape)\n",
        "\n",
        "            # Initialising gradients to be calculated in the loop below\n",
        "            grad_w_L = [np.zeros_like(self.network[\"w\" + str(k)]) for k in range(1,L)]\n",
        "            grad_b_L = [np.zeros_like(self.network[\"b\" + str(k)]) for k in range(1,L)]\n",
        "            grad_h_prev_L, grad_a_prev_L = 0, 0\n",
        "\n",
        "\n",
        "            for k in range(L-1,0,-1):\n",
        "              print(\"grad_a_k_l : \" , grad_a_k_L.shape)\n",
        "              A_k = self.inter_layer[\"a\" + str(k)][:, batch*self.batch_size : (batch+1)*self.batch_size]\n",
        "              print(\"A_K : \" , A_k.shape)\n",
        "              do_derivation = apply_activation_derivative(self.activation_fn[k-1], A_k).do_activation_derivative()\n",
        "              print(\"do derivation : \" , do_derivation.shape)\n",
        "\n",
        "              delta_K = grad_a_k_L * do_derivation\n",
        "              delta_K = np.clip(delta_K, -1e5, 1e5)\n",
        "              print(\"delta k : \" , delta_K.shape)\n",
        "\n",
        "\n",
        "\n",
        "              # Gradients wrt Weights (W_k)\n",
        "\n",
        "              A_prev = self.inter_layer[\"h\" + str(k-1)][:, batch*self.batch_size : (batch+1)*self.batch_size]\n",
        "              print(\"A prev : \" , A_prev.shape)\n",
        "              grad_w_L[k - 1] = (delta_K @ A_prev.T) / self.batch_size #+ self.weight_decay * self.grad_reglr_fn(self.network[\"w\" + str(k)])\n",
        "              print(\"len Grad_w_L : \" , grad_w_L[k - 1].shape)\n",
        "\n",
        "              # Gradients wrt Biases (b_k)\n",
        "              grad_b_L[k - 1] = np.sum(delta_K, axis=1, keepdims= True) / self.batch_size\n",
        "              print(\"shape Grad_b_L : \" , grad_b_L[k - 1].shape)\n",
        "              # Gradients wrt Biases (b_k)\n",
        "              grad_b_L[k - 1] = np.sum(delta_K, axis=1, keepdims=True) / self.batch_size  # No reshape needed\n",
        "              print(\"shape Grad_b_L reshaped : \" , grad_b_L[k - 1].shape)\n",
        "              # Gradients wrt hidden layer\n",
        "              # Gradients wrt h_(k-1)\n",
        "\n",
        "              grad_h_prev_L =self.network[\"w\" + str(k)] @ delta_K\n",
        "              print(\"Grad_h_prev_L : \" , grad_h_prev_L.shape)\n",
        "\n",
        "              grad_a_k_L = grad_h_prev_L\n",
        "\n",
        "              # Gradients wrt a_(k-1)\n",
        "              # if(k > 1):\n",
        "              #   prev_act = self.activation_fn[k - 2]\n",
        "              #   a_prev_activations = self.inter_layer[\"a\" + str(k-2)][:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "\n",
        "              #   grad_act_fn_prev = apply_activation_derivative(prev_act , a_prev_activations).do_activation_derivative()\n",
        "              #   print(\"Grad_h_act_fn_prev : \" , grad_h_prev_L.shape)\n",
        "\n",
        "              #   grad_a_prev_L = grad_h_prev_L * grad_act_fn_prev\n",
        "              #   print(\"Grad_a_prev_L : \" , grad_a_prev_L.shape)\n",
        "\n",
        "              # # Gradient for a_{k-1}\n",
        "              #   grad_a_k_L = grad_h_prev_L\n",
        "              #   print(\"if Grad_a_k_L : \" , grad_a_k_L.shape)\n",
        "              # else:\n",
        "              #   # For k=1 (first hidden layer), input layer has no activation derivative\n",
        "              #   grad_a_k_L = grad_h_prev_L  # Propagate gradient without activation derivative\n",
        "              #   print(\"else Grad_a_k_L : \" , grad_a_k_L.shape)\n",
        "\n",
        "            # Inside the batch loop after computing gradients\n",
        "            # w_updates = CallOptimizers(self.optimizer_fn, grad_w_L).apply_optimization()\n",
        "            # b_updates = CallOptimizers(self.optimizer_fn, grad_b_L).apply_optimization()\n",
        "\n",
        "            # for k in range(1, L):\n",
        "            #   self.network[\"w\" + str(k)] -= w_updates[\"w\" + str(k)]\n",
        "            #   self.network[\"b\" + str(k)] -= b_updates[\"b\" + str(k)]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jHfISCRCqJDP"
      },
      "id": "jHfISCRCqJDP",
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Backpropagation(loss_function=\"se\",X_train= X_train, y_train= y_train,layers_dimensions= [784, 128, 64, 10],activation_fn=['sigmoid', 'sigmoid', 'sigmoid'], batch_size=10000,optimizer_fn=\"Momentum\", epochs= 5,weight_decay=0,grad_reglr_fn=None, method= \"Xavier_U\" ).backward_propagation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WFk5jtH19Ti",
        "outputId": "2b4a0a08-fdf6-4b17-db95-b816ddb41f8a"
      },
      "id": "_WFk5jtH19Ti",
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-7fa67dacb3dc>:10: RuntimeWarning: overflow encountered in exp\n",
            "  sig = np.where(self.m >= 0,1/(1 - np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) - 1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_norm shape  (784, 60000)\n",
            "y_norm shape  (10, 60000)\n",
            "H shape  (10, 60000)\n",
            "length self.network  12\n",
            "self.batch_no  6\n",
            "self.batch_size  10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-7fa67dacb3dc>:92: RuntimeWarning: overflow encountered in exp\n",
            "  sig_d = 1 / (1 + np.exp(-self.m))  # Compute sigmoid\n",
            "\n",
            " 17%|█▋        | 1/6 [00:01<00:07,  1.41s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 33%|███▎      | 2/6 [00:02<00:05,  1.44s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 50%|█████     | 3/6 [00:04<00:04,  1.51s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 67%|██████▋   | 4/6 [00:06<00:03,  1.60s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 83%|████████▎ | 5/6 [00:08<00:01,  1.77s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 6/6 [00:09<00:00,  1.61s/it]\n",
            " 20%|██        | 1/5 [00:09<00:38,  9.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 17%|█▋        | 1/6 [00:01<00:06,  1.34s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 33%|███▎      | 2/6 [00:02<00:05,  1.36s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 50%|█████     | 3/6 [00:04<00:04,  1.36s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 67%|██████▋   | 4/6 [00:05<00:02,  1.35s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 83%|████████▎ | 5/6 [00:06<00:01,  1.35s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 6/6 [00:08<00:00,  1.36s/it]\n",
            " 40%|████      | 2/5 [00:17<00:26,  8.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 17%|█▋        | 1/6 [00:01<00:09,  1.95s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 33%|███▎      | 2/6 [00:03<00:07,  1.89s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 50%|█████     | 3/6 [00:05<00:04,  1.65s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 67%|██████▋   | 4/6 [00:06<00:03,  1.57s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 83%|████████▎ | 5/6 [00:07<00:01,  1.49s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 6/6 [00:09<00:00,  1.55s/it]\n",
            " 60%|██████    | 3/5 [00:27<00:18,  9.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 17%|█▋        | 1/6 [00:01<00:06,  1.35s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 33%|███▎      | 2/6 [00:02<00:05,  1.35s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 50%|█████     | 3/6 [00:04<00:04,  1.36s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 67%|██████▋   | 4/6 [00:06<00:03,  1.70s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 83%|████████▎ | 5/6 [00:07<00:01,  1.67s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 6/6 [00:09<00:00,  1.56s/it]\n",
            " 80%|████████  | 4/5 [00:36<00:09,  9.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 17%|█▋        | 1/6 [00:01<00:06,  1.34s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 33%|███▎      | 2/6 [00:02<00:05,  1.35s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 50%|█████     | 3/6 [00:04<00:04,  1.36s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 67%|██████▋   | 4/6 [00:05<00:02,  1.36s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 83%|████████▎ | 5/6 [00:06<00:01,  1.36s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n",
            "Grad_h_prev_L :  (784, 10000)\n",
            "X_batch shape :  (784, 10000)\n",
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "grad_a_k_L :  (10, 10000)\n",
            "grad_a_k_l :  (10, 10000)\n",
            "A_K :  (10, 10000)\n",
            "do derivation :  (10, 10000)\n",
            "delta k :  (10, 10000)\n",
            "A prev :  (64, 10000)\n",
            "len Grad_w_L :  (10, 64)\n",
            "shape Grad_b_L :  (10, 1)\n",
            "shape Grad_b_L reshaped :  (10, 1)\n",
            "Grad_h_prev_L :  (64, 10000)\n",
            "grad_a_k_l :  (64, 10000)\n",
            "A_K :  (64, 10000)\n",
            "do derivation :  (64, 10000)\n",
            "delta k :  (64, 10000)\n",
            "A prev :  (128, 10000)\n",
            "len Grad_w_L :  (64, 128)\n",
            "shape Grad_b_L :  (64, 1)\n",
            "shape Grad_b_L reshaped :  (64, 1)\n",
            "Grad_h_prev_L :  (128, 10000)\n",
            "grad_a_k_l :  (128, 10000)\n",
            "A_K :  (128, 10000)\n",
            "do derivation :  (128, 10000)\n",
            "delta k :  (128, 10000)\n",
            "A prev :  (784, 10000)\n",
            "len Grad_w_L :  (128, 784)\n",
            "shape Grad_b_L :  (128, 1)\n",
            "shape Grad_b_L reshaped :  (128, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 6/6 [00:08<00:00,  1.40s/it]\n",
            "100%|██████████| 5/5 [00:44<00:00,  8.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grad_h_prev_L :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c,d = Feedforward(X_new,[784, 128, 64, 10],['sigmoid','relu','softmax'],\"Xavier_U\").Forward_prop()\n",
        "c[\"w3\"].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGOmz4yXVtvE",
        "outputId": "196ebcda-fcf0-4345-a556-ad6da155486f"
      },
      "id": "kGOmz4yXVtvE",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c,d = Feedforward(X,[2,3,2,1],['sigmoid','relu','softmax'],\"Xavier_U\").Forward_prop()\n",
        "a,b,c,d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92hGBoNHLYP3",
        "outputId": "60f1e8d9-968f-4f6b-8b64-2db36ceedfaa"
      },
      "id": "92hGBoNHLYP3",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([[0.20997228],\n",
              "         [1.01617777]]),\n",
              "  array([[0.64748392]])),\n",
              " {'h0': array([[8],\n",
              "         [8]]),\n",
              "  'a1': array([[1.64223325],\n",
              "         [2.43353861],\n",
              "         [4.35825011]]),\n",
              "  'h1': array([[1.23999836],\n",
              "         [1.09616172],\n",
              "         [1.01296675]]),\n",
              "  'a2': array([[0.20997228],\n",
              "         [1.01617777]]),\n",
              "  'h2': array([[0.20997228],\n",
              "         [1.01617777]]),\n",
              "  'a3': array([[0.64748392]])},\n",
              " {'w1': array([[ 0.10694503,  0.47145628,  0.22514328],\n",
              "         [ 0.09833413, -0.16726395,  0.31963799]]),\n",
              "  'b1': array([[0.],\n",
              "         [0.],\n",
              "         [0.]]),\n",
              "  'h1': 'sigmoid',\n",
              "  'neuronns in layer 1': 3,\n",
              "  'w2': array([[ 0.10694503,  0.47145628],\n",
              "         [ 0.22514328,  0.09833413],\n",
              "         [-0.16726395,  0.31963799]]),\n",
              "  'b2': array([[0.],\n",
              "         [0.]]),\n",
              "  'h2': 'relu',\n",
              "  'neuronns in layer 2': 2,\n",
              "  'w3': array([[0.13806544],\n",
              "         [0.60864744]]),\n",
              "  'b3': array([[0.]]),\n",
              "  'h3': 'softmax',\n",
              "  'neuronns in layer 3': 1},\n",
              " array([[1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Backpropagation(loss_function=\"se\",X_train= X_train, y_train= y_train,layers_dimensions= [784, 128, 64 ,10],activation_fn=['sigmoid', 'sigmoid', 'sigmoid'], batch_size=10000,optimizer_fn=\"Adam\", epochs= 5,weight_decay=0,grad_reglr_fn=None, method= \"Xavier_U\" ).backward_propogation()"
      ],
      "metadata": {
        "id": "wss0tLWsslKV"
      },
      "id": "wss0tLWsslKV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "class Backprop:\n",
        "\n",
        "      def __init__(self, loss_function,X_train , y_train, activation_fn , layers_dimensions ,method, batch_size, optimizer_fn, epochs, X_val=None, t_val=None, use_wandb=False, optim_params=None):\n",
        "\n",
        "        self.loss_function = loss_function\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.activation_fn = activation_fn\n",
        "        self.method = method\n",
        "        self.n = layers_dimensions\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.optimizer_fn = optimizer_fn\n",
        "        self.X_norm= Normalize(self.X_train).Norm_reshape()\n",
        "        self.y_norm =OneHotEncoder(self.X_train, self.y_train).onehot_encode()\n",
        "        self.batches_number = math.ceil(self.y_norm.shape[1]/self.batch_size)\n",
        "        cache, Inter_layers, network, H = Feedforward(self.X_norm,self.n,self.activation_fn,self.method).Forward_prop()\n",
        "        self.inter_layer = Inter_layers\n",
        "        self.H = H\n",
        "        self.network = network\n",
        "        print(self.X_norm.shape)\n",
        "        print(self.y_norm.shape)\n",
        "        print(self.H.shape)\n",
        "        print(len(self.network))\n",
        "\n",
        "        #self.loss = map_losses[loss]\n",
        "        # self.use_wandb = use_wandb\n",
        "        # if t_val is not None:\n",
        "        #     self.X_val = X_val\n",
        "        #     self.layers[0].a_val = X_val\n",
        "        #     self.t_val = t_val\n",
        "        #self.param_init(optimizer, optim_params)\n",
        "\n",
        "\n",
        "      def backward_propogation(self):\n",
        "        L = len(self.n)\n",
        "        self.y_predicted = self.H\n",
        "        # Initialize variables neesed to keep track of loss\n",
        "        self.eta_hist = []\n",
        "        self.loss_hist = []\n",
        "        self.accuracy_hist = []\n",
        "        self.loss_hist_val = []\n",
        "        self.accuracy_hist_val = []\n",
        "        self.loss = callloss(self.loss_function, self.y_norm.T, self.y_predicted).give_loss()\n",
        "\n",
        "        # Perform Backprop\n",
        "        for epochs in tqdm(range(self.epochs)):\n",
        "\n",
        "          for batch in range(self.batches_number):\n",
        "            self.grad_pre_act = [np.zeros(2)] * L\n",
        "            self.grad_a_w = [np.zeros(2)] * L\n",
        "            self.grad_a_b = [np.zeros(2)] * L\n",
        "            grad_a_w_update = [np.zeros(2)] * L\n",
        "            grad_a_b_updade = [np.zeros(2)] * L\n",
        "\n",
        "\n",
        "            y_true_batch = self.y_norm[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "            y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "            self.y_true_batch = y_true_batch\n",
        "            self.y_pred_batch = y_pred_batch\n",
        "            #print(self.activation_fn[-1])\n",
        "\n",
        "              # try:\n",
        "              #     if self.loss_hist[-1] > self.loss_hist[-2]:\n",
        "              #         for layer in self.layers[1:]:\n",
        "              #             layer.W_optimizer.set_params({\"eta\":self.optimizer.eta/2})\n",
        "              #             layer.b_optimizer.set_params({\"eta\":self.optimizer.eta/2})\n",
        "              #         flag = 1\n",
        "              # except:\n",
        "              #     pass\n",
        "\n",
        "              # if flag == 1:\n",
        "              #     break\n",
        "\n",
        "              # self.layers[-1].cross_grad = self.loss.diff()\n",
        "            self.grad_act  = call_lossgrad(self.loss_function,y_true_batch, y_pred_batch).give_gradloss()\n",
        "\n",
        "            print(\"Shape of grad_act[-1]:\", np.shape(self.grad_act))\n",
        "            print(\"Shape of activation derivative output:\", np.shape(apply_activation_derivative(self.activation_fn[-1], self.inter_layer[\"a\" + str(L- 2)]).do_activation_derivative()))\n",
        "\n",
        "\n",
        "\n",
        "            self.grad_pre_act[-1] = self.grad_act[-1] @ apply_activation_derivative(self.activation_fn[-1], self.inter_layer[\"a\" + str(L-1)]).do_activation_derivative()[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "            # print(type(apply_activation_derivative(self.activation_fn, self.inter_layer[\"a\" + str(L-1)]).do_activation_derivative()))\n",
        "\n",
        "            self.grad_a_w[-1] = self.grad_pre_act[-1] @ self.inter_layer[\"a\" + str(L-2)][:, batch*self.batch_size:(batch+1)*self.batch_size].T\n",
        "            self.grad_a_b[-1] = - np.sum(self.grad_act[-1], axis=1)\n",
        "\n",
        "            grad_a_w_update = CallOptimizers(self.optimizer_fn, self.grad_a_w[-1]).apply_optimization()\n",
        "            grad_a_b_updade = CallOptimizers(self.optimizer_fn, self.grad_a_b[-1]).apply_optimization()\n",
        "\n",
        "              # self.layers[-1].a_grad = self.loss.diff(self.t_batch, self.y_batch)\n",
        "              # self.layers[-1].h_grad = self.layers[-1].a_grad * self.layers[-1].activation.diff(self.layers[-1].h[:, batch*self.batch_size:(batch+1)*self.batch_size])\n",
        "\n",
        "              # self.layers[-1].W_grad = self.layers[-1].h_grad @ self.layers[-2].a[:, batch*self.batch_size:(batch+1)*self.batch_size].T\n",
        "              # self.layers[-1].W_update = self.layers[-1].W_optimizer.get_update(self.layers[-1].W_grad)\n",
        "\n",
        "              # self.layers[-1].b_grad = -np.sum(self.layers[-1].h_grad, axis=1).reshape(-1,1)\n",
        "              # self.layers[-1].b_update = self.layers[-1].b_optimizer.get_update(self.layers[-1].b_grad)\n",
        "\n",
        "              # print(\"Last Layer\")\n",
        "              # print(\"a_grad shape:\", self.layers[-1].a_grad.shape)\n",
        "              # print(\"h_grad shape:\", self.layers[-1].h_grad.shape)\n",
        "              # print(\"W_grad shape:\", self.layers[-1].W_grad.shape)\n",
        "              # print(\"W_update shape:\", self.layers[-1].W_update.shape)\n",
        "              # print(\"W_shape:\", self.layers[-1].W.shape)\n",
        "              # print(\"a_grad:\\n\", self.layers[-1].a_grad)\n",
        "              # print(\"h_grad:\\n\", self.layers[-1].h_grad)\n",
        "              # print(\"W_grad:\\n\", self.layers[-1].W_grad)\n",
        "\n",
        "            # assert self.layers[-1].W_update.shape == self.layers[-1].W.shape, \"Sizes don't match\"\n",
        "            assert grad_a_w_update == self.network[\"w\" + str(L-1)].shape, \"Sizes don't match\"\n",
        "\n",
        "\n",
        "              # Backpropogation for the remaining layers\n",
        "            for i in range(L - 2, 0, -1):\n",
        "              self.grad_act[i] =  self.network[\"w\" + str(i + 1)].T @ self.grad_pre_act[i + 1]\n",
        "              self.grad_pre_act[i] = self.grad_act[i] * apply_activation_derivative(self.activation_fn[i], self.inter_layer[\"h\" + str(i)]).get_derivative()\n",
        "              self.grad_a_b[i] = - np.sum(self.grad_pre_act[i], axis = 1)\n",
        "              self.grad_a_w[i] = self.grad_pre_act[i] @ self.inter_layer[\"h\" + str(i-1)].T\n",
        "\n",
        "              grad_a_w_update = CallOptimizers(self.optimizer_fn, self.grad_a_w[i]).apply_optimization()\n",
        "              grad_a_b_update = CallOptimizers(self.optimizer_fn, self.grad_a_w[i]).apply_optimization()\n",
        "\n",
        "\n",
        "                # Update the weights\n",
        "              for k in range(L-1):\n",
        "                  self.network[\"w\" + str(k)] = self.network[\"w\" + str(k)] - self.grad_a_w_update\n",
        "                  self.network[\"b\" + str(k)] = self.network[\"w\" + str(k)] - self.grad_b_w_update\n",
        "\n",
        "              cache, self.Inter_layers, self.network, self.y_predicted = Feedforward(self.X_train,self.n,self.activation_fn,self.method).Forward_prop()\n"
      ],
      "metadata": {
        "id": "MhJrZkPpp7Zv"
      },
      "id": "MhJrZkPpp7Zv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TMAZN5qaVr3t"
      },
      "id": "TMAZN5qaVr3t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Backprop(loss_function=\"se\",X_train= X_train, y_train= y_train,layers_dimensions= [784, 128, 64 ,10],activation_fn=['sigmoid', 'sigmoid', 'softmax'], batch_size=10000,optimizer_fn=\"Adam\", epochs= 5, method= \"Xavier_U\" ).backward_propogation()"
      ],
      "metadata": {
        "id": "CyW79eripCll"
      },
      "id": "CyW79eripCll",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new= Normalize(X_train).Norm_reshape()\n",
        "y_new =OneHotEncoder(X_train,y_train).onehot_encode()"
      ],
      "metadata": {
        "id": "lhnh7_96ftQs"
      },
      "id": "lhnh7_96ftQs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new.shape"
      ],
      "metadata": {
        "id": "gFlE6dbNAJE7"
      },
      "id": "gFlE6dbNAJE7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c,d = Feedforward(X,[2,3,3,1],['relu','relu','softmax'],\"Xavier_U\").Forward_prop()\n",
        "a,b,c,d"
      ],
      "metadata": {
        "id": "NV7gYASA0m8t"
      },
      "id": "NV7gYASA0m8t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(b[\"a\" + str(4-1)])"
      ],
      "metadata": {
        "id": "Xmxy4rN2cTM-"
      },
      "id": "Xmxy4rN2cTM-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(apply_activation_derivative([\"sigmoid\"], b[\"a\" + str(4-1)]).do_activation_derivative()))"
      ],
      "metadata": {
        "id": "X06X6gfoV1Ol"
      },
      "id": "X06X6gfoV1Ol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2064d83c-b27e-46e6-bef2-c24d9faf70ff",
      "metadata": {
        "id": "2064d83c-b27e-46e6-bef2-c24d9faf70ff"
      },
      "outputs": [],
      "source": [
        "Initilize(3,5,\"Xavier_N\").Init_weight(\"Xavier_N\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [relu,relu,sigmoid]\n",
        "print(x[1])"
      ],
      "metadata": {
        "id": "AIOXHXcuRF9-"
      },
      "id": "AIOXHXcuRF9-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weight_bias([4,5,3,1],['relu','relu','sigmoid'],\"Xavier_N\").Init_network([4,5,3,1])"
      ],
      "metadata": {
        "id": "4NKWIZNDdN7u"
      },
      "id": "4NKWIZNDdN7u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callloss(\"se\",y_new,d).give_loss()"
      ],
      "metadata": {
        "id": "BOwYYfPZCFzT"
      },
      "id": "BOwYYfPZCFzT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CmG8ToWcnOR7"
      },
      "id": "CmG8ToWcnOR7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}