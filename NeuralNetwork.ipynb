{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "EuDCYeTFOWhL"
      },
      "id": "EuDCYeTFOWhL",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([[8,8,1],[7,9,1],[6,10,0],[5,5,0]], columns=['cgpa', 'profile_score', 'placed'])"
      ],
      "metadata": {
        "id": "fJ8rJlerOYGB"
      },
      "id": "fJ8rJlerOYGB",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['cgpa', 'profile_score']].values[0].reshape(2,1) # Shape(no of features, no. of training example)\n",
        "y = df[['placed']].values[0][0]"
      ],
      "metadata": {
        "id": "9724OEXSOX35"
      },
      "id": "9724OEXSOX35",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNIlLeY_gnbE",
        "outputId": "912fa6ae-342c-4415-bf52-cb2071dfe127"
      },
      "id": "aNIlLeY_gnbE",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.size"
      ],
      "metadata": {
        "id": "62rXWMGhyBbp",
        "outputId": "82c8222f-f7ae-4da7-c472-8753eef178da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "62rXWMGhyBbp",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47040000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy  as np\n",
        "\n",
        "class OneHotEncoder:\n",
        "\n",
        "    def __init__(self,x, y):\n",
        "        self.y = y\n",
        "        self.x = x\n",
        "        #self.num_class = num_class\n",
        "        self.onehot_encode()\n",
        "\n",
        "    def onehot_encode(self):\n",
        "        onehot = np.zeros((self.x.shape[0], 10))\n",
        "\n",
        "        for i, j in zip(range(len(self.x)), self.y):\n",
        "            onehot[i, j] = 1\n",
        "        return onehot.T\n",
        "\n",
        "class Normalize:\n",
        "\n",
        "    def __init__(self, unprocessed_X):\n",
        "        self.unprocessed_X = unprocessed_X\n",
        "        self.Norm_reshape()\n",
        "\n",
        "    def Norm_reshape(self):\n",
        "        X_norm = np.reshape(self.unprocessed_X,(self.unprocessed_X.shape[0],784)).T/255\n",
        "        X_norm= np.array(X_norm)\n",
        "\n",
        "        return X_norm"
      ],
      "metadata": {
        "id": "BW34ZLm_wv9o"
      },
      "id": "BW34ZLm_wv9o",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new= Normalize(X_train).Norm_reshape()"
      ],
      "metadata": {
        "id": "RUTyqFl2zbjL"
      },
      "id": "RUTyqFl2zbjL",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new= Normalize(X_train).Norm_reshape()\n",
        "y_new =OneHotEncoder(X_train,y_train).onehot_encode()"
      ],
      "metadata": {
        "id": "K4hGhlz9w6pa"
      },
      "id": "K4hGhlz9w6pa",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOeMXNH9g0hp",
        "outputId": "cbc6006e-12e3-479d-a8ce-e11f5fb94424"
      },
      "id": "IOeMXNH9g0hp",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 60000)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This function calculates sigmoid activation function\n",
        "class sigmoid:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.use_sigmoid()\n",
        "\n",
        "    def use_sigmoid(self):\n",
        "        sig = np.where(self.m >= 0,1/(1 - np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) - 1))\n",
        "        return sig\n",
        "        #if self.m >= 0:\n",
        "            #return 1/(1 - np.exp(-self.m))\n",
        "        #else:\n",
        "            #return np.exp(self.m)/(np.exp(-self.m) - 1)\n",
        "\n",
        "\n",
        "# This function calculates tanh activation function\n",
        "class tanh:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.use_tanh()\n",
        "\n",
        "    def use_tanh(self):\n",
        "        z = (np.exp(self.m) - np.exp(-self.m))/(np.exp(self.m) + np.exp(-self.m))\n",
        "        return z\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates relu activation function\n",
        "class relu:\n",
        "\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "        self.use_relu()\n",
        "\n",
        "    def use_relu(self):\n",
        "        if np.all(self.m > 0):\n",
        "            return self.m\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates softmax activation function\n",
        "class softmax:\n",
        "\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "        self.use_softmax()\n",
        "\n",
        "    def use_softmax(self):\n",
        "        x = np.copy(self.m)\n",
        "        max_exp = np.max(x)\n",
        "\n",
        "        x = np.exp(x - max_exp)\n",
        "        x = x / np.sum(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# This function call different activation function\n",
        "class apply_activation:\n",
        "\n",
        "    def __init__(self, activation_function, value):\n",
        "        self.activation_function = activation_function.lower()\n",
        "        self.value = value\n",
        "        self.do_activation()\n",
        "\n",
        "\n",
        "    def do_activation(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.value).use_sigmoid()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.value).use_relu()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.value).use_tanh()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.value).use_softmax()\n",
        "\n",
        "\n",
        "\n",
        "class sigmoid_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.sigmoid_d()\n",
        "\n",
        "    def sigmoid_d(self):\n",
        "        sig_d = np.where(self.m >= 0,1/(1 - np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) - 1))\n",
        "        return sig_d * (1 - sig_d)\n",
        "\n",
        "\n",
        "\n",
        "class relu_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.relu_d()\n",
        "\n",
        "    def relu_d(self):\n",
        "        if self.m > 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "\n",
        "\n",
        "class tanh_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.tanh_d()\n",
        "\n",
        "    def tanh_d(self):\n",
        "        z = (np.exp(self.m) - np.exp(-self.m))/(np.exp(self.m) + np.exp(-self.m))\n",
        "        return (1 - (z)**2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class apply_activation_derivative:\n",
        "\n",
        "    def __init__(self,activation_function, value):\n",
        "        self.activation_function = activation_function.lower()\n",
        "        self.value = value\n",
        "        self.do_activation_derivative()\n",
        "\n",
        "    def do_activation_derivative(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid_derv(self.value).sigmoid_d()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu_derv(self.value).relu_d()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh_derv(self.value).tanh_d()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "01ZCcJdxviK6"
      },
      "id": "01ZCcJdxviK6",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class crossentropy:\n",
        "\n",
        "    def __init__(self, y, y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.give_celoss()\n",
        "\n",
        "    def give_celoss(self):\n",
        "        threshold = 10**(-8)\n",
        "        res = -np.sum(np.sum(self.y*np.log(self.y_predicted)))\n",
        "        return res\n",
        "\n",
        "class squarderror:\n",
        "\n",
        "    def __init__(self, y, y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.give_seloss()\n",
        "\n",
        "    def give_seloss(self):\n",
        "        res = np.sum((self.y - self.y_predicted)**2)\n",
        "        return np.sum(res)\n",
        "\n",
        "\n",
        "class callloss:\n",
        "\n",
        "    def __init__(self,loss_function, value1, value2):\n",
        "        self.loss_function = loss_function.lower()\n",
        "        self.value1 = value1\n",
        "        self.value2 = value2\n",
        "        self.give_loss()\n",
        "\n",
        "    def give_loss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return crossentropy(self.value1, self.value2).give_celoss()\n",
        "        if self.loss_function == 'se':\n",
        "            return squarderror(self.value1, self.value2).give_seloss()\n",
        "\n",
        "class crossentropy_grad:\n",
        "\n",
        "    def __init__(self,y ,y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.Give_cegrad()\n",
        "\n",
        "    def Give_cegrad(self):\n",
        "        grad = -self.y/(self.y_predicted)\n",
        "        return grad\n",
        "\n",
        "\n",
        "class squarederror_grad:\n",
        "\n",
        "    def __init__(self,y , y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.Give_segrad()\n",
        "\n",
        "    def Give_segrad(self):\n",
        "        res = -np.sum(self.y - self.y_predicted)\n",
        "        return res\n",
        "\n",
        "class call_lossgrad:\n",
        "\n",
        "    def __init__(self,loss_function, value1, value2):\n",
        "        self.loss_function = loss_function.lower()\n",
        "        self.value1 = value1\n",
        "        self.value2 = value2\n",
        "        self.give_gradloss()\n",
        "\n",
        "    def give_gradloss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return crossentropy_grad(self.value1, self.value2).Give_cegrad()\n",
        "        if self.loss_function == 'se':\n",
        "            return squarederror_grad(self.value1, self.value2).Give_segrad()"
      ],
      "metadata": {
        "id": "iA7A6vN-l5K1"
      },
      "id": "iA7A6vN-l5K1",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "895d572d-2ffd-466a-a260-e57859a6449e",
      "metadata": {
        "id": "895d572d-2ffd-466a-a260-e57859a6449e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Initilize:\n",
        "\n",
        "    def __init__(self, i_size, o_size, method = \"Xavier_U\"):\n",
        "        self.i_size = i_size\n",
        "        self.o_size = o_size\n",
        "        self.w, self.b = self.Init_weight(method)\n",
        "\n",
        "    def Init_weight(self, init_method):\n",
        "\n",
        "\n",
        "        if init_method == \"Xavier_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(1 / self.i_size)\n",
        "          w = np.random.randn(self.i_size,self.o_size)*a\n",
        "\n",
        "        elif init_method == \"Xavier_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / (self.o_size + self.i_size))\n",
        "          w = np.random.uniform((-a), a,( self.i_size, self.o_size))\n",
        "\n",
        "        elif init_method == \"He_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(2 / self.i_size)\n",
        "          w = np.random.randn(self.i_size, self.o_size)*a\n",
        "\n",
        "        elif init_method == \"He_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / self.i_size)\n",
        "          w = np.random.uniform(-a, a, (self.i_size, self.o_size))\n",
        "\n",
        "        elif init_method == \"Random\":\n",
        "          np.random.seed(0)\n",
        "          w = np.random.randn(self.i_size, self.o_size)*0.01\n",
        "\n",
        "        b = np.zeros((self.o_size,1))\n",
        "\n",
        "        return w, b\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Weight_bias:\n",
        "  def __init__(self, layer_dimension, activation_fn, method):\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.method = method\n",
        "\n",
        "  def Init_network(self, layer_dimension):\n",
        "    self.n = layer_dimension\n",
        "    self.network = {}\n",
        "\n",
        "    for i in range(1, len(self.n)):\n",
        "      self.network[\"w\" + str(i)] = Initilize(self.n[i-1], self.n[i], self.method).Init_weight(self.method)[0]\n",
        "      self.network[\"b\" + str(i)] = Initilize(self.n[i-1], self.n[i], self.method).Init_weight(self.method)[1]\n",
        "      self.network [\"h\"+ str(i)] = str(self.activation_fn[i-1])\n",
        "      self.network [\"neuronns in layer \"+ str(i)] = self.n[i]\n",
        "\n",
        "    return self.network\n",
        "\n",
        "\n",
        "\n",
        "class Pre_Feedforward:\n",
        "\n",
        "  def __init__(self, Prev_layer_H, w, b, activation_fn):\n",
        "    self.Prev_layer_H= Prev_layer_H\n",
        "    self.w = w\n",
        "    self.b = b\n",
        "    self.activation_fn = activation_fn\n",
        "    self.Preactivation_cal()\n",
        "\n",
        "\n",
        "  def Preactivation_cal(self):\n",
        "\n",
        "    A = np.dot(self.w.T, self.Prev_layer_H) + self.b\n",
        "    H = apply_activation(self.activation_fn, A).do_activation()\n",
        "    cache = (self.Prev_layer_H, A)\n",
        "    #self.cache[\"Input\"] = self.Prev_layer_H\n",
        "    #self.cache[\"Pre_act\"] = H\n",
        "    return cache , H\n",
        "\n",
        "class Feedforward:\n",
        "\n",
        "  def __init__(self,X_train, layer_dimensions, activation_fn, method):\n",
        "    self.X_train = X_train\n",
        "    self.n = layer_dimensions\n",
        "    self.activation_fn = activation_fn\n",
        "    self.method = method\n",
        "\n",
        "\n",
        "  def Forward_prop(self):\n",
        "    network = Weight_bias(self.n, self.activation_fn,self.method).Init_network(self.n)\n",
        "    H = self.X_train\n",
        "    L = len(self.n) - 1\n",
        "    Inter_layers = {}\n",
        "\n",
        "    for i in range(1,L+1):\n",
        "      Prev_layer_H = H\n",
        "\n",
        "      w_l = network[\"w\"+  str(i)]\n",
        "      b_l = network[\"b\"+  str(i)]\n",
        "      Act_fn_l = network[\"h\" + str(i)]\n",
        "\n",
        "      cache, H = Pre_Feedforward(Prev_layer_H, w_l,b_l,Act_fn_l).Preactivation_cal()\n",
        "\n",
        "\n",
        "      #A = np.dot(w_l.T, Prev_layer_H) + b_l\n",
        "      #H = apply_activation(Act_fn_l, A).do_activation()\n",
        "\n",
        "      Inter_layers[\"h\" + str(i-1)] = cache[0]\n",
        "      Inter_layers[\"a\" + str(i)] = cache[1]\n",
        "      #cache.append((Prev_layer_H, A))\n",
        "\n",
        "      #cache , New_H = Pre_Feedforward(Prev_layer_H, w_l, b_l, Act_fn_l).Preactivation_cal()\n",
        "      #cache.appand()\n",
        "\n",
        "    return cache,Inter_layers,network , H\n",
        "\n",
        "\n",
        "class Findloss:\n",
        "  def __init__(self,X_train, y_train, layer_dimension, activation_fn, method, loss_function):\n",
        "    self.X_train = X_train\n",
        "    self.y_train = y_train\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.loss_function = loss_function\n",
        "    self.method = method\n",
        "    self.y_pred = Feedforward(self.X_train, self.n, self.activation_fn, self.method).Forward_prop()[-1]\n",
        "\n",
        "  def loss(self):\n",
        "    return callloss(self.loss_function, self.y_train, self.y_pred).give_loss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Normal:\n",
        "    def __init__(self, eta=0.01):\n",
        "        self.update = 0\n",
        "        self.eta = eta\n",
        "\n",
        "    def set_params(self, params):\n",
        "        for key in params:\n",
        "            setattr(self, key, params[key])\n",
        "\n",
        "    def get_update(self, grad):\n",
        "        self.update = self.eta*grad\n",
        "        return self.update\n",
        "\n",
        "class Momentum:\n",
        "    def __init__(self, eta=1e-3, gamma=0.9):\n",
        "        self.update = 0\n",
        "        self.eta = eta\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def set_params(self, params):\n",
        "        for key in params:\n",
        "            setattr(self, key, params[key])\n",
        "\n",
        "    def get_update(self, grad):\n",
        "        self.update = self.gamma*self.update + self.eta*grad\n",
        "        return self.update\n",
        "\n",
        "class Nesterov:\n",
        "    def __init__(self, eta=1e-3, gamma=0.9):\n",
        "        self.update = 0\n",
        "        self.eta = eta\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def set_params(self, params):\n",
        "        for key in params:\n",
        "            setattr(self, key, params[key])\n",
        "\n",
        "    def get_update(self, W, grad=None):\n",
        "        # Have to still work on this\n",
        "        W_lookahead = W - self.gamma*self.update\n",
        "        self.update = self.gamma*self.update + self.eta*gradient(W_lookahead) # Need to call gradient function\n",
        "        W = W - self.update\n",
        "        return W\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "    def __init__(self, eta=1e-2, eps=1e-7):\n",
        "        self.v = 0\n",
        "        self.eta = eta\n",
        "        self.eps = eps\n",
        "\n",
        "    def set_params(self, params):\n",
        "        for key in params:\n",
        "            setattr(self, key, params[key])\n",
        "\n",
        "    def get_update(self, grad):\n",
        "        self.v = self.v + grad**2\n",
        "        return (self.eta/(self.v+self.eps)**0.5)*grad\n",
        "\n",
        "class RMSProp:\n",
        "    def __init__(self, beta=0.9, eta = 1e-3, eps = 1e-7):\n",
        "        self.v = 0\n",
        "        self.beta = beta\n",
        "        self.eta = eta\n",
        "        self.eps = eps\n",
        "\n",
        "    def set_params(self, params):\n",
        "        for key in params:\n",
        "            setattr(self, key, params[key])\n",
        "\n",
        "    def get_update(self, grad):\n",
        "        self.v = self.beta*self.v + (1-self.beta)*(grad**2)\n",
        "        return (self.eta/(self.v+self.eps)**0.5)*grad\n",
        "\n",
        "class Adam:\n",
        "    def __init__(self, beta1=0.9, beta2=0.999, eta=1e-2, eps=1e-8):\n",
        "        self.m = 0\n",
        "        self.v = 0\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eta = eta\n",
        "        self.eps = eps\n",
        "        self.iter = 1\n",
        "\n",
        "    def set_params(self, params):\n",
        "        for key in params:\n",
        "            setattr(self, key, params[key])\n",
        "\n",
        "    def get_update(self, grad):\n",
        "        # try:\n",
        "        #     print(\"Size of M:\", self.m.shape)\n",
        "        # except:\n",
        "        #     pass\n",
        "        # print(\"Size of term 2:\", grad.shape)\n",
        "        self.m = self.beta1*self.m + (1-self.beta1)*grad\n",
        "        self.v = self.beta2*self.v + (1-self.beta2)*(grad**2)\n",
        "        m_cap = self.m/(1-self.beta1**self.iter)\n",
        "        v_cap = self.v/(1-self.beta2**self.iter)\n",
        "        self.iter += 1\n",
        "        return (self.eta/(v_cap+self.eps)**0.5)*m_cap\n",
        "\n",
        "class Nadam:\n",
        "    # Reference: https://ruder.io/optimizing-gradient-descent/index.html#nadam\n",
        "    def __init__(self, beta1=0.9, beta2=0.999, eta=1e-3, eps=1e-7):\n",
        "        self.m = 0\n",
        "        self.v = 0\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eta = eta\n",
        "        self.eps = eps\n",
        "        self.iter = 1\n",
        "\n",
        "    def set_params(self, params):\n",
        "        for key in params:\n",
        "            setattr(self, key, params[key])\n",
        "\n",
        "    def get_update(self, grad):\n",
        "        self.m = self.beta1*self.m + (1-self.beta1)*grad\n",
        "        self.v = self.beta2*self.v + (1-self.beta2)*(grad**2)\n",
        "        m_cap = self.m/(1-self.beta1**self.iter)\n",
        "        v_cap = self.v/(1-self.beta2**self.iter)\n",
        "        update = self.beta1*m_cap + ((1-self.beta1)/(1-self.beta1**self.iter))*grad\n",
        "        self.iter += 1\n",
        "        return (self.eta/(v_cap+self.eps)**0.5)*update\n",
        "\n",
        "class CallOptimizers:\n",
        "\n",
        "  def __init__(self, optimization_function, gradient):\n",
        "    self.optimization_function = optimization_function\n",
        "    self.gradient = gradient\n",
        "    self.apply_optimization()\n",
        "\n",
        "  def apply_optimization(self):\n",
        "    if self.optimization_function == 'Normal':\n",
        "        return Normal(self.gradient).get_update()\n",
        "    elif self.optimization_function == 'Momentum':\n",
        "        return Momentum(self.gradient).get_update()\n",
        "    elif self.optimization_function == 'Nesterov':\n",
        "        return Nesterov(self.gradient).get_update()\n",
        "    elif self.optimization_function == 'AdaGrad':\n",
        "        return AdaGrad(self.gradient).get_update()\n",
        "    elif self.optimization_function == 'RMSProp':\n",
        "        return RMSProp(self.gradient).get_update()\n",
        "    elif self.optimization_function == 'Adam':\n",
        "        return Adam(self.gradient).get_update()\n",
        "    elif self.optimization_function == 'Nadam':\n",
        "        return Nadam(self.gradient).get_update()"
      ],
      "metadata": {
        "id": "_nKlxgD6qnsO"
      },
      "id": "_nKlxgD6qnsO",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "class Backprop:\n",
        "\n",
        "    def __init__(self, loss_function,X_train , y_train, activation_fn , layers_dimensions ,method, batch_size, optimizer_fn, epochs, X_val=None, t_val=None, use_wandb=False, optim_params=None):\n",
        "\n",
        "      self.loss_function = loss_function\n",
        "      self.X_train = X_train\n",
        "      self.y_train = y_train\n",
        "      self.activation_fn = activation_fn\n",
        "      self.method = method\n",
        "      self.n = layers_dimensions\n",
        "      self.batch_size = batch_size\n",
        "      self.epochs = epochs\n",
        "      self.optimizer_fn = optimizer_fn\n",
        "      self.X_norm= Normalize(self.X_train).Norm_reshape()\n",
        "      self.y_norm =OneHotEncoder(self.X_train, self.y_train).onehot_encode()\n",
        "      self.batches_number = math.ceil(y_norm.shape[1]/self.batch_size)\n",
        "      cache, Inter_layers, network, H = Feedforward(self.X_train,self.n,self.activation_fn,self.method).Forward_prop()\n",
        "      self.inter_layer = Inter_layers\n",
        "      self.H = H\n",
        "      self.network = network\n",
        "\n",
        "      #self.loss = map_losses[loss]\n",
        "      # self.use_wandb = use_wandb\n",
        "      # if t_val is not None:\n",
        "      #     self.X_val = X_val\n",
        "      #     self.layers[0].a_val = X_val\n",
        "      #     self.t_val = t_val\n",
        "      #self.param_init(optimizer, optim_params)\n",
        "\n",
        "\n",
        "    def backward_propogation(self):\n",
        "      L = len(self.n)\n",
        "      self.y_predicted = self.H\n",
        "      # Initialize variables neesed to keep track of loss\n",
        "      self.eta_hist = []\n",
        "      self.loss_hist = []\n",
        "      self.accuracy_hist = []\n",
        "      self.loss_hist_val = []\n",
        "      self.accuracy_hist_val = []\n",
        "      self.loss = callloss(self.loss_function, self.y_train, self.y_predicted).give_loss()\n",
        "\n",
        "      # Perform Backprop\n",
        "      for epochs in tqdm(range(self.epochs)):\n",
        "\n",
        "        for batch in range(self.batches_number):\n",
        "\n",
        "          mini_batch = np.arrange(batch * self.batch_size, (batch+1)* self.batch_size, self.y_norm)\n",
        "          y_true_batch = np.take(self.y_self, mini_batch, axis = 1)\n",
        "          y_pred_batch = np.take(self.y_predicted,mini_batch, axis = 1)\n",
        "\n",
        "            # try:\n",
        "            #     if self.loss_hist[-1] > self.loss_hist[-2]:\n",
        "            #         for layer in self.layers[1:]:\n",
        "            #             layer.W_optimizer.set_params({\"eta\":self.optimizer.eta/2})\n",
        "            #             layer.b_optimizer.set_params({\"eta\":self.optimizer.eta/2})\n",
        "            #         flag = 1\n",
        "            # except:\n",
        "            #     pass\n",
        "\n",
        "            # if flag == 1:\n",
        "            #     break\n",
        "\n",
        "            # self.layers[-1].cross_grad = self.loss.diff()\n",
        "          grad_L_yhat  = call_lossgrad(self.loss_function,y_true_batch, y_pred_batch).give_gradloss()\n",
        "          grad_yhat_a = grad_L_yhat @ apply_activation_derivative(self.activation_fn, self.inter_layer[\"a\" + str(L-1)]).give_gradloss()\n",
        "\n",
        "          grad_a_w = grad_yhat_a @ self.network[\"w\" + str(L-1)].T\n",
        "          grad_a_b = - np.sum(grad_L_yhat, axis=1)\n",
        "\n",
        "          grad_a_w_update = CallOptimizers(self.optimizer_fn, grad_a_w).apply_optimization()\n",
        "          grad_a_b_updade = CallOptimizers(self.optimizer_fn, grad_a_b).apply_optimization()\n",
        "\n",
        "\n",
        "\n",
        "            self.layers[-1].a_grad = self.loss.diff(self.t_batch, self.y_batch)\n",
        "            self.layers[-1].h_grad = self.layers[-1].a_grad * self.layers[-1].activation.diff(self.layers[-1].h[:, batch*self.batch_size:(batch+1)*self.batch_size])\n",
        "\n",
        "            self.layers[-1].W_grad = self.layers[-1].h_grad @ self.layers[-2].a[:, batch*self.batch_size:(batch+1)*self.batch_size].T\n",
        "            self.layers[-1].W_update = self.layers[-1].W_optimizer.get_update(self.layers[-1].W_grad)\n",
        "\n",
        "            self.layers[-1].b_grad = -np.sum(self.layers[-1].h_grad, axis=1).reshape(-1,1)\n",
        "            self.layers[-1].b_update = self.layers[-1].b_optimizer.get_update(self.layers[-1].b_grad)\n",
        "\n",
        "            # print(\"Last Layer\")\n",
        "            # print(\"a_grad shape:\", self.layers[-1].a_grad.shape)\n",
        "            # print(\"h_grad shape:\", self.layers[-1].h_grad.shape)\n",
        "            # print(\"W_grad shape:\", self.layers[-1].W_grad.shape)\n",
        "            # print(\"W_update shape:\", self.layers[-1].W_update.shape)\n",
        "            # print(\"W_shape:\", self.layers[-1].W.shape)\n",
        "            # print(\"a_grad:\\n\", self.layers[-1].a_grad)\n",
        "            # print(\"h_grad:\\n\", self.layers[-1].h_grad)\n",
        "            # print(\"W_grad:\\n\", self.layers[-1].W_grad)\n",
        "\n",
        "            assert self.layers[-1].W_update.shape == self.layers[-1].W.shape, \"Sizes don't match\"\n",
        "\n",
        "\n",
        "            # Backpropogation for the remaining layers\n",
        "            for i in range(len(self.layers[:-2]), 0, -1):\n",
        "                self.layers[i].a_grad = self.layers[i+1].W.T @ self.layers[i+1].h_grad\n",
        "                self.layers[i].h_grad = self.layers[i].a_grad * self.layers[i].activation.diff(self.layers[i].h[:, batch*self.batch_size:(batch+1)*self.batch_size])\n",
        "\n",
        "\n",
        "                self.layers[i].b_grad = -np.sum(self.layers[i].h_grad, axis=1).reshape(-1,1)\n",
        "                self.layers[i].W_grad = self.layers[i].h_grad @ self.layers[i-1].a[:, batch*self.batch_size:(batch+1)*self.batch_size].T\n",
        "\n",
        "                self.layers[i].W_update = self.layers[i].W_optimizer.get_update(self.layers[i].W_grad)\n",
        "                self.layers[i].b_update = self.layers[i].b_optimizer.get_update(self.layers[i].b_grad)\n",
        "\n",
        "\n",
        "              # Update the weights\n",
        "            for _, layer in enumerate(self.layers[1:]):\n",
        "                layer.W = layer.W - layer.W_update\n",
        "                layer.b = layer.b - layer.b_update\n",
        "                # print(\"Layer -\", idx)\n",
        "                # print(\"W:\\n\", layer.W)\n",
        "                # print(\"h:\\n\", layer.h)\n",
        "\n",
        "                # layer.b = layer.b - self.b_update\n",
        "            # print(\"Y:\\n\", self.layers[-1].y)\n",
        "            self.forward_propogation()\n",
        "\n",
        "            if flag == 1:\n",
        "                break\n",
        "\n",
        "    def describe(self):\n",
        "        print(\"Model with the following layers:\")\n",
        "        for i in self.layers:\n",
        "            print(i)\n",
        "        print(\"Loss:\", self.loss)\n",
        "        print(\"Epochs:\", self.epochs)\n",
        "        print(\"Batch Size:\", self.batch_size)\n",
        "        print(\"Optimizer:\", self.optimizer)\n",
        "        print(\"Initialization:\", self.initialization)"
      ],
      "metadata": {
        "id": "MhJrZkPpp7Zv"
      },
      "id": "MhJrZkPpp7Zv",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Backprop(loss_function=\"se\",X_train= X_new, y_train= y_new,layers= [784, 128, 64 ,10],activation_fn=['relu', 'relu', \"softmax\"], batch_size=10000,optimizer=Adam(), epochs= 5, t = y_new, loss=\"se\", method= \"Xavier_U\" ).backward_propogation()"
      ],
      "metadata": {
        "id": "CyW79eripCll",
        "outputId": "4845fa14-4e29-4cc8-a4dd-48d93c376a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "id": "CyW79eripCll",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Backprop.__init__() got an unexpected keyword argument 'layers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-92e331295c4c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"se\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0my_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"se\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"Xavier_U\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propogation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: Backprop.__init__() got an unexpected keyword argument 'layers'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Findloss(X,y,[2,3,2,1], ['sigmoid','sigmoid','softmax'],\"Xavier_U\",\"se\").loss()"
      ],
      "metadata": {
        "id": "3PFRmUzwVmaO"
      },
      "id": "3PFRmUzwVmaO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "lhnh7_96ftQs"
      },
      "id": "lhnh7_96ftQs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a, b , c,d =Feedforward(X,[2,3,1],['sigmoid','sigmoid','sigmoid','softmax'],\"Xavier_U\").Forward_prop()\n",
        "a,b,c,d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV7gYASA0m8t",
        "outputId": "7c02652d-e0d2-4c4b-f99c-b0d8dc83dcf7"
      },
      "id": "NV7gYASA0m8t",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([[1.23999836],\n",
              "         [1.09616172],\n",
              "         [1.01296675]]),\n",
              "  array([[0.98103753]])),\n",
              " {'h0': array([[8],\n",
              "         [8]]),\n",
              "  'a1': array([[1.64223325],\n",
              "         [2.43353861],\n",
              "         [4.35825011]]),\n",
              "  'h1': array([[1.23999836],\n",
              "         [1.09616172],\n",
              "         [1.01296675]]),\n",
              "  'a2': array([[0.98103753]])},\n",
              " {'w1': array([[ 0.10694503,  0.47145628,  0.22514328],\n",
              "         [ 0.09833413, -0.16726395,  0.31963799]]),\n",
              "  'b1': array([[0.],\n",
              "         [0.],\n",
              "         [0.]]),\n",
              "  'h1': 'sigmoid',\n",
              "  'neuronns in layer 1': 3,\n",
              "  'w2': array([[0.11956818],\n",
              "         [0.52710415],\n",
              "         [0.25171784]]),\n",
              "  'b2': array([[0.]]),\n",
              "  'h2': 'sigmoid',\n",
              "  'neuronns in layer 2': 1},\n",
              " array([[1.5998001]]))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2064d83c-b27e-46e6-bef2-c24d9faf70ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "2064d83c-b27e-46e6-bef2-c24d9faf70ff",
        "outputId": "d8f41c02-1f9b-4fee-ff27-c7e926afe26e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cInitilize' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-049459aee159>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcInitilize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Xavier_N\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInit_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Xavier_N\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'cInitilize' is not defined"
          ]
        }
      ],
      "source": [
        "cInitilize(3,5,\"Xavier_N\").Init_weight(\"Xavier_N\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [relu,relu,sigmoid]\n",
        "print(x[1])"
      ],
      "metadata": {
        "id": "AIOXHXcuRF9-"
      },
      "id": "AIOXHXcuRF9-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weight_bias([4,2,3,1],['relu','relu','sigmoid'],\"Xavier_N\").Init_network([4,2,3,1])"
      ],
      "metadata": {
        "id": "4NKWIZNDdN7u"
      },
      "id": "4NKWIZNDdN7u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eAhzWxfwt_ub"
      },
      "id": "eAhzWxfwt_ub",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}