{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "EuDCYeTFOWhL"
      },
      "id": "EuDCYeTFOWhL",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([[8,8,1],[7,9,1],[6,10,0],[5,5,0]], columns=['cgpa', 'profile_score', 'placed'])\n",
        "X = df[['cgpa', 'profile_score']].values[0].reshape(2,1) # Shape(no of features, no. of training example)\n",
        "y = df[['placed']].values[0][0]"
      ],
      "metadata": {
        "id": "H2q--JEjvU-Q"
      },
      "id": "H2q--JEjvU-Q",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "aNIlLeY_gnbE"
      },
      "id": "aNIlLeY_gnbE",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62rXWMGhyBbp",
        "outputId": "48b4b564-0f52-4772-aa5c-686977cc5c8d"
      },
      "id": "62rXWMGhyBbp",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47040000"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy  as np\n",
        "\n",
        "class OneHotEncoder:\n",
        "\n",
        "    def __init__(self,x, y):\n",
        "        self.y = y\n",
        "        self.x = x\n",
        "        #self.num_class = num_class\n",
        "        self.onehot_encode()\n",
        "\n",
        "    def onehot_encode(self):\n",
        "        onehot = np.zeros((self.x.shape[0], 10))\n",
        "\n",
        "        for i, j in zip(range(len(self.x)), self.y):\n",
        "            onehot[i, j] = 1\n",
        "        return onehot.T\n",
        "\n",
        "class Normalize:\n",
        "\n",
        "    def __init__(self, unprocessed_X):\n",
        "        self.unprocessed_X = unprocessed_X\n",
        "        self.Norm_reshape()\n",
        "\n",
        "    def Norm_reshape(self):\n",
        "        X_norm = np.reshape(self.unprocessed_X,(self.unprocessed_X.shape[0],784)).T/255\n",
        "        X_norm= np.array(X_norm)\n",
        "\n",
        "        return X_norm"
      ],
      "metadata": {
        "id": "BW34ZLm_wv9o"
      },
      "id": "BW34ZLm_wv9o",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new= Normalize(X_train).Norm_reshape()"
      ],
      "metadata": {
        "id": "RUTyqFl2zbjL"
      },
      "id": "RUTyqFl2zbjL",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new= Normalize(X_train).Norm_reshape()\n",
        "y_new =OneHotEncoder(X_train,y_train).onehot_encode()"
      ],
      "metadata": {
        "id": "K4hGhlz9w6pa"
      },
      "id": "K4hGhlz9w6pa",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOeMXNH9g0hp",
        "outputId": "47d2a284-0ee4-4ed7-adcd-c774ca61337b"
      },
      "id": "IOeMXNH9g0hp",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# This function calculates sigmoid activation function\n",
        "class sigmoid:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.result =self.use_sigmoid()\n",
        "\n",
        "    def use_sigmoid(self):\n",
        "        sig = np.where(self.m >= 0,1/(1 - np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) - 1))\n",
        "        return sig\n",
        "        #if self.m >= 0:\n",
        "            #return 1/(1 - np.exp(-self.m))\n",
        "        #else:\n",
        "            #return np.exp(self.m)/(np.exp(-self.m) - 1)\n",
        "\n",
        "\n",
        "# This function calculates tanh activation function\n",
        "class tanh:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.result =self.use_tanh()\n",
        "\n",
        "    def use_tanh(self):\n",
        "        z = (np.exp(self.m) - np.exp(-self.m))/(np.exp(self.m) + np.exp(-self.m))\n",
        "        return z\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates relu activation function\n",
        "class relu:\n",
        "\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "        self.result =self.use_relu()\n",
        "\n",
        "    def use_relu(self):\n",
        "        return np.where(self.m > 0, self.m, 0)\n",
        "        #     return self.m\n",
        "        # else:\n",
        "        #     return 0\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates softmax activation function\n",
        "class softmax:\n",
        "\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "        self.result =self.use_softmax()\n",
        "\n",
        "    def use_softmax(self):\n",
        "        x = np.copy(self.m)\n",
        "        max_exp = np.max(x)\n",
        "\n",
        "        x = np.exp(x - max_exp)\n",
        "        x = x / np.sum(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# This function call different activation function\n",
        "class apply_activation:\n",
        "\n",
        "    def __init__(self, activation_function, value):\n",
        "        self.activation_function = activation_function.lower()\n",
        "        self.value = value\n",
        "        self.result = self.do_activation()\n",
        "\n",
        "\n",
        "    def do_activation(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid(self.value).use_sigmoid()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu(self.value).use_relu()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh(self.value).use_tanh()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax(self.value).use_softmax()\n",
        "\n",
        "\n",
        "\n",
        "class sigmoid_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.result = self.sigmoid_d()\n",
        "\n",
        "    def sigmoid_d(self):\n",
        "        sig_d = 1 / (1 + np.exp(-self.m))  # Compute sigmoid\n",
        "        # sig_d = np.where(self.m >= 0,1/(1 + np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) + 1))\n",
        "        return sig_d * (1 - sig_d)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class relu_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.result = self.relu_d()\n",
        "\n",
        "    def relu_d(self):\n",
        "        return np.where(self.m >0 , 1 ,0)\n",
        "\n",
        "\n",
        "\n",
        "class tanh_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.result =self.tanh_d()\n",
        "\n",
        "    def tanh_d(self):\n",
        "        z = (np.exp(self.m) - np.exp(-self.m))/(np.exp(self.m) + np.exp(-self.m))\n",
        "        return (1 - (z)**2)\n",
        "\n",
        "\n",
        "class softmax_derv:\n",
        "\n",
        "    def __init__(self,m):\n",
        "        self.m = m\n",
        "        self.result =self.softmax_d()\n",
        "\n",
        "    def softmax_d(self):\n",
        "        z=self.m - np.max(self.m,axis=0)\n",
        "        soft=np.exp(z)/np.sum(np.exp(z),axis=0)\n",
        "        return soft*(1-soft)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class apply_activation_derivative:\n",
        "\n",
        "    def __init__(self,activation_function, value):\n",
        "        self.activation_function= activation_function\n",
        "        self.value = value\n",
        "        self.do_activation_derivative()\n",
        "\n",
        "    def do_activation_derivative(self):\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            return sigmoid_derv(self.value).sigmoid_d()\n",
        "        elif self.activation_function == 'relu':\n",
        "            return relu_derv(self.value).relu_d()\n",
        "        elif self.activation_function == 'tanh':\n",
        "            return tanh_derv(self.value).tanh_d()\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return softmax_derv(self.value).softmax_d()\n",
        "\n",
        "        # else:\n",
        "        #   raise ValueError(\"Unknown activation function\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "01ZCcJdxviK6"
      },
      "id": "01ZCcJdxviK6",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class crossentropy:\n",
        "\n",
        "    def __init__(self, y, y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.give_celoss()\n",
        "\n",
        "    def give_celoss(self):\n",
        "        threshold = 10**(-8)\n",
        "        res = -np.sum(np.sum(self.y*np.log(self.y_predicted)))\n",
        "        return res\n",
        "\n",
        "class squarderror:\n",
        "\n",
        "    def __init__(self, y, y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.give_seloss()\n",
        "        if self.y.shape != self.y_predicted.shape:\n",
        "          raise ValueError(f\"Shape mismatch: y shape is {self.y.shape} and y_predicted shape is {self.y_predicted.shape}\")\n",
        "\n",
        "    def give_seloss(self):\n",
        "      if self.y_predicted.shape != self.y.shape:\n",
        "        self.y_predicted = self.y_predicted.T\n",
        "        res = np.sum((self.y - self.y_predicted)**2)\n",
        "        return np.sum(res)\n",
        "\n",
        "\n",
        "class callloss:\n",
        "\n",
        "    def __init__(self,loss_function, value1, value2):\n",
        "        self.loss_function = loss_function.lower()\n",
        "        self.value1 = value1\n",
        "        self.value2 = value2\n",
        "        self.give_loss()\n",
        "\n",
        "    def give_loss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return crossentropy(self.value1, self.value2).give_celoss()\n",
        "        if self.loss_function == 'se':\n",
        "            return squarderror(self.value1, self.value2).give_seloss()\n",
        "\n",
        "class crossentropy_grad:\n",
        "\n",
        "    def __init__(self,y ,y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.Give_cegrad()\n",
        "\n",
        "    def Give_cegrad(self):\n",
        "        grad = -self.y/(self.y_predicted)\n",
        "        return grad\n",
        "\n",
        "\n",
        "class squarederror_grad:\n",
        "\n",
        "    def __init__(self,y , y_predicted):\n",
        "        self.y = y\n",
        "        self.y_predicted = y_predicted\n",
        "        self.Give_segrad()\n",
        "\n",
        "    def Give_segrad(self):\n",
        "        res = -(self.y - self.y_predicted)\n",
        "        return res\n",
        "\n",
        "class call_lossgrad:\n",
        "\n",
        "    def __init__(self,loss_function, value1, value2):\n",
        "        self.loss_function = loss_function.lower()\n",
        "        self.value1 = value1\n",
        "        self.value2 = value2\n",
        "        self.give_gradloss()\n",
        "\n",
        "    def give_gradloss(self):\n",
        "        if self.loss_function == 'ce':\n",
        "            return crossentropy_grad(self.value1, self.value2).Give_cegrad()\n",
        "        if self.loss_function == 'se':\n",
        "            return squarederror_grad(self.value1, self.value2).Give_segrad()"
      ],
      "metadata": {
        "id": "iA7A6vN-l5K1"
      },
      "id": "iA7A6vN-l5K1",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "895d572d-2ffd-466a-a260-e57859a6449e",
      "metadata": {
        "id": "895d572d-2ffd-466a-a260-e57859a6449e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Initilize:\n",
        "\n",
        "    def __init__(self, i_size, o_size, method = \"Xavier_U\"):\n",
        "        self.i_size = i_size\n",
        "        self.o_size = o_size\n",
        "        self.w, self.b = self.Init_weight(method)\n",
        "\n",
        "    def Init_weight(self, init_method):\n",
        "\n",
        "\n",
        "        if init_method == \"Xavier_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(1 / self.i_size)\n",
        "          w = np.random.randn(self.i_size,self.o_size)*a\n",
        "\n",
        "        elif init_method == \"Xavier_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / (self.o_size + self.i_size))\n",
        "          w = np.random.uniform((-a), a,( self.i_size, self.o_size))\n",
        "\n",
        "        elif init_method == \"He_N\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(2 / self.i_size)\n",
        "          w = np.random.randn(self.i_size, self.o_size)*a\n",
        "\n",
        "        elif init_method == \"He_U\":\n",
        "          np.random.seed(0)\n",
        "          a = np.sqrt(6 / self.i_size)\n",
        "          w = np.random.uniform(-a, a, (self.i_size, self.o_size))\n",
        "\n",
        "        elif init_method == \"Random\":\n",
        "          np.random.seed(0)\n",
        "          w = np.random.randn(self.i_size, self.o_size)*0.01\n",
        "\n",
        "        b = np.zeros((self.o_size,1))\n",
        "\n",
        "        return w, b\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Weight_bias:\n",
        "  def __init__(self, layer_dimension, activation_fn, method):\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.method = method\n",
        "\n",
        "  def Init_network(self, layer_dimension):\n",
        "    self.n = layer_dimension\n",
        "    self.network = {}\n",
        "\n",
        "    for i in range(1, len(self.n)):\n",
        "      self.network[\"w\" + str(i)] = Initilize(self.n[i-1], self.n[i], self.method).Init_weight(self.method)[0]\n",
        "      self.network[\"b\" + str(i)] = Initilize(self.n[i-1], self.n[i], self.method).Init_weight(self.method)[1]\n",
        "      self.network [\"h\"+ str(i)] = str(self.activation_fn[i-1])\n",
        "      self.network [\"neuronns in layer \"+ str(i)] = self.n[i]\n",
        "\n",
        "    return self.network\n",
        "\n",
        "\n",
        "\n",
        "class Pre_Feedforward:\n",
        "\n",
        "  def __init__(self, Prev_layer_H, w, b, activation_fn):\n",
        "    self.Prev_layer_H= Prev_layer_H\n",
        "    self.w = w\n",
        "    self.b = b\n",
        "    self.activation_fn = activation_fn\n",
        "    self.Preactivation_cal()\n",
        "\n",
        "\n",
        "  def Preactivation_cal(self):\n",
        "    self.cache = {}\n",
        "\n",
        "    A = np.dot(self.w.T, self.Prev_layer_H) + self.b\n",
        "    H = apply_activation(self.activation_fn, A).do_activation()\n",
        "    cache = (self.Prev_layer_H, A)\n",
        "    #self.cache[\"Input\"] = self.Prev_layer_H\n",
        "    #self.cache[\"Pre_act\"] = H\n",
        "    return cache , H\n",
        "\n",
        "class Feedforward:\n",
        "\n",
        "  def __init__(self,X_train, layer_dimension, activation_fn, method):\n",
        "    self.X_train = X_train\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.method = method\n",
        "\n",
        "\n",
        "  def Forward_prop(self):\n",
        "    network = Weight_bias(self.n, self.activation_fn,self.method).Init_network(self.n)\n",
        "    H = self.X_train\n",
        "    L = len(self.n) -1\n",
        "    Inter_layers = {}\n",
        "\n",
        "    for i in range(1,L+1):\n",
        "      Prev_layer_H = H\n",
        "\n",
        "      w_l = network[\"w\"+  str(i)]\n",
        "      b_l = network[\"b\"+  str(i)]\n",
        "      Act_fn_l = network[\"h\" + str(i)]\n",
        "\n",
        "      cache, H = Pre_Feedforward(Prev_layer_H, w_l,b_l,Act_fn_l).Preactivation_cal()\n",
        "\n",
        "\n",
        "      #A = np.dot(w_l.T, Prev_layer_H) + b_l\n",
        "      #H = apply_activation(Act_fn_l, A).do_activation()\n",
        "\n",
        "      Inter_layers[\"h\" + str(i-1)] = cache[0]\n",
        "      Inter_layers[\"a\" + str(i)] = cache[1]\n",
        "      #cache.append((Prev_layer_H, A))\n",
        "\n",
        "      #cache , New_H = Pre_Feedforward(Prev_layer_H, w_l, b_l, Act_fn_l).Preactivation_cal()\n",
        "      #cache.appand()\n",
        "\n",
        "    return cache,Inter_layers,network , H\n",
        "\n",
        "\n",
        "class Findloss:\n",
        "  def __init__(self,X_train, y_train, layer_dimension, activation_fn, method, loss_function):\n",
        "    self.X_train = X_train\n",
        "    self.y_train = y_train\n",
        "    self.n = layer_dimension\n",
        "    self.activation_fn = activation_fn\n",
        "    self.loss_function = loss_function\n",
        "    self.method = method\n",
        "    self.y_pred = Feedforward(self.X_train, self.n, self.activation_fn, self.method).Forward_prop()[-1]\n",
        "\n",
        "  def loss(self):\n",
        "    return callloss(self.loss_function, self.y_train, self.y_pred).give_loss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c,d = Feedforward(X,[2,4,3,1],['sigmoid','relu','softmax'],\"Xavier_U\").Forward_prop()\n",
        "a,b,c,d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeaxRE4jdJ5C",
        "outputId": "35f19436-4566-4ad2-8885-88daf2dd0d88"
      },
      "id": "IeaxRE4jdJ5C",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([[0.        ],\n",
              "         [2.38736017],\n",
              "         [2.35077871]]),\n",
              "  array([[1.85012037]])),\n",
              " {'h0': array([[8],\n",
              "         [8]]),\n",
              "  'a1': array([[-0.44050715],\n",
              "         [ 5.77733567],\n",
              "         [ 0.6456094 ],\n",
              "         [ 6.98649894]]),\n",
              "  'h1': array([[1.1629916 ],\n",
              "         [1.00310658],\n",
              "         [2.10235488],\n",
              "         [1.00092513]]),\n",
              "  'a2': array([[-0.2705021 ],\n",
              "         [ 2.38736017],\n",
              "         [ 2.35077871]]),\n",
              "  'h2': array([[0.        ],\n",
              "         [2.38736017],\n",
              "         [2.35077871]]),\n",
              "  'a3': array([[1.85012037]])},\n",
              " {'w1': array([[ 0.09762701,  0.43037873,  0.20552675,  0.08976637],\n",
              "         [-0.1526904 ,  0.29178823, -0.12482558,  0.783546  ]]),\n",
              "  'b1': array([[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]]),\n",
              "  'h1': 'sigmoid',\n",
              "  'neuronns in layer 1': 4,\n",
              "  'w2': array([[ 0.09038505,  0.39845328,  0.1902808 ],\n",
              "         [ 0.08310751, -0.14136384,  0.2701434 ],\n",
              "         [-0.11556603,  0.72542264,  0.85853661],\n",
              "         [-0.21582437,  0.54016981,  0.05350299]]),\n",
              "  'b2': array([[0.],\n",
              "         [0.],\n",
              "         [0.]]),\n",
              "  'h2': 'relu',\n",
              "  'neuronns in layer 2': 3,\n",
              "  'w3': array([[0.11956818],\n",
              "         [0.52710415],\n",
              "         [0.25171784]]),\n",
              "  'b3': array([[0.]]),\n",
              "  'h3': 'softmax',\n",
              "  'neuronns in layer 3': 1},\n",
              " array([[1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        for param, grad in zip(params, grads):\n",
        "            param -= self.lr * grad\n",
        "\n",
        "class Momentum:\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.velocities = None\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.velocities is None:\n",
        "            self.velocities = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.velocities[i] = self.momentum * self.velocities[i] - self.lr * grad\n",
        "            param += self.velocities[i]\n",
        "\n",
        "\n",
        "class Adagrad:\n",
        "    def __init__(self, lr=0.01, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "        self.cache = None\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.cache is None:\n",
        "            self.cache = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.cache[i] += grad ** 2\n",
        "            param -= self.lr * grad / (np.sqrt(self.cache[i]) + self.epsilon)\n",
        "\n",
        "\n",
        "class RMSProp:\n",
        "    def __init__(self, lr=0.001, beta=0.9, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "        self.cache = None\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.cache is None:\n",
        "            self.cache = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.cache[i] = self.beta * self.cache[i] + (1 - self.beta) * grad ** 2\n",
        "            param -= self.lr * grad / (np.sqrt(self.cache[i]) + self.epsilon\n",
        "\n",
        "\n",
        "\n",
        "class Adam:\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        self.t = 0\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m = [np.zeros_like(param) for param in params]\n",
        "            self.v = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        self.t += 1\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n",
        "\n",
        "            # Bias correction\n",
        "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
        "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon\n",
        "\n",
        "\n",
        "class Nadam:\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        self.t = 0\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m = [np.zeros_like(param) for param in params]\n",
        "            self.v = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        self.t += 1\n",
        "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "            # Update biased moment estimates\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n",
        "\n",
        "            # Compute bias-corrected estimates with Nesterov acceleration\n",
        "            m_hat = (self.beta1 * self.m[i] / (1 - self.beta1 ** (self.t + 1)) +\n",
        "                    ((1 - self.beta1) * grad / (1 - self.beta1 ** self.t))\n",
        "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "ynXoTieZS-7O",
        "outputId": "07e601f9-fedd-4014-f9c3-9b579bba3094"
      },
      "id": "ynXoTieZS-7O",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-36-229680b660aa>, line 56)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-229680b660aa>\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    class Adam:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Regularization Code\n",
        "\n",
        "class L2_regularisation:\n",
        "\n",
        "  def __init__(self,layers_dimensions , network):\n",
        "    self.layers_dimensions = layers_dimensions\n",
        "    self.network = network\n",
        "\n",
        "  def Apply_L2(self):\n",
        "    # Function that returns L2 regularisation loss for the given network\n",
        "    L = self.layers_dimensions\n",
        "    res = 0\n",
        "    for j in range(1, L):\n",
        "      res += np.sum(self.network[\"w\" + str(j)] ** 2)\n",
        "\n",
        "    return res\n",
        "\n",
        "class L1_regularisation:\n",
        "\n",
        "  def __init__(self,layers_dimensions , network):\n",
        "    self.layers_dimensions = layers_dimensions\n",
        "    self.network = network\n",
        "\n",
        "  def Apply_L1(self):\n",
        "    # Function that returns L1 regularisation loss for the given network\n",
        "    L = len(self.layers_dimensions)\n",
        "    res = 0\n",
        "    for j in range(1, L):\n",
        "      res += np.sum(np.absolute(self.network[\"w\" + str(j)]))\n",
        "\n",
        "    return res\n",
        "\n",
        "class L2_regularisation_grad:\n",
        "\n",
        "  def __init__(self, weight):\n",
        "    self.weight = weight\n",
        "\n",
        "  def Apply_L2_grad(self):\n",
        "    # Function that returns L2 regularisation gradient for the given Weight matrix / tensor\n",
        "    return 2 * self.weight\n",
        "\n",
        "class L1_regularisation_grad:\n",
        "\n",
        "  def __init__(self, weight):\n",
        "    self.weight = weight\n",
        "\n",
        "  def Apply_L1_grad(self):\n",
        "    # Function that returns L1 regularisation gradient for the given Weight matrix / tensor\n",
        "    return np.sign(self.weight)\n",
        "\n",
        "class ApplyReg:\n",
        "\n",
        "    def __init__(self,reg_function, layer_dimension, network):\n",
        "        self.reg_function= reg_function\n",
        "        self.layer_dimension = layer_dimension\n",
        "        self.network = network\n",
        "        self.do_reg()\n",
        "\n",
        "    def do_reg(self):\n",
        "        if self.reg_function == 'L2':\n",
        "            return L2_regularisation(self.layer_dimension, self.network).Apply_L2()\n",
        "        if self.reg_function == 'L1':\n",
        "            return L1_regularisation(self.layer_dimension, self.network).Apply_L1()\n",
        "\n",
        "class ApplyRegDerv:\n",
        "\n",
        "    def __init__(self,reg_function, weight):\n",
        "        self.reg_function= reg_function\n",
        "        self.weight = weight\n",
        "        self.do_reg_derv()\n",
        "\n",
        "    def do_reg_derv(self):\n",
        "        if self.reg_function == 'L2_d':\n",
        "            return L2_regularisation_grad(self.weight).Apply_L2_grad()\n",
        "        if self.reg_function == 'L1_d':\n",
        "            return L1_regularisation_grad(self.weight).Apply_L1_grad()"
      ],
      "metadata": {
        "id": "H0NBqkSyhNWW"
      },
      "id": "H0NBqkSyhNWW",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, learning_rate=0.01, decay=0.0):\n",
        "        self.eta = learning_rate  # Learning rate\n",
        "        self.decay = decay        # Learning rate decay\n",
        "        self.iterations = 0       # Track steps for decay\n",
        "\n",
        "    def update(self, weights, gradients):\n",
        "        \"\"\"Update weights using gradients.\"\"\"\n",
        "        weights -= self.eta * gradients  # Core SGD update rule\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Decay learning rate after each epoch (optional).\"\"\"\n",
        "        if self.decay > 0:\n",
        "            self.eta = self.eta / (1 + self.decay * self.iterations)\n",
        "            self.iterations += 1"
      ],
      "metadata": {
        "id": "mQDo0hSwfYx4"
      },
      "id": "mQDo0hSwfYx4",
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CalculateAllLoss:\n",
        "  def __init__(self, X_train, y_predicted,network, y_true, primary_loss, weight_decay=0, regularisation_fn=None):\n",
        "    self.y_predicted = y_predicted\n",
        "    self.y_true = y_true\n",
        "    self.network = network\n",
        "    self.X_train = X_train\n",
        "    self.loss_value = primary_loss\n",
        "    self.weight_decay = weight_decay\n",
        "    self.regularisation_fn= regularisation_fn\n",
        "    self.calc_accuracy_loss()\n",
        "\n",
        "\n",
        "  #def overall_loss(self):\n",
        "    \"\"\"\n",
        "    Calculates the total loss of the network.\n",
        "\n",
        "    This includes:\n",
        "    - The primary loss (e.g., Cross-Entropy Loss)\n",
        "    - Optional regularization (like L2 regularization)\n",
        "\n",
        "    Parameters:\n",
        "    - network: The neural network model.\n",
        "    - Y_pred: The predicted output from the network.\n",
        "    - Y_true: The actual labels (ground truth).\n",
        "    - loss_fn: The loss function to be used (e.g., CrossEntropy_loss).\n",
        "    - weight_decay: A coefficient for regularization (default is 0, meaning no regularization).\n",
        "    - regularisation_fn: A function for computing the regularization term (optional).\n",
        "\n",
        "    Returns:\n",
        "    - Total loss value.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "  def calc_accuracy_loss(self):\n",
        "    \"\"\"\n",
        "    Computes the accuracy and loss for a given neural network.\n",
        "\n",
        "    Parameters:\n",
        "    - network: The neural network model.\n",
        "    - X: Input data (features for prediction).\n",
        "    - Y: Actual labels (ground truth values).\n",
        "    - loss_fn: The loss function to be used.\n",
        "    - weight_decay: Regularization strength (default is 0, meaning no regularization).\n",
        "    - regularisation_fn: A function to compute the regularization term (optional).\n",
        "\n",
        "    Returns:\n",
        "    - accuracy: The percentage of correctly classified examples.\n",
        "    - loss: The computed total loss.\n",
        "    \"\"\"\n",
        "    # Get network predictions self.y_prediction from model\n",
        "    layer_dimension = len(self.network)//4\n",
        "    total_loss = self.loss_value  # Compute primary loss\n",
        "    print(f\"total loss : {total_loss}\")\n",
        "    if self.weight_decay > 0 and self.regularisation_fn:\n",
        "        regularized_val = ApplyReg(self.regularisation_fn, layer_dimension, self.network).do_reg()\n",
        "        print(f\"Reg value: {regularized_val}\")\n",
        "        total_loss += self.weight_decay * regularized_val # Add regularization term if applicable  # Compute total loss\n",
        "\n",
        "\n",
        "    # Ensure dimensions match between input and labels\n",
        "    assert self.X_train.shape[1] == self.y_true.shape[1], \"Mismatch in batch size between inputs and labels\"\n",
        "\n",
        "    # Compute accuracy by comparing predicted vs actual labels\n",
        "    batch_size = self.X_train.shape[1]  # Number of examples\n",
        "    correct_predictions = np.sum(np.argmax(self.y_predicted, axis=0) == np.argmax(self.y_true, axis=0))\n",
        "    accuracy = correct_predictions / batch_size  # Compute accuracy as a fraction\n",
        "\n",
        "    return accuracy, total_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "6B3z_WoEeGA5"
      },
      "id": "6B3z_WoEeGA5",
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "class Backpropagation:\n",
        "\n",
        "      def __init__(self, loss_function,X_train , y_train, activation_fn , layers_dimensions ,method, batch_size, optimizer_fn, epochs,weight_decay,regularization_fn,grad_reglr_fn ,X_val=None, t_val=None, use_wandb=False, optim_params=None):\n",
        "\n",
        "        self.loss_function = loss_function\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.activation_fn = activation_fn\n",
        "        self.method = method\n",
        "        self.n = layers_dimensions\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.optimizer_fn = optimizer_fn\n",
        "        self.weight_decay = weight_decay\n",
        "        self.regularization_fn = regularization_fn\n",
        "        self.grad_reglr_fn = grad_reglr_fn\n",
        "        self.X_norm= Normalize(self.X_train).Norm_reshape()\n",
        "        self.y_norm =OneHotEncoder(self.X_train, self.y_train).onehot_encode()\n",
        "        self.batches_number = self.X_norm.shape[1]//self.batch_size\n",
        "        cache, Inter_layers, network, H = Feedforward(self.X_norm,self.n,self.activation_fn,self.method).Forward_prop()\n",
        "        self.inter_layer = Inter_layers\n",
        "        self.H = H\n",
        "        self.network = network\n",
        "        print(\"X_norm shape \",self.X_norm.shape)\n",
        "        print(\"y_norm shape \",self.y_norm.shape)\n",
        "        print(\"H shape \",self.H.shape)\n",
        "        print(\"length self.network \",len(self.network))\n",
        "        print(\"self.batch_no \",self.batches_number)\n",
        "        print(\"self.batch_size \",self.batch_size)\n",
        "\n",
        "        #self.loss = map_losses[loss]\n",
        "        # self.use_wandb = use_wandb\n",
        "        # if t_val is not None:\n",
        "        #     self.X_val = X_val\n",
        "        #     self.layers[0].a_val = X_val\n",
        "        #     self.t_val = t_val\n",
        "        #self.param_init(optimizer, optim_params)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      def backward_propagation(self):\n",
        "        L = len(self.n)\n",
        "        self.y_predicted = self.H\n",
        "        # Initialize variables neesed to keep track of loss\n",
        "        self.eta_hist = []\n",
        "        self.loss_history = []\n",
        "        self.accuracy_history = []\n",
        "        self.loss_hist_val = []\n",
        "        self.accuracy_hist_val = []\n",
        "        self.loss = callloss(self.loss_function, self.y_norm.T, self.H).give_loss()\n",
        "\n",
        "        # Perform Backprop\n",
        "        for epochs in tqdm(range(self.epochs)):\n",
        "\n",
        "          for batch in tqdm(range(self.batches_number)):\n",
        "\n",
        "            X_batch = self.X_norm[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "            self.X_batch = X_batch\n",
        "            print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "            cache, self.inter_layers, self.network, self.H = Feedforward(self.X_batch,self.n,self.activation_fn,self.method).Forward_prop()\n",
        "            self.y_predicted = self.H\n",
        "            print(\"y_predicted shape : \" , self.y_predicted.shape)\n",
        "\n",
        "            y_true_batch = self.y_norm[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "            # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "            self.y_true_batch = y_true_batch\n",
        "            self.y_pred_batch = self.y_predicted\n",
        "            print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "            print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "            assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "            self.loss = callloss(self.loss_function, self.y_true_batch.T, self.y_pred_batch).give_loss()\n",
        "            print(\"loss : \" , self.loss)\n",
        "            accuracy, overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.network, self.y_true_batch,self.loss,self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "            self.accuracy = accuracy\n",
        "            self.overall_loss = overall_loss\n",
        "            self.loss_history.append(self.overall_loss)\n",
        "            self.accuracy_history.append(self.accuracy)\n",
        "            print(\"loss_history : \" , self.loss_history)\n",
        "            print(\"accuracy_history : \" , self.accuracy_history)\n",
        "\n",
        "\n",
        "            grad_a_k_L = call_lossgrad(self.loss_function,self.y_true_batch, self.y_pred_batch).give_gradloss()\n",
        "            print(\"grad_a_k_L : \" , grad_a_k_L.shape)\n",
        "\n",
        "            # Initialising gradients to be calculated in the loop below\n",
        "            grad_w_L = [np.zeros_like(self.network[\"w\" + str(k)]) for k in range(1,L)]\n",
        "            grad_b_L = [np.zeros_like(self.network[\"b\" + str(k)]) for k in range(1,L)]\n",
        "            grad_h_prev_L, grad_a_prev_L = 0, 0\n",
        "\n",
        "\n",
        "            for k in range(L-1,0,-1):\n",
        "              print(\"grad_a_k_l : \" , grad_a_k_L.shape)\n",
        "              A_k = self.inter_layer[\"a\" + str(k)][:, batch*self.batch_size : (batch+1)*self.batch_size]\n",
        "              print(\"A_K : \" , A_k.shape)\n",
        "              do_derivation = apply_activation_derivative(self.activation_fn[k-1], A_k).do_activation_derivative()\n",
        "              print(\"do derivation : \" , do_derivation.shape)\n",
        "\n",
        "              delta_K = grad_a_k_L * do_derivation\n",
        "              delta_K = np.clip(delta_K, -1e5, 1e5)\n",
        "              print(\"delta k : \" , delta_K.shape)\n",
        "\n",
        "\n",
        "\n",
        "              # Gradients wrt Weights (W_k)\n",
        "\n",
        "              A_prev = self.inter_layer[\"h\" + str(k-1)][:, batch*self.batch_size : (batch+1)*self.batch_size]\n",
        "              print(\"A prev : \" , A_prev.shape)\n",
        "              grad_w_L[k - 1] = (delta_K @ A_prev.T) / self.batch_size + self.weight_decay * ApplyRegDerv(self.grad_reglr_fn,self.network[\"w\" + str(k)].T).do_reg_derv()\n",
        "              print(\"len Grad_w_L : \" , grad_w_L[k - 1].shape)\n",
        "\n",
        "              # Gradients wrt Biases (b_k)\n",
        "              grad_b_L[k - 1] = np.sum(delta_K, axis=1, keepdims= True) / self.batch_size\n",
        "              print(\"shape Grad_b_L : \" , grad_b_L[k - 1].shape)\n",
        "              # Gradients wrt Biases (b_k)\n",
        "              grad_b_L[k - 1] = np.sum(delta_K, axis=1, keepdims=True) / self.batch_size  # No reshape needed\n",
        "              print(\"shape Grad_b_L reshaped : \" , grad_b_L[k - 1].shape)\n",
        "              # Gradients wrt hidden layer\n",
        "              # Gradients wrt h_(k-1)\n",
        "\n",
        "              grad_h_prev_L =self.network[\"w\" + str(k)] @ delta_K\n",
        "              print(\"Grad_h_prev_L : \" , grad_h_prev_L.shape)\n",
        "\n",
        "              grad_a_k_L = grad_h_prev_L\n",
        "\n",
        "              # Gradients wrt a_(k-1)\n",
        "              # if(k > 1):\n",
        "              #   prev_act = self.activation_fn[k - 2]\n",
        "              #   a_prev_activations = self.inter_layer[\"a\" + str(k-2)][:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "\n",
        "              #   grad_act_fn_prev = apply_activation_derivative(prev_act , a_prev_activations).do_activation_derivative()\n",
        "              #   print(\"Grad_h_act_fn_prev : \" , grad_h_prev_L.shape)\n",
        "\n",
        "              #   grad_a_prev_L = grad_h_prev_L * grad_act_fn_prev\n",
        "              #   print(\"Grad_a_prev_L : \" , grad_a_prev_L.shape)\n",
        "\n",
        "              # # Gradient for a_{k-1}\n",
        "              #   grad_a_k_L = grad_h_prev_L\n",
        "              #   print(\"if Grad_a_k_L : \" , grad_a_k_L.shape)\n",
        "              # else:\n",
        "              #   # For k=1 (first hidden layer), input layer has no activation derivative\n",
        "              #   grad_a_k_L = grad_h_prev_L  # Propagate gradient without activation derivative\n",
        "              #   print(\"else Grad_a_k_L : \" , grad_a_k_L.shape)\n",
        "\n",
        "            # Inside the batch loop after computing gradients\n",
        "            # w_updates = CallOptimizers(self.optimizer_fn, grad_w_L).apply_optimization()\n",
        "            # b_updates = CallOptimizers(self.optimizer_fn, grad_b_L).apply_optimization()\n",
        "\n",
        "            # for k in range(1, L):\n",
        "            #   self.network[\"w\" + str(k)] -= w_updates[\"w\" + str(k)]\n",
        "            #   self.network[\"b\" + str(k)] -= b_updates[\"b\" + str(k)]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jHfISCRCqJDP"
      },
      "id": "jHfISCRCqJDP",
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Backpropagation(loss_function=\"se\",X_train= X_train, y_train= y_train,layers_dimensions= [784, 128, 64, 10],activation_fn=['sigmoid', 'sigmoid', 'sigmoid'], batch_size=10000,optimizer_fn=\"Momentum\", epochs= 5,weight_decay=0.01,grad_reglr_fn= \"L2_d\", regularization_fn=\"L2\", method= \"Xavier_U\" ).backward_propagation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "_WFk5jtH19Ti",
        "outputId": "c43401ef-48e5-4d16-c9c8-629a6e00b41f"
      },
      "id": "_WFk5jtH19Ti",
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-7fa67dacb3dc>:10: RuntimeWarning: overflow encountered in exp\n",
            "  sig = np.where(self.m >= 0,1/(1 - np.exp(-self.m)),np.exp(self.m)/(np.exp(-self.m) - 1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_norm shape  (784, 60000)\n",
            "y_norm shape  (10, 60000)\n",
            "H shape  (10, 60000)\n",
            "length self.network  12\n",
            "self.batch_no  6\n",
            "self.batch_size  10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_batch shape :  (784, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/6 [00:00<?, ?it/s]\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_predicted shape :  (10, 10000)\n",
            "y_pred_batch shape :  (10, 10000)\n",
            "y_true_batch shape :  (10, 10000)\n",
            "loss :  None\n",
            "total loss : None\n",
            "Reg value: 306.31485732824206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for +: 'NoneType' and 'float'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-7cb6bce29975>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"se\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers_dimensions\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Momentum\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_reglr_fn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"L2_d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"Xavier_U\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-96-5b46bd198c5c>\u001b[0m in \u001b[0;36mbackward_propagation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_true_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgive_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss : \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalculateAllLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_true_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularization_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_accuracy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverall_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverall_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-90-d672e1e1b448>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X_train, y_predicted, network, y_true, primary_loss, weight_decay, regularisation_fn)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularisation_fn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mregularisation_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_accuracy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-90-d672e1e1b448>\u001b[0m in \u001b[0;36mcalc_accuracy_loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mregularized_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mApplyReg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularisation_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reg value: {regularized_val}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_decay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mregularized_val\u001b[0m \u001b[0;31m# Add regularization term if applicable  # Compute total loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'float'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " a,b,c,d = Feedforward(X_new,[784, 128, 64, 10],['sigmoid','relu','softmax'],\"Xavier_U\").Forward_prop()\n",
        "c[\"w3\"].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "kGOmz4yXVtvE",
        "outputId": "be439e84-3423-483b-d304-c4f3275c7700"
      },
      "id": "kGOmz4yXVtvE",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-340ae0364150>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Xavier_U\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mForward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"w3\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-e93e048af4ef>\u001b[0m in \u001b[0;36mForward_prop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mAct_fn_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"h\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPre_Feedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPrev_layer_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAct_fn_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreactivation_cal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-e93e048af4ef>\u001b[0m in \u001b[0;36mPreactivation_cal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrev_layer_H\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrev_layer_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m#self.cache[\"Input\"] = self.Prev_layer_H\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-7fa67dacb3dc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, activation_function, value)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-7fa67dacb3dc>\u001b[0m in \u001b[0;36mdo_activation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-7fa67dacb3dc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, m)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0muse_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-7fa67dacb3dc>\u001b[0m in \u001b[0;36muse_sigmoid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0muse_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0msig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#if self.m >= 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c,d = Feedforward(X,[2,3,2,1],['sigmoid','relu','softmax'],\"Xavier_U\").Forward_prop()\n",
        "a,b,c,d"
      ],
      "metadata": {
        "id": "92hGBoNHLYP3"
      },
      "id": "92hGBoNHLYP3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Backpropagation(loss_function=\"se\",X_train= X_train, y_train= y_train,layers_dimensions= [784, 128, 64 ,10],activation_fn=['sigmoid', 'sigmoid', 'sigmoid'], batch_size=10000,optimizer_fn=\"Adam\", epochs= 5,weight_decay=0,grad_reglr_fn=None, method= \"Xavier_U\" ).backward_propogation()"
      ],
      "metadata": {
        "id": "wss0tLWsslKV"
      },
      "id": "wss0tLWsslKV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "class Backprop:\n",
        "\n",
        "      def __init__(self, loss_function,X_train , y_train, activation_fn , layers_dimensions ,method, batch_size, optimizer_fn, epochs, X_val=None, t_val=None, use_wandb=False, optim_params=None):\n",
        "\n",
        "        self.loss_function = loss_function\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.activation_fn = activation_fn\n",
        "        self.method = method\n",
        "        self.n = layers_dimensions\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.optimizer_fn = optimizer_fn\n",
        "        self.X_norm= Normalize(self.X_train).Norm_reshape()\n",
        "        self.y_norm =OneHotEncoder(self.X_train, self.y_train).onehot_encode()\n",
        "        self.batches_number = math.ceil(self.y_norm.shape[1]/self.batch_size)\n",
        "        cache, Inter_layers, network, H = Feedforward(self.X_norm,self.n,self.activation_fn,self.method).Forward_prop()\n",
        "        self.inter_layer = Inter_layers\n",
        "        self.H = H\n",
        "        self.network = network\n",
        "        print(self.X_norm.shape)\n",
        "        print(self.y_norm.shape)\n",
        "        print(self.H.shape)\n",
        "        print(len(self.network))\n",
        "\n",
        "        #self.loss = map_losses[loss]\n",
        "        # self.use_wandb = use_wandb\n",
        "        # if t_val is not None:\n",
        "        #     self.X_val = X_val\n",
        "        #     self.layers[0].a_val = X_val\n",
        "        #     self.t_val = t_val\n",
        "        #self.param_init(optimizer, optim_params)\n",
        "\n",
        "\n",
        "      def backward_propogation(self):\n",
        "        L = len(self.n)\n",
        "        self.y_predicted = self.H\n",
        "        # Initialize variables neesed to keep track of loss\n",
        "        self.eta_hist = []\n",
        "        self.loss_hist = []\n",
        "        self.accuracy_hist = []\n",
        "        self.loss_hist_val = []\n",
        "        self.accuracy_hist_val = []\n",
        "        self.loss = callloss(self.loss_function, self.y_norm.T, self.y_predicted).give_loss()\n",
        "\n",
        "        # Perform Backprop\n",
        "        for epochs in tqdm(range(self.epochs)):\n",
        "\n",
        "          for batch in range(self.batches_number):\n",
        "            self.grad_pre_act = [np.zeros(2)] * L\n",
        "            self.grad_a_w = [np.zeros(2)] * L\n",
        "            self.grad_a_b = [np.zeros(2)] * L\n",
        "            grad_a_w_update = [np.zeros(2)] * L\n",
        "            grad_a_b_updade = [np.zeros(2)] * L\n",
        "\n",
        "\n",
        "            y_true_batch = self.y_norm[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "            y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "            self.y_true_batch = y_true_batch\n",
        "            self.y_pred_batch = y_pred_batch\n",
        "            #print(self.activation_fn[-1])\n",
        "\n",
        "              # try:\n",
        "              #     if self.loss_hist[-1] > self.loss_hist[-2]:\n",
        "              #         for layer in self.layers[1:]:\n",
        "              #             layer.W_optimizer.set_params({\"eta\":self.optimizer.eta/2})\n",
        "              #             layer.b_optimizer.set_params({\"eta\":self.optimizer.eta/2})\n",
        "              #         flag = 1\n",
        "              # except:\n",
        "              #     pass\n",
        "\n",
        "              # if flag == 1:\n",
        "              #     break\n",
        "\n",
        "              # self.layers[-1].cross_grad = self.loss.diff()\n",
        "            self.grad_act  = call_lossgrad(self.loss_function,y_true_batch, y_pred_batch).give_gradloss()\n",
        "\n",
        "            print(\"Shape of grad_act[-1]:\", np.shape(self.grad_act))\n",
        "            print(\"Shape of activation derivative output:\", np.shape(apply_activation_derivative(self.activation_fn[-1], self.inter_layer[\"a\" + str(L- 2)]).do_activation_derivative()))\n",
        "\n",
        "\n",
        "\n",
        "            self.grad_pre_act[-1] = self.grad_act[-1] @ apply_activation_derivative(self.activation_fn[-1], self.inter_layer[\"a\" + str(L-1)]).do_activation_derivative()[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "            # print(type(apply_activation_derivative(self.activation_fn, self.inter_layer[\"a\" + str(L-1)]).do_activation_derivative()))\n",
        "\n",
        "            self.grad_a_w[-1] = self.grad_pre_act[-1] @ self.inter_layer[\"a\" + str(L-2)][:, batch*self.batch_size:(batch+1)*self.batch_size].T\n",
        "            self.grad_a_b[-1] = - np.sum(self.grad_act[-1], axis=1)\n",
        "\n",
        "            grad_a_w_update = CallOptimizers(self.optimizer_fn, self.grad_a_w[-1]).apply_optimization()\n",
        "            grad_a_b_updade = CallOptimizers(self.optimizer_fn, self.grad_a_b[-1]).apply_optimization()\n",
        "\n",
        "              # self.layers[-1].a_grad = self.loss.diff(self.t_batch, self.y_batch)\n",
        "              # self.layers[-1].h_grad = self.layers[-1].a_grad * self.layers[-1].activation.diff(self.layers[-1].h[:, batch*self.batch_size:(batch+1)*self.batch_size])\n",
        "\n",
        "              # self.layers[-1].W_grad = self.layers[-1].h_grad @ self.layers[-2].a[:, batch*self.batch_size:(batch+1)*self.batch_size].T\n",
        "              # self.layers[-1].W_update = self.layers[-1].W_optimizer.get_update(self.layers[-1].W_grad)\n",
        "\n",
        "              # self.layers[-1].b_grad = -np.sum(self.layers[-1].h_grad, axis=1).reshape(-1,1)\n",
        "              # self.layers[-1].b_update = self.layers[-1].b_optimizer.get_update(self.layers[-1].b_grad)\n",
        "\n",
        "              # print(\"Last Layer\")\n",
        "              # print(\"a_grad shape:\", self.layers[-1].a_grad.shape)\n",
        "              # print(\"h_grad shape:\", self.layers[-1].h_grad.shape)\n",
        "              # print(\"W_grad shape:\", self.layers[-1].W_grad.shape)\n",
        "              # print(\"W_update shape:\", self.layers[-1].W_update.shape)\n",
        "              # print(\"W_shape:\", self.layers[-1].W.shape)\n",
        "              # print(\"a_grad:\\n\", self.layers[-1].a_grad)\n",
        "              # print(\"h_grad:\\n\", self.layers[-1].h_grad)\n",
        "              # print(\"W_grad:\\n\", self.layers[-1].W_grad)\n",
        "\n",
        "            # assert self.layers[-1].W_update.shape == self.layers[-1].W.shape, \"Sizes don't match\"\n",
        "            assert grad_a_w_update == self.network[\"w\" + str(L-1)].shape, \"Sizes don't match\"\n",
        "\n",
        "\n",
        "              # Backpropogation for the remaining layers\n",
        "            for i in range(L - 2, 0, -1):\n",
        "              self.grad_act[i] =  self.network[\"w\" + str(i + 1)].T @ self.grad_pre_act[i + 1]\n",
        "              self.grad_pre_act[i] = self.grad_act[i] * apply_activation_derivative(self.activation_fn[i], self.inter_layer[\"h\" + str(i)]).get_derivative()\n",
        "              self.grad_a_b[i] = - np.sum(self.grad_pre_act[i], axis = 1)\n",
        "              self.grad_a_w[i] = self.grad_pre_act[i] @ self.inter_layer[\"h\" + str(i-1)].T\n",
        "\n",
        "              grad_a_w_update = CallOptimizers(self.optimizer_fn, self.grad_a_w[i]).apply_optimization()\n",
        "              grad_a_b_update = CallOptimizers(self.optimizer_fn, self.grad_a_w[i]).apply_optimization()\n",
        "\n",
        "\n",
        "                # Update the weights\n",
        "              for k in range(L-1):\n",
        "                  self.network[\"w\" + str(k)] = self.network[\"w\" + str(k)] - self.grad_a_w_update\n",
        "                  self.network[\"b\" + str(k)] = self.network[\"w\" + str(k)] - self.grad_b_w_update\n",
        "\n",
        "              cache, self.Inter_layers, self.network, self.y_predicted = Feedforward(self.X_train,self.n,self.activation_fn,self.method).Forward_prop()\n"
      ],
      "metadata": {
        "id": "MhJrZkPpp7Zv"
      },
      "id": "MhJrZkPpp7Zv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TMAZN5qaVr3t"
      },
      "id": "TMAZN5qaVr3t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Backprop(loss_function=\"se\",X_train= X_train, y_train= y_train,layers_dimensions= [784, 128, 64 ,10],activation_fn=['sigmoid', 'sigmoid', 'softmax'], batch_size=10000,optimizer_fn=\"Adam\", epochs= 5, method= \"Xavier_U\" ).backward_propogation()"
      ],
      "metadata": {
        "id": "CyW79eripCll"
      },
      "id": "CyW79eripCll",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new= Normalize(X_train).Norm_reshape()\n",
        "y_new =OneHotEncoder(X_train,y_train).onehot_encode()"
      ],
      "metadata": {
        "id": "lhnh7_96ftQs"
      },
      "id": "lhnh7_96ftQs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new.shape"
      ],
      "metadata": {
        "id": "gFlE6dbNAJE7"
      },
      "id": "gFlE6dbNAJE7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c,d = Feedforward(X,[2,3,3,1],['relu','relu','softmax'],\"Xavier_U\").Forward_prop()\n",
        "a,b,c,d"
      ],
      "metadata": {
        "id": "NV7gYASA0m8t"
      },
      "id": "NV7gYASA0m8t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(b[\"a\" + str(4-1)])"
      ],
      "metadata": {
        "id": "Xmxy4rN2cTM-"
      },
      "id": "Xmxy4rN2cTM-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(apply_activation_derivative([\"sigmoid\"], b[\"a\" + str(4-1)]).do_activation_derivative()))"
      ],
      "metadata": {
        "id": "X06X6gfoV1Ol"
      },
      "id": "X06X6gfoV1Ol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2064d83c-b27e-46e6-bef2-c24d9faf70ff",
      "metadata": {
        "id": "2064d83c-b27e-46e6-bef2-c24d9faf70ff"
      },
      "outputs": [],
      "source": [
        "Initilize(3,5,\"Xavier_N\").Init_weight(\"Xavier_N\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [relu,relu,sigmoid]\n",
        "print(x[1])"
      ],
      "metadata": {
        "id": "AIOXHXcuRF9-"
      },
      "id": "AIOXHXcuRF9-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weight_bias([4,5,3,1],['relu','relu','sigmoid'],\"Xavier_N\").Init_network([4,5,3,1])"
      ],
      "metadata": {
        "id": "4NKWIZNDdN7u"
      },
      "id": "4NKWIZNDdN7u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callloss(\"se\",y_new,d).give_loss()"
      ],
      "metadata": {
        "id": "BOwYYfPZCFzT"
      },
      "id": "BOwYYfPZCFzT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CmG8ToWcnOR7"
      },
      "id": "CmG8ToWcnOR7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}