{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48961999-4605-403b-bf90-8139c9597121",
      "metadata": {
        "id": "48961999-4605-403b-bf90-8139c9597121"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "class Optimizer:\n",
        "  def __init__(self, loss_function, X_train, y_train, activation_fn, layers_dimensions ,method, batch_size, epochs,validX_train = None, validy_train = None,weight_decay = 0, eta = 0.01, beta = 0.9, beta2 = 0.999, regularization_fn = \"L2\",grad_reglr_fn = \"L2_d\" , use_wandb=False):\n",
        "      self.loss_function = loss_function\n",
        "      self.X_train = X_train\n",
        "      self.y_train = y_train\n",
        "      self.validX_train = validX_train\n",
        "      self.validy_train = validy_train\n",
        "      self.activation_fn = activation_fn\n",
        "      self.method = method\n",
        "      self.n = layers_dimensions\n",
        "      self.batch_size = batch_size\n",
        "      self.epochs = epochs\n",
        "      self.weight_decay = weight_decay\n",
        "      self.eta = eta\n",
        "      self.epsilon =1e-10\n",
        "      self.beta = beta\n",
        "      self.beta2 = beta2\n",
        "      self.regularization_fn = regularization_fn\n",
        "      self.grad_reglr_fn = grad_reglr_fn\n",
        "      self.batches_number = self.X_train.shape[1]//self.batch_size\n",
        "      z = Weight_bias(self.n, self.activation_fn, self.y_train, self.method)\n",
        "      self.init_network = z.Init_network()\n",
        "      self.min_eta = 1e-4\n",
        "      self.use_wandb = use_wandb\n",
        "      # wandb.init(project= \"DA6401_Assignment_1\" , name = \"RMSProp with validation data of 1/3rd of 60k rows\")\n",
        "\n",
        "\n",
        "class SGD(Optimizer):\n",
        "\n",
        "  def Gradient_descent(self):\n",
        "\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "    L = len(self.n) - 1\n",
        "    # Initialize variables neesed to keep track of loss\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    overall_loss = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "\n",
        "    for epochs in range(self.epochs):\n",
        "\n",
        "      for batch in tqdm(range(self.batches_number)):\n",
        "        print(f\"   batch no: {batch+1}     Train acc: {accuracy}     val acc: {val_accuracy}    Train Loss: {train_loss}\" )\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:  # Ensure we have enough data points\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)  # Compute standard deviation of recent losses\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "\n",
        "        for j in range(L):\n",
        "          self.fw_network[j].weight -= self.eta * self.bp_network[j].d_w\n",
        "          self.fw_network[j].bias -= self.eta * self.bp_network[j].d_b\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 4)\n",
        "        train_loss = round(loss , 4)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 4)\n",
        "          val_loss = round(v_loss , 4)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        wandb.log({'epoch': epochs,  # Use current epoch, not total epochs\n",
        "                    'validation_accuracy': val_accuracy if self.validX_train is not None else 0,\n",
        "                    'accuracy': accuracy,\n",
        "                    'validation_loss': val_loss if self.validX_train is not None else 0,\n",
        "                    'loss': train_loss\n",
        "                })\n",
        "\n",
        "    return self.loss_history, self.training_history, self.eta_history , self.validation_history\n",
        "\n",
        "\n",
        "\n",
        "class MGD(Optimizer):\n",
        "\n",
        "  def momentum_GD(self):\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "    L = len(self.n) - 1\n",
        "    # Initialize variables neesed to keep track of loss\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "\n",
        "    u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "    for epochs in range(self.epochs):\n",
        "\n",
        "      for batch in tqdm(range(self.batches_number)):\n",
        "        print(f\"   batch no: {batch+1}     Train acc: {accuracy}     val acc: {val_accuracy}    Train Loss: {train_loss}\" )\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:  # Ensure we have enough data points\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)  # Compute standard deviation of recent losses\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "        # self.beta = min(1 - 2 ** (-1 - math.log((self.calls / 250.0) + 1, 2)), self.beta)\n",
        "\n",
        "        for j in range(L):\n",
        "          u_w[j] = u_w[j] * self.beta + self.bp_network[j].d_w * self.eta\n",
        "          u_b[j] = u_b[j] * self.beta + self.bp_network[j].d_b * self.eta\n",
        "\n",
        "          self.fw_network[j].weight -= u_w[j]\n",
        "          self.fw_network[j].bias -= u_b[j]\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 4)\n",
        "        train_loss = round(loss , 4)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 4)\n",
        "          val_loss = round(v_loss , 4)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        # wandb.log({'epoch': epochs,  # Use current epoch, not total epochs\n",
        "        #             'validation_accuracy': val_accuracy if self.validX_train is not None else 0,\n",
        "        #             'accuracy': accuracy,\n",
        "        #             'validation_loss': val_loss if self.validX_train is not None else 0,\n",
        "        #             'loss': train_loss\n",
        "        #         })\n",
        "\n",
        "    return self.loss_history, self.training_history, self.eta_history , self.validation_history\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NAG(Optimizer):\n",
        "\n",
        "  def Nesterov_AGD(self):\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "    L = len(self.n) - 1\n",
        "    # Initialize variables neesed to keep track of loss\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "    u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "    lookleap_network = self.init_network[:]\n",
        "\n",
        "    for epochs in range(self.epochs):\n",
        "\n",
        "      for batch in tqdm(range(self.batches_number)):\n",
        "        print(f\"   batch no: {batch+1}     Train acc: {accuracy}     val acc: {val_accuracy}    Train Loss: {train_loss}\" )\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:  # Ensure we have enough data points\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)  # Compute standard deviation of recent losses\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "\n",
        "        for j in range(L):\n",
        "          u_w[j] = u_w[j] * self.beta + self.bp_network[j].d_w * self.eta\n",
        "          u_b[j] = u_b[j] * self.beta + self.bp_network[j].d_b * self.eta\n",
        "\n",
        "          self.fw_network[j].weight -= u_w[j]\n",
        "          self.fw_network[j].bias -= u_b[j]\n",
        "\n",
        "          lookleap_network[j].weight -= (self.eta * self.bp_network[j].d_w + self.beta * u_w[j])\n",
        "          lookleap_network[j].bias -= (self.eta * self.bp_network[j].d_b + self.beta * u_b[j])\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 4)\n",
        "        train_loss = round(loss , 4)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 4)\n",
        "          val_loss = round(v_loss , 4)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        # wandb.log({'epoch': epochs,  # Use current epoch, not total epochs\n",
        "        #             'validation_accuracy': val_accuracy if self.validX_train is not None else 0,\n",
        "        #             'accuracy': accuracy,\n",
        "        #             'validation_loss': val_loss if self.validX_train is not None else 0,\n",
        "        #             'loss': train_loss\n",
        "        #         })\n",
        "    return self.loss_history, self.training_history, self.eta_history , self.validation_history\n",
        "\n",
        "\n",
        "\n",
        "class RMSProp(Optimizer):\n",
        "\n",
        "  def rms_GD(self):\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "    L = len(self.n) - 1\n",
        "    # Initialize variables neesed to keep track of loss\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "    u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "    for epochs in range(self.epochs):\n",
        "\n",
        "      for batch in tqdm(range(self.batches_number)):\n",
        "        print(f\"   batch no: {batch+1}     Train acc: {accuracy}     val acc: {val_accuracy}    Train Loss: {train_loss}\" )\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:  # Ensure we have enough data points\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)  # Compute standard deviation of recent losses\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "\n",
        "        for j in range(L):\n",
        "          u_w[j] = u_w[j] * self.beta +  (1  - self.beta) * (self.bp_network[j].d_w)**2\n",
        "          u_b[j] = u_b[j] * self.beta +  (1  - self.beta) * (self.bp_network[j].d_b)**2\n",
        "\n",
        "          self.fw_network[j].weight -= (self.eta / np.sqrt(u_w[j] + self.epsilon)) * self.bp_network[j].d_w\n",
        "          self.fw_network[j].bias -= (self.eta / np.sqrt(u_b[j] + self.epsilon)) * self.bp_network[j].d_b\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 4)\n",
        "        train_loss = round(loss , 4)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 4)\n",
        "          val_loss = round(v_loss , 4)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        # wandb.log({'epoch': epochs,  # Use current epoch, not total epochs\n",
        "        #             'validation_accuracy': val_accuracy if self.validX_train is not None else 0,\n",
        "        #             'accuracy': accuracy,\n",
        "        #             'validation_loss': val_loss if self.validX_train is not None else 0,\n",
        "        #             'loss': train_loss\n",
        "        #         })\n",
        "\n",
        "    return self.loss_history, self.training_history, self.eta_history , self.validation_history\n",
        "\n",
        "\n",
        "class ADAM(Optimizer):\n",
        "\n",
        "  def adam_GD(self):\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "    L = len(self.n) - 1\n",
        "    # Initialize variables neesed to keep track of loss\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "\n",
        "    i = 0\n",
        "    u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "    v_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    v_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "    for epochs in range(self.epochs):\n",
        "\n",
        "      for batch in tqdm(range(self.batches_number)):\n",
        "        print(f\"   batch no: {batch+1}     Train acc: {accuracy}     val acc: {val_accuracy}    Train Loss: {train_loss}\" )\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:  # Ensure we have enough data points\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)  # Compute standard deviation of recent losses\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "\n",
        "        for j in range(L):\n",
        "          u_w[j] = u_w[j] * self.beta +  (1  - self.beta) * (self.bp_network[j].d_w)\n",
        "          u_b[j] = u_b[j] * self.beta +  (1  - self.beta) * (self.bp_network[j].d_b)\n",
        "          v_w[j] = v_w[j] * self.beta2 +  (1  - self.beta2) * (self.bp_network[j].d_w)**2\n",
        "          v_b[j] = v_b[j] * self.beta2 +  (1  - self.beta2) * (self.bp_network[j].d_b)**2\n",
        "\n",
        "          u_w_pred = u_w[j] / (1 - np.power(self.beta , i+1))\n",
        "          u_b_pred = u_b[j] / (1 - np.power(self.beta , i+1))\n",
        "          v_w_pred = v_w[j] / (1 - np.power(self.beta2 , i+1))\n",
        "          v_b_pred = v_b[j] / (1 - np.power(self.beta2 , i+1))\n",
        "\n",
        "          self.fw_network[j].weight -= (self.eta / (np.sqrt(v_w_pred) + self.epsilon)) * u_w_pred\n",
        "          self.fw_network[j].bias -= (self.eta / (np.sqrt(v_b_pred) + self.epsilon)) * u_b_pred\n",
        "\n",
        "        self.init_network = self.fw_network\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 4)\n",
        "        train_loss = round(loss , 4)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "        i+=1\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 4)\n",
        "          val_loss = round(v_loss , 4)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        # wandb.log({'epoch': epochs,  # Use current epoch, not total epochs\n",
        "        #             'validation_accuracy': val_accuracy if self.validX_train is not None else 0,\n",
        "        #             'accuracy': accuracy,\n",
        "        #             'validation_loss': val_loss if self.validX_train is not None else 0,\n",
        "        #             'loss': train_loss\n",
        "        #         })\n",
        "\n",
        "    return self.loss_history, self.training_history, self.eta_history , self.validation_history\n",
        "\n",
        "\n",
        "class GiveOptimizers(Optimizer):\n",
        "    def __init__(self, optimization_fn, loss_function, X_train, y_train, activation_fn, layers_dimensions ,method, batch_size, epochs,  validX_train , validy_train ,weight_decay = 0, eta = 0.01, beta = 0.9, beta2 = 0.999, regularization_fn = \"L2\",grad_reglr_fn = \"L2_d\", use_wandb=False):\n",
        "      self.optimization_function = optimization_fn.lower()\n",
        "      super().__init__( loss_function, X_train, y_train, activation_fn, layers_dimensions ,method, batch_size, epochs, validX_train, validy_train ,weight_decay = 0, eta = 0.01, beta = 0.9, beta2 = 0.999, regularization_fn = \"L2\",grad_reglr_fn = \"L2_d\" , use_wandb=False)\n",
        "\n",
        "    def apply_optimization(self):\n",
        "      if self.optimization_function == 'sgd':\n",
        "          return SGD(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs,  self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn, self.use_wandb).Gradient_descent()\n",
        "      elif self.optimization_function == 'momentum':\n",
        "          return MGD(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn, self.use_wandb).momentum_GD()\n",
        "      elif self.optimization_function == 'nag':\n",
        "          return NAG(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn, self.use_wandb).Nesterov_AGD()\n",
        "      elif self.optimization_function == 'rmsp':\n",
        "          return RMSProp(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn, self.use_wandb).rms_GD()\n",
        "      elif self.optimization_function == 'adam':\n",
        "          return ADAM(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn, self.use_wandb).adam_GD()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}