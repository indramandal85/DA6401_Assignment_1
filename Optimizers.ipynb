{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48961999-4605-403b-bf90-8139c9597121",
      "metadata": {
        "id": "48961999-4605-403b-bf90-8139c9597121"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "class Optimizer:\n",
        "  def __init__(self, loss_function, X_train, y_train, activation_fn, layers_dimensions ,method, batch_size, epochs,validX_train = None, validy_train = None,weight_decay = 0, eta = 0.01, beta = 0.9, beta2 = 0.999, regularization_fn = \"L2\",grad_reglr_fn = \"L2_d\" , use_wandb=False):\n",
        "      self.loss_function = loss_function\n",
        "      self.X_train = X_train\n",
        "      self.y_train = y_train\n",
        "      self.validX_train = validX_train\n",
        "      self.validy_train = validy_train\n",
        "      self.activation_fn = activation_fn\n",
        "      self.method = method\n",
        "      self.n = layers_dimensions\n",
        "      self.batch_size = batch_size\n",
        "      self.epochs = epochs\n",
        "      self.weight_decay = weight_decay\n",
        "      self.eta = eta\n",
        "      self.epsilon =1e-10\n",
        "      self.beta = beta\n",
        "      self.beta2 = beta2\n",
        "      self.regularization_fn = regularization_fn\n",
        "      self.grad_reglr_fn = grad_reglr_fn\n",
        "      self.batches_number = self.X_train.shape[1]//self.batch_size\n",
        "      z = Weight_bias(self.n, self.activation_fn, self.y_train, self.method)\n",
        "      self.init_network = z.Init_network()\n",
        "      self.min_eta = 1e-4\n",
        "      self.use_wandb = use_wandb\n",
        "\n",
        "\n",
        "\n",
        "class SGD(Optimizer):\n",
        "\n",
        "  def Gradient_descent(self):\n",
        "\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    overall_loss = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "\n",
        "    for epochs in range(self.epochs):\n",
        "\n",
        "      for batch in tqdm(range(self.batches_number)):\n",
        "        print(f\"   batch no: {batch+1}     Train acc: {accuracy}     val acc: {val_accuracy}    Train Loss: {train_loss}\" )\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "\n",
        "        for j in range(L):\n",
        "          self.fw_network[j].weight -= self.eta * self.bp_network[j].d_w\n",
        "          self.fw_network[j].bias -= self.eta * self.bp_network[j].d_b\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 4)\n",
        "        train_loss = round(loss , 4)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 4)\n",
        "          val_loss = round(v_loss , 4)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "    return self.loss_history, self.training_history, self.eta_history , self.validation_history\n",
        "\n",
        "\n",
        "\n",
        "class MGD(Optimizer):\n",
        "\n",
        "  def momentum_GD(self):\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "    L = len(self.n) - 1\n",
        "\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "\n",
        "    u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "    for epochs in range(self.epochs):\n",
        "\n",
        "      for batch in tqdm(range(self.batches_number)):\n",
        "        print(f\"   batch no: {batch+1}     Train acc: {accuracy}     val acc: {val_accuracy}    Train Loss: {train_loss}\" )\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "\n",
        "\n",
        "        for j in range(L):\n",
        "          u_w[j] = u_w[j] * self.beta + self.bp_network[j].d_w * self.eta\n",
        "          u_b[j] = u_b[j] * self.beta + self.bp_network[j].d_b * self.eta\n",
        "\n",
        "          self.fw_network[j].weight -= u_w[j]\n",
        "          self.fw_network[j].bias -= u_b[j]\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 4)\n",
        "        train_loss = round(loss , 4)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 4)\n",
        "          val_loss = round(v_loss , 4)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "    return self.loss_history, self.training_history, self.eta_history , self.validation_history\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NAG(Optimizer):\n",
        "\n",
        "  def Nesterov_AGD(self):\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "      assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "    L = len(self.n) - 1\n",
        "\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "    overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "    u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "    lookleap_network = self.init_network[:]\n",
        "\n",
        "    for epochs in range(self.epochs):\n",
        "\n",
        "      for batch in tqdm(range(self.batches_number)):\n",
        "        print(f\"   batch no: {batch+1}     Train acc: {accuracy}     val acc: {val_accuracy}    Train Loss: {train_loss}\" )\n",
        "\n",
        "        X_batch = self.X_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.X_batch = X_batch\n",
        "        # print(\"X_batch shape : \" , self.X_batch.shape)\n",
        "\n",
        "        y_true_batch = self.y_train[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        self.y_true_batch = y_true_batch\n",
        "        fw_network = Feedforward(self.X_batch,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "        self.fw_network = fw_network\n",
        "        self.y_predicted = self.fw_network[L-1].h\n",
        "        self.y_pred_batch = self.y_predicted\n",
        "        # self.y_pred_batch = self.y_predicted[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        # print(\"y_pred_batch shape : \" , self.y_pred_batch.shape)\n",
        "        # print(\"y_true_batch shape : \" , self.y_true_batch.shape)\n",
        "        assert(self.y_true_batch.shape[1] == self.y_pred_batch.shape[1])\n",
        "\n",
        "        self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "        overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "        self.loss_history.append(overall_loss)\n",
        "        bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch, self.fw_network, self.weight_decay, self.batch, self.batch_size, self.activation_fn ).backward_propagation()\n",
        "        self.bp_network = bp_network\n",
        "\n",
        "        if len(self.loss_history) > 5:\n",
        "          recent_losses = self.loss_history[-5:]\n",
        "          loss_std = np.std(recent_losses)\n",
        "\n",
        "          if self.loss_history[-1] > self.loss_history[-2]:\n",
        "              self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "        self.eta_history.append(self.eta)\n",
        "\n",
        "        for j in range(L):\n",
        "          u_w[j] = u_w[j] * self.beta + self.bp_network[j].d_w * self.eta\n",
        "          u_b[j] = u_b[j] * self.beta + self.bp_network[j].d_b * self.eta\n",
        "\n",
        "          self.fw_network[j].weight -= u_w[j]\n",
        "          self.fw_network[j].bias -= u_b[j]\n",
        "\n",
        "          lookleap_network[j].weight -= (self.eta * self.bp_network[j].d_w + self.beta * u_w[j])\n",
        "          lookleap_network[j].bias -= (self.eta * self.bp_network[j].d_b + self.beta * u_b[j])\n",
        "\n",
        "        acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "        accuracy = round(acc , 4)\n",
        "        train_loss = round(loss , 4)\n",
        "        self.training_history.append(CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "        if self.validX_train is not None:\n",
        "          network = Feedforward(self.validX_train,self.activation_fn,self.method, self.init_network).Forward_prop()\n",
        "          self.network = network\n",
        "          self.valy_predicted = self.network[L-1].h\n",
        "          val_acc, v_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "          val_accuracy = round(val_acc , 4)\n",
        "          val_loss = round(v_loss , 4)\n",
        "          val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "          self.validation_history.append(CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network, self.validy_train, self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss())\n",
        "\n",
        "\n",
        "    return self.loss_history, self.training_history, self.eta_history , self.validation_history\n",
        "\n",
        "\n",
        "\n",
        "class RMSProp(Optimizer):\n",
        "\n",
        "  def rms_GD(self):\n",
        "\n",
        "    assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "    if self.validy_train is not None:\n",
        "        assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "\n",
        "    L = len(self.n) - 1\n",
        "\n",
        "    # Initialize loss and training tracking variables\n",
        "    self.eta_history = []\n",
        "    self.loss_history = []\n",
        "    self.training_history = []\n",
        "    self.valid_loss_history = []\n",
        "    self.validation_history = []\n",
        "\n",
        "    overall_loss = 0\n",
        "    self.batch = 0\n",
        "    accuracy = 0\n",
        "    val_accuracy = 0\n",
        "    val_overall_loss = 0\n",
        "    train_loss = 0\n",
        "\n",
        "    # Initialize squared gradient accumulators for RMSProp\n",
        "    u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "    u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "    for epoch in range(self.epochs):\n",
        "        print(f\"Epoch {epoch+1}/{self.epochs} | Train Acc: {accuracy} | Val Acc: {val_accuracy} | Train Loss: {train_loss}\")\n",
        "\n",
        "        for batch in tqdm(range(self.batches_number)):\n",
        "            print(f\"   batch no: {batch+1}     Train acc: {accuracy}     val acc: {val_accuracy}    Train Loss: {train_loss}\")\n",
        "\n",
        "            # Mini-batch selection\n",
        "            X_batch = self.X_train[:, batch * self.batch_size:(batch + 1) * self.batch_size]\n",
        "            y_true_batch = self.y_train[:, batch * self.batch_size:(batch + 1) * self.batch_size]\n",
        "\n",
        "            self.X_batch = X_batch\n",
        "            self.y_true_batch = y_true_batch\n",
        "\n",
        "            # Forward propagation\n",
        "            fw_network = Feedforward(self.X_batch, self.activation_fn, self.method, self.init_network).Forward_prop()\n",
        "            self.fw_network = fw_network\n",
        "            self.y_predicted = self.fw_network[L-1].h\n",
        "            self.y_pred_batch = self.y_predicted\n",
        "\n",
        "            # Compute loss\n",
        "            self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "            overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch,\n",
        "                                            self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "            self.loss_history.append(overall_loss)\n",
        "\n",
        "            # Backpropagation\n",
        "            bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch,\n",
        "                                         self.fw_network, self.weight_decay, self.batch, self.batch_size,\n",
        "                                         self.activation_fn).backward_propagation()\n",
        "            self.bp_network = bp_network\n",
        "\n",
        "\n",
        "            if len(self.loss_history) > 5:\n",
        "                recent_losses = self.loss_history[-5:]\n",
        "                loss_std = np.std(recent_losses)\n",
        "\n",
        "                if self.loss_history[-1] > self.loss_history[-2]:\n",
        "                    self.eta = max(self.eta * (0.9 if loss_std < 0.01 else 0.8), self.min_eta)\n",
        "\n",
        "            self.eta_history.append(self.eta)\n",
        "\n",
        "            for j in range(L):\n",
        "\n",
        "                u_w[j] = self.beta * u_w[j] + (1 - self.beta) * (self.bp_network[j].d_w) ** 2\n",
        "                u_b[j] = self.beta * u_b[j] + (1 - self.beta) * (self.bp_network[j].d_b) ** 2\n",
        "\n",
        "                denom_w = np.maximum(np.sqrt(u_w[j]) + self.epsilon, 1e-8)\n",
        "                denom_b = np.maximum(np.sqrt(u_b[j]) + self.epsilon, 1e-8)\n",
        "\n",
        "                self.fw_network[j].weight -= (self.eta / denom_w) * self.bp_network[j].d_w\n",
        "                self.fw_network[j].bias -= (self.eta / denom_b) * self.bp_network[j].d_b\n",
        "\n",
        "\n",
        "            max_grad_norm = 1.0\n",
        "            for j in range(L):\n",
        "                self.bp_network[j].d_w = np.clip(self.bp_network[j].d_w, -max_grad_norm, max_grad_norm)\n",
        "                self.bp_network[j].d_b = np.clip(self.bp_network[j].d_b, -max_grad_norm, max_grad_norm)\n",
        "\n",
        "\n",
        "            acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch,\n",
        "                                         self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "            accuracy = round(acc, 4)\n",
        "            train_loss = round(loss, 4)\n",
        "            self.training_history.append((accuracy, train_loss))\n",
        "\n",
        "            if self.validX_train is not None:\n",
        "                network = Feedforward(self.validX_train, self.activation_fn, self.method, self.init_network).Forward_prop()\n",
        "                self.network = network\n",
        "                self.valy_predicted = self.network[L-1].h\n",
        "\n",
        "                val_acc, val_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network,\n",
        "                                                     self.validy_train, self.loss, self.weight_decay,\n",
        "                                                     self.regularization_fn).calc_accuracy_loss()\n",
        "                val_accuracy = round(val_acc, 4)\n",
        "                val_loss = round(val_loss, 4)\n",
        "                val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network,\n",
        "                                                    self.validy_train, self.loss, self.weight_decay,\n",
        "                                                    self.regularization_fn).overall_loss()\n",
        "                self.validation_history.append((val_accuracy, val_loss))\n",
        "\n",
        "    return self.loss_history, self.training_history, self.eta_history, self.validation_history\n",
        "\n",
        "\n",
        "\n",
        "class ADAM(Optimizer):\n",
        "\n",
        "    def adam_GD(self):\n",
        "\n",
        "        assert(self.X_train.shape[1] == self.y_train.shape[1])\n",
        "        if self.validy_train is not None:\n",
        "            assert(self.validX_train.shape[1] == self.validy_train.shape[1])\n",
        "\n",
        "        L = len(self.n) - 1\n",
        "        self.eta_history = []\n",
        "        self.loss_history = []\n",
        "        self.training_history = []\n",
        "        self.valid_loss_history = []\n",
        "        self.validation_history = []\n",
        "        self.batch = 0\n",
        "        accuracy = 0\n",
        "        val_accuracy = 0\n",
        "        val_overall_loss = 0\n",
        "        train_loss = 0\n",
        "        i = 0\n",
        "\n",
        "        # Initialize moment estimates\n",
        "        u_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "        u_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "        v_w = [np.zeros_like(self.init_network[k].weight) for k in range(L)]\n",
        "        v_b = [np.zeros_like(self.init_network[k].bias) for k in range(L)]\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            print(f\"Epoch {epoch+1}/{self.epochs} | Train Acc: {accuracy} | Val Acc: {val_accuracy} | Train Loss: {train_loss}\")\n",
        "\n",
        "            for batch in tqdm(range(self.batches_number)):\n",
        "\n",
        "                # Mini-batch selection\n",
        "                X_batch = self.X_train[:, batch * self.batch_size:(batch + 1) * self.batch_size]\n",
        "                y_true_batch = self.y_train[:, batch * self.batch_size:(batch + 1) * self.batch_size]\n",
        "\n",
        "                self.X_batch = X_batch\n",
        "                self.y_true_batch = y_true_batch\n",
        "\n",
        "                # Forward propagation\n",
        "                fw_network = Feedforward(self.X_batch, self.activation_fn, self.method, self.init_network).Forward_prop()\n",
        "                self.fw_network = fw_network\n",
        "                self.y_predicted = self.fw_network[L-1].h\n",
        "                self.y_pred_batch = self.y_predicted\n",
        "\n",
        "                # Compute loss\n",
        "                self.loss = callloss(self.loss_function, self.y_true_batch, self.y_pred_batch).give_loss()\n",
        "                overall_loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.fw_network, self.y_true_batch,\n",
        "                                                self.loss, self.weight_decay, self.regularization_fn).overall_loss()\n",
        "                self.loss_history.append(overall_loss)\n",
        "\n",
        "                # Backpropagation\n",
        "                bp_network = Backpropagation(self.loss_function, self.X_batch, self.y_true_batch, self.y_pred_batch,\n",
        "                                             self.fw_network, self.weight_decay, self.batch, self.batch_size,\n",
        "                                             self.activation_fn).backward_propagation()\n",
        "                self.bp_network = bp_network\n",
        "\n",
        "                # **Dynamic Learning Rate Adjustment\n",
        "                self.eta = self.min_eta + 0.5 * (self.eta - self.min_eta) * (1 + np.cos((epoch / self.epochs) * np.pi))\n",
        "                self.eta_history.append(self.eta)\n",
        "\n",
        "                for j in range(L):\n",
        "                    # Compute moment estimates\n",
        "                    u_w[j] = self.beta * u_w[j] + (1 - self.beta) * self.bp_network[j].d_w\n",
        "                    u_b[j] = self.beta * u_b[j] + (1 - self.beta) * self.bp_network[j].d_b\n",
        "                    v_w[j] = self.beta2 * v_w[j] + (1 - self.beta2) * (self.bp_network[j].d_w) ** 2\n",
        "                    v_b[j] = self.beta2 * v_b[j] + (1 - self.beta2) * (self.bp_network[j].d_b) ** 2\n",
        "\n",
        "                    # Bias correction\n",
        "                    u_w_pred = u_w[j] / (1 - np.power(self.beta, i + 1))\n",
        "                    u_b_pred = u_b[j] / (1 - np.power(self.beta, i + 1))\n",
        "                    v_w_pred = v_w[j] / (1 - np.power(self.beta2, i + 1))\n",
        "                    v_b_pred = v_b[j] / (1 - np.power(self.beta2, i + 1))\n",
        "\n",
        "                    denom_w = np.maximum(np.sqrt(v_w_pred) + self.epsilon, 1e-8)\n",
        "                    denom_b = np.maximum(np.sqrt(v_b_pred) + self.epsilon, 1e-8)\n",
        "\n",
        "                    self.fw_network[j].weight -= (self.eta / denom_w) * u_w_pred + self.eta * self.weight_decay * self.fw_network[j].weight\n",
        "                    self.fw_network[j].bias -= (self.eta / denom_b) * u_b_pred\n",
        "\n",
        "\n",
        "                self.init_network = self.fw_network\n",
        "\n",
        "                acc, loss = CalculateAllLoss(self.X_batch, self.y_pred_batch, self.init_network, self.y_true_batch,\n",
        "                                             self.loss, self.weight_decay, self.regularization_fn).calc_accuracy_loss()\n",
        "                accuracy = round(acc, 4)\n",
        "                train_loss = round(loss, 4)\n",
        "                self.training_history.append((accuracy, train_loss))\n",
        "\n",
        "                i += 1\n",
        "\n",
        "                if self.validX_train is not None:\n",
        "                    network = Feedforward(self.validX_train, self.activation_fn, self.method, self.init_network).Forward_prop()\n",
        "                    self.network = network\n",
        "                    self.valy_predicted = self.network[L-1].h\n",
        "\n",
        "                    val_acc, val_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network,\n",
        "                                                         self.validy_train, self.loss, self.weight_decay,\n",
        "                                                         self.regularization_fn).calc_accuracy_loss()\n",
        "                    val_accuracy = round(val_acc, 4)\n",
        "                    val_loss = round(val_loss, 4)\n",
        "                    val_overall_loss = CalculateAllLoss(self.validX_train, self.valy_predicted, self.init_network,\n",
        "                                                        self.validy_train, self.loss, self.weight_decay,\n",
        "                                                        self.regularization_fn).overall_loss()\n",
        "                    self.validation_history.append((val_accuracy, val_loss))\n",
        "\n",
        "        return self.loss_history, self.training_history, self.eta_history, self.validation_history\n",
        "\n",
        "\n",
        "\n",
        "class GiveOptimizers(Optimizer):\n",
        "    def __init__(self, optimization_fn, loss_function, X_train, y_train, activation_fn, layers_dimensions ,method, batch_size, epochs,  validX_train , validy_train ,weight_decay = 0, eta = 0.01, beta = 0.9, beta2 = 0.999, regularization_fn = \"L2\",grad_reglr_fn = \"L2_d\", use_wandb=False):\n",
        "      self.optimization_function = optimization_fn.lower()\n",
        "      super().__init__( loss_function, X_train, y_train, activation_fn, layers_dimensions ,method, batch_size, epochs, validX_train, validy_train ,weight_decay = 0, eta = 0.01, beta = 0.9, beta2 = 0.999, regularization_fn = \"L2\",grad_reglr_fn = \"L2_d\" , use_wandb=False)\n",
        "\n",
        "    def apply_optimization(self):\n",
        "      if self.optimization_function == 'sgd':\n",
        "          return SGD(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs,  self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn, self.use_wandb).Gradient_descent()\n",
        "      elif self.optimization_function == 'momentum':\n",
        "          return MGD(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn, self.use_wandb).momentum_GD()\n",
        "      elif self.optimization_function == 'nag':\n",
        "          return NAG(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn, self.use_wandb).Nesterov_AGD()\n",
        "      elif self.optimization_function == 'rmsp':\n",
        "          return RMSProp(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn, self.use_wandb).rms_GD()\n",
        "      elif self.optimization_function == 'adam':\n",
        "          return ADAM(self.loss_function, self.X_train, self.y_train, self.activation_fn, self.n, self.method, self.batch_size, self.epochs, self.validX_train, self.validy_train, self.weight_decay,self.eta, self.beta, self.beta2, self.regularization_fn, self.grad_reglr_fn, self.use_wandb).adam_GD()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}